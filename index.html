<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Databricks Certified Data Engineer ‚Äî Exam Prep Platform</title>
<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
/* ‚îÄ‚îÄ‚îÄ RESET & ROOT ‚îÄ‚îÄ‚îÄ */
*,*::before,*::after{box-sizing:border-box;margin:0;padding:0}
:root{
  --bg:#0d0f14;
  --surface:#141720;
  --card:#1a1e2a;
  --card2:#1f2433;
  --border:#252b3b;
  --border2:#2f3748;
  --accent:#4f8ef7;
  --accent2:#6ba3ff;
  --accent-dim:rgba(79,142,247,0.12);
  --accent-glow:rgba(79,142,247,0.25);
  --green:#22c55e;
  --green-dim:rgba(34,197,94,0.12);
  --red:#ef4444;
  --red-dim:rgba(239,68,68,0.12);
  --yellow:#f59e0b;
  --yellow-dim:rgba(245,158,11,0.12);
  --text:#e8ecf8;
  --text2:#9aa5c4;
  --text3:#5a6485;
  --mono:'JetBrains Mono',monospace;
  --sans:'Outfit',sans-serif;
  --radius:10px;
  --shadow:0 4px 24px rgba(0,0,0,0.4);
}
html{scroll-behavior:smooth}
body{background:var(--bg);color:var(--text);font-family:var(--sans);min-height:100vh;overflow-x:hidden;font-size:15px;line-height:1.6}
button{font-family:var(--sans);cursor:pointer}
input{font-family:var(--sans)}
::-webkit-scrollbar{width:6px}
::-webkit-scrollbar-track{background:var(--surface)}
::-webkit-scrollbar-thumb{background:var(--border2);border-radius:3px}

/* ‚îÄ‚îÄ‚îÄ SCREENS ‚îÄ‚îÄ‚îÄ */
.screen{display:none;min-height:100vh}
.screen.active{display:flex;flex-direction:column}

/* ‚îÄ‚îÄ‚îÄ AUTH SCREEN ‚îÄ‚îÄ‚îÄ */
#screen-auth{align-items:center;justify-content:center;background:var(--bg);position:relative;overflow:hidden}
#screen-auth::before{
  content:'';position:absolute;inset:0;
  background:radial-gradient(ellipse 80% 60% at 50% -10%,rgba(79,142,247,0.15),transparent),
             radial-gradient(ellipse 40% 40% at 80% 80%,rgba(99,102,241,0.08),transparent);
  pointer-events:none;
}
.auth-grid{position:absolute;inset:0;background-image:linear-gradient(rgba(79,142,247,0.04) 1px,transparent 1px),linear-gradient(90deg,rgba(79,142,247,0.04) 1px,transparent 1px);background-size:48px 48px;pointer-events:none}

.auth-box{
  position:relative;z-index:1;
  background:var(--card);
  border:1px solid var(--border2);
  border-radius:16px;
  padding:48px;
  width:100%;max-width:440px;
  box-shadow:0 24px 80px rgba(0,0,0,0.5),0 0 0 1px rgba(79,142,247,0.1);
}
.auth-logo{display:flex;align-items:center;gap:12px;margin-bottom:32px}
.auth-logo-icon{
  width:44px;height:44px;
  background:linear-gradient(135deg,var(--accent),#6366f1);
  border-radius:10px;
  display:flex;align-items:center;justify-content:center;
  font-family:var(--mono);font-size:14px;font-weight:700;color:white;
  box-shadow:0 4px 16px rgba(79,142,247,0.4);
}
.auth-logo-text{font-size:13px;font-weight:600;letter-spacing:0.05em;color:var(--text2);line-height:1.3}
.auth-logo-text strong{display:block;color:var(--text);font-size:17px;letter-spacing:0;font-weight:700}

.auth-title{font-size:26px;font-weight:800;margin-bottom:6px;letter-spacing:-0.02em}
.auth-sub{color:var(--text2);font-size:14px;margin-bottom:32px}

.auth-tabs{display:flex;gap:0;background:var(--surface);border-radius:8px;padding:3px;margin-bottom:28px}
.auth-tab{flex:1;padding:8px;border:none;background:transparent;color:var(--text3);font-size:14px;font-weight:600;border-radius:6px;transition:all 0.2s}
.auth-tab.active{background:var(--card2);color:var(--text);box-shadow:0 1px 4px rgba(0,0,0,0.3)}

.form-group{margin-bottom:18px}
.form-label{display:block;font-size:12px;font-weight:600;letter-spacing:0.06em;text-transform:uppercase;color:var(--text2);margin-bottom:8px}
.form-input{
  width:100%;padding:12px 16px;
  background:var(--surface);border:1px solid var(--border2);
  border-radius:8px;color:var(--text);font-size:15px;
  transition:all 0.2s;outline:none;
}
.form-input:focus{border-color:var(--accent);box-shadow:0 0 0 3px var(--accent-glow)}
.form-input::placeholder{color:var(--text3)}

.btn-auth{
  width:100%;padding:14px;
  background:linear-gradient(135deg,var(--accent),#6366f1);
  color:white;border:none;border-radius:8px;
  font-size:15px;font-weight:700;
  transition:all 0.2s;margin-top:4px;
  box-shadow:0 4px 16px rgba(79,142,247,0.3);
}
.btn-auth:hover{transform:translateY(-1px);box-shadow:0 6px 24px rgba(79,142,247,0.45)}
.btn-auth:active{transform:translateY(0)}

.auth-error{
  background:var(--red-dim);border:1px solid rgba(239,68,68,0.3);
  border-radius:8px;padding:10px 14px;
  font-size:13px;color:var(--red);margin-bottom:16px;display:none;
}
.auth-error.show{display:block}

.auth-divider{text-align:center;color:var(--text3);font-size:12px;margin:20px 0;position:relative}
.auth-divider::before{content:'';position:absolute;left:0;top:50%;right:0;height:1px;background:var(--border)}
.auth-divider span{background:var(--card);padding:0 12px;position:relative}

/* ‚îÄ‚îÄ‚îÄ DASHBOARD SCREEN ‚îÄ‚îÄ‚îÄ */
#screen-dashboard{background:var(--bg)}

.top-nav{
  background:var(--surface);border-bottom:1px solid var(--border);
  padding:0 32px;height:60px;
  display:flex;align-items:center;justify-content:space-between;
  position:sticky;top:0;z-index:100;flex-shrink:0;
}
.nav-logo{display:flex;align-items:center;gap:10px}
.nav-logo-badge{
  width:32px;height:32px;
  background:linear-gradient(135deg,var(--accent),#6366f1);
  border-radius:7px;display:flex;align-items:center;justify-content:center;
  font-family:var(--mono);font-size:11px;font-weight:700;color:white;
}
.nav-logo-text{font-size:14px;font-weight:700;color:var(--text)}
.nav-logo-text span{color:var(--text2);font-weight:400}

.nav-right{display:flex;align-items:center;gap:16px}
.nav-user{
  display:flex;align-items:center;gap:10px;
  padding:6px 12px;border-radius:8px;
  background:var(--card);border:1px solid var(--border);
}
.nav-avatar{
  width:28px;height:28px;border-radius:50%;
  background:linear-gradient(135deg,var(--accent),#6366f1);
  display:flex;align-items:center;justify-content:center;
  font-size:12px;font-weight:700;color:white;
}
.nav-username{font-size:13px;font-weight:600;color:var(--text)}
.btn-logout{
  padding:6px 14px;border-radius:7px;border:1px solid var(--border2);
  background:transparent;color:var(--text2);font-size:13px;font-weight:500;
  transition:all 0.2s;
}
.btn-logout:hover{border-color:var(--red);color:var(--red);background:var(--red-dim)}

.dash-body{flex:1;padding:32px;max-width:1200px;margin:0 auto;width:100%}

.dash-hero{
  background:linear-gradient(135deg,var(--card) 0%,var(--card2) 100%);
  border:1px solid var(--border2);border-radius:16px;
  padding:36px 40px;margin-bottom:28px;
  display:flex;align-items:center;justify-content:space-between;
  position:relative;overflow:hidden;
}
.dash-hero::after{
  content:'';position:absolute;right:-40px;top:-40px;
  width:200px;height:200px;
  background:radial-gradient(circle,rgba(79,142,247,0.15),transparent 70%);
  pointer-events:none;
}
.dash-hero-text h1{font-size:28px;font-weight:800;letter-spacing:-0.02em;margin-bottom:6px}
.dash-hero-text h1 span{color:var(--accent)}
.dash-hero-text p{color:var(--text2);font-size:14px;max-width:420px;line-height:1.6}
.dash-hero-stats{display:flex;gap:24px;flex-shrink:0}
.hero-stat{text-align:center}
.hero-stat-val{font-family:var(--mono);font-size:32px;font-weight:700;color:var(--accent);line-height:1}
.hero-stat-label{font-size:11px;color:var(--text3);text-transform:uppercase;letter-spacing:0.1em;margin-top:4px}

.dash-grid{display:grid;grid-template-columns:1fr 340px;gap:20px;margin-bottom:20px}

.card{background:var(--card);border:1px solid var(--border);border-radius:var(--radius);padding:24px}
.card-title{font-size:12px;font-weight:700;letter-spacing:0.1em;text-transform:uppercase;color:var(--text3);margin-bottom:18px;font-family:var(--mono)}

/* Exam history table */
.history-table{width:100%;border-collapse:collapse}
.history-table th{font-size:11px;font-weight:700;letter-spacing:0.08em;text-transform:uppercase;color:var(--text3);padding:8px 12px;text-align:left;border-bottom:1px solid var(--border)}
.history-table td{padding:12px 12px;border-bottom:1px solid var(--border);font-size:13px;color:var(--text2);vertical-align:middle}
.history-table tr:last-child td{border-bottom:none}
.history-table tr:hover td{background:var(--card2)}

.score-pill{
  display:inline-block;padding:3px 10px;border-radius:100px;
  font-family:var(--mono);font-size:12px;font-weight:700;
}
.score-pill.pass{background:var(--green-dim);color:var(--green)}
.score-pill.fail{background:var(--red-dim);color:var(--red)}
.score-pill.warn{background:var(--yellow-dim);color:var(--yellow)}

.empty-state{text-align:center;padding:40px 20px;color:var(--text3)}
.empty-state .empty-icon{font-size:40px;margin-bottom:12px;opacity:0.4}
.empty-state p{font-size:13px}

/* Streak / domain cards */
.domain-mini{margin-bottom:12px;last-child{margin-bottom:0}}
.domain-mini-top{display:flex;justify-content:space-between;align-items:center;margin-bottom:6px}
.domain-mini-name{font-size:13px;font-weight:500;color:var(--text)}
.domain-mini-pct{font-family:var(--mono);font-size:12px;font-weight:700}
.domain-mini-bar{height:4px;background:var(--surface);border-radius:2px;overflow:hidden}
.domain-mini-fill{height:100%;border-radius:2px;transition:width 0.8s ease}

.btn-start-exam{
  width:100%;padding:16px;
  background:linear-gradient(135deg,var(--accent),#6366f1);
  color:white;border:none;border-radius:10px;
  font-size:16px;font-weight:700;
  transition:all 0.2s;
  box-shadow:0 4px 20px rgba(79,142,247,0.35);
  display:flex;align-items:center;justify-content:center;gap:10px;
  margin-bottom:12px;
}
.btn-start-exam:hover{transform:translateY(-2px);box-shadow:0 8px 28px rgba(79,142,247,0.5)}

.btn-practice{
  width:100%;padding:12px;
  background:var(--surface);
  color:var(--text2);border:1px solid var(--border2);border-radius:10px;
  font-size:14px;font-weight:600;
  transition:all 0.2s;
  display:flex;align-items:center;justify-content:center;gap:8px;
}
.btn-practice:hover{border-color:var(--accent);color:var(--accent);background:var(--accent-dim)}

.exam-type-select{
  display:grid;grid-template-columns:1fr 1fr;gap:8px;margin-bottom:16px;
}
.exam-type-btn{
  padding:10px;border:1px solid var(--border2);border-radius:8px;
  background:var(--surface);color:var(--text2);
  font-size:12px;font-weight:600;text-align:center;
  cursor:pointer;transition:all 0.2s;
}
.exam-type-btn.active{border-color:var(--accent);color:var(--accent);background:var(--accent-dim)}

/* Progress chart */
.progress-chart{position:relative;height:120px;margin-bottom:8px}
.chart-bars{display:flex;align-items:flex-end;gap:6px;height:100%}
.chart-bar-wrap{flex:1;display:flex;flex-direction:column;align-items:center;gap:4px;height:100%}
.chart-bar-outer{flex:1;width:100%;display:flex;align-items:flex-end;border-radius:4px;overflow:hidden;background:var(--surface)}
.chart-bar-inner{width:100%;border-radius:4px;min-height:4px;transition:height 0.6s ease}
.chart-bar-label{font-family:var(--mono);font-size:9px;color:var(--text3);writing-mode:horizontal-tb}

/* ‚îÄ‚îÄ‚îÄ EXAM SCREEN ‚îÄ‚îÄ‚îÄ */
#screen-exam{background:var(--bg);flex-direction:column}

.exam-topbar{
  background:var(--surface);border-bottom:1px solid var(--border);
  height:56px;padding:0 24px;
  display:flex;align-items:center;justify-content:space-between;
  position:sticky;top:0;z-index:100;flex-shrink:0;
}
.exam-topbar-left{display:flex;align-items:center;gap:16px}
.exam-domain-tag{
  font-family:var(--mono);font-size:11px;font-weight:600;
  letter-spacing:0.08em;text-transform:uppercase;
  color:var(--accent);padding:4px 10px;
  border:1px solid rgba(79,142,247,0.3);border-radius:4px;
  background:var(--accent-dim);
}
.exam-topbar-center{display:flex;align-items:center;gap:20px}
.exam-topbar-right{display:flex;align-items:center;gap:16px}

.timer-display{
  font-family:var(--mono);font-size:20px;font-weight:700;
  color:var(--text);min-width:72px;text-align:right;
  transition:color 0.3s;
}
.timer-display.warn{color:var(--yellow)}
.timer-display.danger{color:var(--red);animation:blink 1s infinite}
@keyframes blink{0%,100%{opacity:1}50%{opacity:0.4}}

.btn-submit-exam{
  padding:7px 18px;border-radius:7px;border:1px solid rgba(239,68,68,0.4);
  background:var(--red-dim);color:var(--red);
  font-size:13px;font-weight:700;transition:all 0.2s;
}
.btn-submit-exam:hover{background:var(--red);color:white;border-color:var(--red)}

.exam-progress-bar{height:2px;background:var(--border);flex-shrink:0}
.exam-progress-fill{height:100%;background:linear-gradient(90deg,var(--accent),#6366f1);transition:width 0.3s}

.exam-layout{display:flex;flex:1;overflow:hidden}

/* Sidebar */
.exam-sidebar{
  width:200px;flex-shrink:0;
  background:var(--surface);border-right:1px solid var(--border);
  padding:16px 12px;overflow-y:auto;
}
.sidebar-section-title{font-family:var(--mono);font-size:10px;font-weight:700;letter-spacing:0.1em;text-transform:uppercase;color:var(--text3);margin-bottom:10px;padding:0 4px}
.q-grid{display:grid;grid-template-columns:repeat(5,1fr);gap:3px;margin-bottom:20px}
.q-btn{
  aspect-ratio:1;border-radius:5px;border:1px solid var(--border);
  background:transparent;font-family:var(--mono);font-size:10px;font-weight:600;
  color:var(--text3);transition:all 0.15s;cursor:pointer;
  display:flex;align-items:center;justify-content:center;
}
.q-btn:hover{border-color:var(--accent2);color:var(--accent2)}
.q-btn.current{background:var(--accent);border-color:var(--accent);color:white}
.q-btn.answered{background:rgba(34,197,94,0.15);border-color:rgba(34,197,94,0.4);color:var(--green)}
.q-btn.flagged{background:var(--yellow-dim);border-color:rgba(245,158,11,0.4);color:var(--yellow)}
.q-btn.answered.flagged{background:var(--yellow-dim);border-color:rgba(245,158,11,0.4);color:var(--yellow)}

.sidebar-legend{border-top:1px solid var(--border);padding-top:14px;display:flex;flex-direction:column;gap:7px}
.legend-row{display:flex;align-items:center;gap:8px;font-size:11px;color:var(--text3)}
.legend-sq{width:11px;height:11px;border-radius:3px;border:1px solid var(--border)}
.legend-sq.lc{background:var(--accent);border-color:var(--accent)}
.legend-sq.la{background:rgba(34,197,94,0.15);border-color:rgba(34,197,94,0.4)}
.legend-sq.lf{background:var(--yellow-dim);border-color:rgba(245,158,11,0.4)}

/* Main question area */
.exam-main{flex:1;overflow-y:auto;padding:32px 40px;max-width:800px}

.q-header{display:flex;align-items:center;justify-content:space-between;margin-bottom:20px}
.q-counter{font-family:var(--mono);font-size:12px;color:var(--text3)}
.flag-toggle{
  display:flex;align-items:center;gap:6px;padding:6px 12px;
  border:1px solid var(--border2);border-radius:6px;
  background:transparent;color:var(--text3);font-size:12px;font-weight:600;
  transition:all 0.2s;
}
.flag-toggle:hover{border-color:var(--yellow);color:var(--yellow);background:var(--yellow-dim)}
.flag-toggle.flagged{border-color:rgba(245,158,11,0.4);color:var(--yellow);background:var(--yellow-dim)}

.question-stem{font-size:17px;font-weight:500;line-height:1.65;color:var(--text);margin-bottom:20px}

.code-snippet{
  background:#0d1117;border:1px solid var(--border2);border-left:3px solid var(--accent);
  border-radius:8px;padding:20px 22px;margin-bottom:22px;
  font-family:var(--mono);font-size:13px;line-height:1.8;
  overflow-x:auto;white-space:pre;color:#c9d1d9;
}
.kw{color:#ff7b72}.fn{color:#d2a8ff}.str{color:#a5d6ff}.cm{color:#6e7681}.num{color:#79c0ff}.op{color:#ff7b72}

.options-list{display:flex;flex-direction:column;gap:8px;margin-bottom:24px}
.opt{
  display:flex;align-items:flex-start;gap:14px;
  padding:14px 18px;border:1.5px solid var(--border2);
  border-radius:8px;cursor:pointer;background:var(--card);
  transition:all 0.18s;user-select:none;
}
.opt:hover:not(.locked){border-color:rgba(79,142,247,0.4);background:var(--accent-dim)}
.opt.selected{border-color:var(--accent);background:var(--accent-dim)}
.opt.correct{border-color:rgba(34,197,94,0.5);background:var(--green-dim)}
.opt.wrong{border-color:rgba(239,68,68,0.5);background:var(--red-dim)}
.opt.reveal{border-color:rgba(34,197,94,0.5);background:var(--green-dim)}
.opt.locked{cursor:default}

.opt-key{
  width:28px;height:28px;border-radius:6px;border:1.5px solid var(--border2);
  display:flex;align-items:center;justify-content:center;
  font-family:var(--mono);font-size:12px;font-weight:700;
  flex-shrink:0;margin-top:1px;background:var(--surface);color:var(--text2);
  transition:all 0.18s;
}
.opt.selected .opt-key{background:var(--accent);border-color:var(--accent);color:white}
.opt.correct .opt-key,.opt.reveal .opt-key{background:var(--green);border-color:var(--green);color:white}
.opt.wrong .opt-key{background:var(--red);border-color:var(--red);color:white}
.opt-text{font-size:14.5px;line-height:1.55;padding-top:3px;color:var(--text)}

.explanation{
  display:none;padding:16px 20px;border-radius:8px;
  font-size:13.5px;line-height:1.7;margin-bottom:20px;
  animation:fadeUp 0.25s ease;
}
.explanation.show-correct{display:block;background:var(--green-dim);border:1px solid rgba(34,197,94,0.25);color:#86efac}
.explanation.show-wrong{display:block;background:var(--red-dim);border:1px solid rgba(239,68,68,0.25);color:#fca5a5}
@keyframes fadeUp{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}
.exp-heading{font-family:var(--mono);font-size:10px;font-weight:700;letter-spacing:0.1em;text-transform:uppercase;margin-bottom:6px;opacity:0.7}

.q-footer{display:flex;align-items:center;justify-content:space-between;padding-top:16px;border-top:1px solid var(--border)}
.btn-nav{
  display:flex;align-items:center;gap:6px;padding:9px 20px;
  border-radius:7px;font-size:14px;font-weight:600;
  transition:all 0.18s;border:1.5px solid var(--border2);
  background:transparent;color:var(--text2);
}
.btn-nav:hover:not(:disabled){border-color:var(--text);color:var(--text)}
.btn-nav:disabled{opacity:0.3;cursor:default}
.btn-nav.primary{background:var(--accent);border-color:var(--accent);color:white}
.btn-nav.primary:hover:not(:disabled){background:var(--accent2);border-color:var(--accent2)}

.q-status-mid{font-family:var(--mono);font-size:12px;color:var(--text3);text-align:center}

/* ‚îÄ‚îÄ‚îÄ RESULTS SCREEN ‚îÄ‚îÄ‚îÄ */
#screen-results{background:var(--bg);overflow-y:auto}

.results-hero{
  background:linear-gradient(to bottom,var(--card),var(--bg));
  border-bottom:1px solid var(--border);
  padding:48px 40px;text-align:center;
}
.results-verdict{
  display:inline-block;padding:6px 20px;border-radius:100px;
  font-family:var(--mono);font-size:12px;font-weight:700;letter-spacing:0.12em;
  text-transform:uppercase;margin-bottom:16px;
}
.verdict-pass{background:var(--green-dim);color:var(--green);border:1px solid rgba(34,197,94,0.3)}
.verdict-fail{background:var(--red-dim);color:var(--red);border:1px solid rgba(239,68,68,0.3)}

.results-pct{font-family:var(--mono);font-size:80px;font-weight:700;line-height:1;letter-spacing:-0.04em;margin-bottom:4px}
.results-pct.pass{color:var(--green)}
.results-pct.fail{color:var(--red)}

.results-meta{font-size:14px;color:var(--text3);margin-bottom:32px}
.results-meta strong{color:var(--text2)}

.results-scorecards{display:flex;gap:1px;background:var(--border);border:1px solid var(--border);border-radius:10px;overflow:hidden;max-width:500px;margin:0 auto}
.scard{flex:1;background:var(--card);padding:20px 16px;text-align:center}
.scard-val{font-family:var(--mono);font-size:28px;font-weight:700;line-height:1;margin-bottom:4px}
.scard-label{font-size:10px;text-transform:uppercase;letter-spacing:0.1em;color:var(--text3)}

.results-body{max-width:960px;margin:0 auto;padding:40px 32px}

.section-heading{
  font-family:var(--mono);font-size:11px;font-weight:700;
  letter-spacing:0.12em;text-transform:uppercase;color:var(--text3);
  padding-bottom:12px;border-bottom:1px solid var(--border);margin-bottom:20px;
}

.domain-cards{display:flex;flex-direction:column;gap:10px;margin-bottom:40px}
.dcard{background:var(--card);border:1px solid var(--border);border-radius:10px;padding:20px 24px}
.dcard-top{display:flex;align-items:center;justify-content:space-between;margin-bottom:10px}
.dcard-name{font-size:14px;font-weight:600}
.dcard-score{font-family:var(--mono);font-size:14px;font-weight:700}
.dcard-bar{height:5px;background:var(--surface);border-radius:3px;overflow:hidden;margin-bottom:8px}
.dcard-fill{height:100%;border-radius:3px}
.dcard-rec{font-size:12.5px;color:var(--text3);line-height:1.5}
.dcard-rec.weak{color:#fca5a5}

.weak-box{
  background:rgba(239,68,68,0.05);border:1px solid rgba(239,68,68,0.2);
  border-radius:10px;padding:24px;margin-bottom:40px;
}
.weak-box-title{font-size:14px;font-weight:700;color:var(--red);margin-bottom:16px;display:flex;align-items:center;gap:8px}
.weak-item{display:flex;align-items:flex-start;gap:12px;padding:12px 0;border-bottom:1px solid rgba(239,68,68,0.1)}
.weak-item:last-child{border-bottom:none;padding-bottom:0}
.weak-num{width:22px;height:22px;border-radius:50%;background:var(--red);color:white;font-family:var(--mono);font-size:10px;font-weight:700;display:flex;align-items:center;justify-content:center;flex-shrink:0;margin-top:1px}
.weak-info strong{display:block;font-size:13px;color:var(--text);margin-bottom:2px}
.weak-info span{font-size:12px;color:var(--text3)}

.review-list{display:flex;flex-direction:column;gap:8px;margin-bottom:40px}
.review-item{background:var(--card);border:1px solid var(--border);border-radius:8px;overflow:hidden}
.review-header{display:flex;align-items:center;gap:12px;padding:14px 18px;cursor:pointer}
.review-header:hover{background:var(--card2)}
.r-icon{width:22px;height:22px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:11px;font-weight:700;flex-shrink:0}
.r-icon.c{background:var(--green-dim);color:var(--green);border:1px solid rgba(34,197,94,0.3)}
.r-icon.w{background:var(--red-dim);color:var(--red);border:1px solid rgba(239,68,68,0.3)}
.r-icon.s{background:var(--yellow-dim);color:var(--yellow);border:1px solid rgba(245,158,11,0.3)}
.review-q-text{flex:1;font-size:13px;color:var(--text2);line-height:1.4}
.review-domain-tag{font-family:var(--mono);font-size:10px;color:var(--text3);flex-shrink:0}
.review-body{display:none;padding:0 18px 16px;border-top:1px solid var(--border);background:var(--card2)}
.review-body.open{display:block}
.review-ans-row{display:flex;align-items:center;gap:8px;font-size:13px;padding:8px 12px;border-radius:6px;margin-top:6px}
.review-ans-row.correct{background:var(--green-dim);color:var(--green)}
.review-ans-row.wrong{background:var(--red-dim);color:var(--red)}
.review-exp-text{margin-top:10px;padding:12px 14px;background:var(--surface);border-radius:6px;font-size:13px;line-height:1.65;color:var(--text2)}

.results-actions{display:flex;gap:12px;justify-content:center;padding:20px 0 40px;border-top:1px solid var(--border)}
.btn-res{padding:12px 28px;border-radius:8px;font-size:14px;font-weight:700;transition:all 0.2s;border:none}
.btn-res.primary{background:linear-gradient(135deg,var(--accent),#6366f1);color:white;box-shadow:0 4px 16px rgba(79,142,247,0.3)}
.btn-res.primary:hover{transform:translateY(-1px);box-shadow:0 6px 24px rgba(79,142,247,0.5)}
.btn-res.ghost{background:var(--card);border:1px solid var(--border2);color:var(--text2)}
.btn-res.ghost:hover{border-color:var(--text);color:var(--text)}

/* ‚îÄ‚îÄ‚îÄ MODAL ‚îÄ‚îÄ‚îÄ */
.modal-overlay{display:none;position:fixed;inset:0;background:rgba(0,0,0,0.7);z-index:1000;align-items:center;justify-content:center;backdrop-filter:blur(6px)}
.modal-overlay.open{display:flex}
.modal{background:var(--card);border:1px solid var(--border2);border-radius:14px;padding:36px;max-width:420px;width:90%;box-shadow:0 24px 80px rgba(0,0,0,0.6);animation:modalIn 0.25s ease;text-align:center}
@keyframes modalIn{from{opacity:0;transform:scale(0.94) translateY(-12px)}to{opacity:1;transform:scale(1) translateY(0)}}
.modal h2{font-size:22px;font-weight:800;margin-bottom:10px}
.modal p{font-size:14px;color:var(--text2);line-height:1.6;margin-bottom:28px}
.modal-actions{display:flex;gap:10px;justify-content:center}
.btn-modal-cancel{padding:10px 22px;border-radius:8px;border:1px solid var(--border2);background:transparent;color:var(--text2);font-size:14px;font-weight:600;transition:all 0.2s}
.btn-modal-cancel:hover{border-color:var(--text);color:var(--text)}
.btn-modal-confirm{padding:10px 22px;border-radius:8px;background:var(--red);color:white;border:none;font-size:14px;font-weight:700;transition:all 0.2s}
.btn-modal-confirm:hover{background:#dc2626}

/* ‚îÄ‚îÄ‚îÄ UTILS ‚îÄ‚îÄ‚îÄ */
.hidden{display:none!important}
.mt4{margin-top:4px}.mt8{margin-top:8px}.mt16{margin-top:16px}.mt24{margin-top:24px}

@media(max-width:768px){
  .exam-sidebar{display:none}
  .exam-main{padding:20px}
  .dash-grid{grid-template-columns:1fr}
  .dash-hero{flex-direction:column;gap:24px}
  .results-body{padding:24px 16px}
  .exam-topbar{padding:0 16px}
}
</style>
</head>
<body>

<!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê AUTH ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
<div id="screen-auth" class="screen active">
  <div class="auth-grid"></div>
  <div class="auth-box">
    <div class="auth-logo">
      <div class="auth-logo-icon">DB</div>
      <div class="auth-logo-text"><strong>DataBricks Prep</strong>DE Associate Exam Platform</div>
    </div>
    <h1 class="auth-title">Welcome back</h1>
    <p class="auth-sub">Sign in to track your progress and resume practice</p>

    <div class="auth-tabs">
      <button class="auth-tab active" onclick="switchTab('login')">Sign In</button>
      <button class="auth-tab" onclick="switchTab('register')">Create Account</button>
    </div>

    <div id="auth-error" class="auth-error"></div>

    <div id="tab-login">
      <div class="form-group">
        <label class="form-label">Username</label>
        <input class="form-input" id="login-user" type="text" placeholder="Enter your username" autocomplete="username">
      </div>
      <div class="form-group">
        <label class="form-label">Password</label>
        <input class="form-input" id="login-pass" type="password" placeholder="Enter your password" autocomplete="current-password">
      </div>
      <button class="btn-auth" onclick="doLogin()">Sign In ‚Üí</button>
      <div class="auth-divider"><span>Demo: use any username + password</span></div>
    </div>

    <div id="tab-register" style="display:none">
      <div class="form-group">
        <label class="form-label">Full Name</label>
        <input class="form-input" id="reg-name" type="text" placeholder="Your name">
      </div>
      <div class="form-group">
        <label class="form-label">Username</label>
        <input class="form-input" id="reg-user" type="text" placeholder="Choose a username" autocomplete="username">
      </div>
      <div class="form-group">
        <label class="form-label">Password</label>
        <input class="form-input" id="reg-pass" type="password" placeholder="Choose a password (6+ chars)" autocomplete="new-password">
      </div>
      <button class="btn-auth" onclick="doRegister()">Create Account ‚Üí</button>
    </div>
  </div>
</div>

<!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê DASHBOARD ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
<div id="screen-dashboard" class="screen">
  <nav class="top-nav">
    <div class="nav-logo">
      <div class="nav-logo-badge">DB</div>
      <div class="nav-logo-text">DataBricks Prep <span>¬∑ DE Associate</span></div>
    </div>
    <div class="nav-right">
      <div class="nav-user">
        <div class="nav-avatar" id="nav-avatar">U</div>
        <span class="nav-username" id="nav-username">User</span>
      </div>
      <button class="btn-logout" onclick="doLogout()">Sign Out</button>
    </div>
  </nav>

  <div class="dash-body">
    <div class="dash-hero">
      <div class="dash-hero-text">
        <h1>Hello, <span id="hero-name">there</span> üëã</h1>
        <p>Practice smarter with 380+ exam-style questions. Track your progress, identify weak areas, and build confidence before exam day.</p>
      </div>
      <div class="dash-hero-stats">
        <div class="hero-stat">
          <div class="hero-stat-val" id="stat-exams-taken">0</div>
          <div class="hero-stat-label">Exams Taken</div>
        </div>
        <div class="hero-stat">
          <div class="hero-stat-val" id="stat-best-score">‚Äî</div>
          <div class="hero-stat-label">Best Score</div>
        </div>
        <div class="hero-stat">
          <div class="hero-stat-val" id="stat-avg-score">‚Äî</div>
          <div class="hero-stat-label">Avg Score</div>
        </div>
      </div>
    </div>

    <div class="dash-grid">
      <!-- LEFT: History -->
      <div class="card">
        <div class="card-title">Exam History</div>
        <div id="history-container"></div>
      </div>

      <!-- RIGHT: Start + domain -->
      <div>
        <div class="card" style="margin-bottom:16px">
          <div class="card-title">Start New Exam</div>
          <div class="exam-type-select">
            <div class="exam-type-btn active" id="type-full" onclick="setExamType('full')">üìã Full Exam<br><small style="font-weight:400;color:var(--text3)">90 Q ¬∑ 120 min</small></div>
            <div class="exam-type-btn" id="type-quick" onclick="setExamType('quick')">‚ö° Quick Practice<br><small style="font-weight:400;color:var(--text3)">30 Q ¬∑ 40 min</small></div>
            <div class="exam-type-btn" id="type-mini" onclick="setExamType('mini')">üéØ Mini Quiz<br><small style="font-weight:400;color:var(--text3)">15 Q ¬∑ 20 min</small></div>
            <div class="exam-type-btn" id="type-domain" onclick="setExamType('domain')">üóÇÔ∏è Domain Focus<br><small style="font-weight:400;color:var(--text3)">25 Q per topic</small></div>
          </div>
          <div id="domain-select-area" style="display:none;margin-bottom:12px">
            <label class="form-label" style="margin-bottom:6px">Select Domain</label>
            <select id="domain-select" style="width:100%;padding:10px 12px;background:var(--surface);border:1px solid var(--border2);border-radius:8px;color:var(--text);font-size:14px;font-family:var(--sans)">
              <option value="Lakehouse Platform">Lakehouse Platform</option>
              <option value="ELT with Spark & Python">ELT with Spark & Python</option>
              <option value="Incremental Data Processing">Incremental Data Processing</option>
              <option value="Production Pipelines">Production Pipelines</option>
              <option value="Data Governance">Data Governance</option>
            </select>
          </div>
          <button class="btn-start-exam" onclick="startExam()">
            <span>Start Exam</span><span>‚Üí</span>
          </button>
        </div>

        <div class="card">
          <div class="card-title">Domain Performance</div>
          <div id="domain-perf-container">
            <div class="empty-state"><div class="empty-icon">üìä</div><p>Complete an exam to see domain performance</p></div>
          </div>
        </div>
      </div>
    </div>

    <!-- Score trend -->
    <div class="card" id="trend-card" style="display:none">
      <div class="card-title">Score Trend</div>
      <div class="progress-chart">
        <div class="chart-bars" id="trend-chart"></div>
      </div>
      <div style="text-align:center;font-size:11px;color:var(--text3);margin-top:4px">Last 10 exams ‚Äî 70% pass line shown in blue</div>
    </div>
  </div>
</div>

<!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê EXAM ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
<div id="screen-exam" class="screen">
  <div class="exam-topbar">
    <div class="exam-topbar-left">
      <span class="exam-domain-tag" id="exam-domain-tag">Domain</span>
    </div>
    <div class="exam-topbar-center">
      <span style="font-family:var(--mono);font-size:12px;color:var(--text3)" id="exam-qcounter">1/90</span>
      <span style="font-family:var(--mono);font-size:12px;color:var(--text3)" id="exam-answered-count">Answered: 0</span>
    </div>
    <div class="exam-topbar-right">
      <div class="timer-display" id="timer-display">120:00</div>
      <button class="btn-submit-exam" onclick="openSubmitModal()">Submit Exam</button>
    </div>
  </div>
  <div class="exam-progress-bar"><div class="exam-progress-fill" id="exam-progress-fill" style="width:0%"></div></div>

  <div class="exam-layout">
    <aside class="exam-sidebar">
      <div class="sidebar-section-title">Questions</div>
      <div class="q-grid" id="q-grid"></div>
      <div class="sidebar-legend">
        <div class="legend-row"><div class="legend-sq lc"></div>Current</div>
        <div class="legend-row"><div class="legend-sq la"></div>Answered</div>
        <div class="legend-row"><div class="legend-sq lf"></div>Flagged</div>
      </div>
    </aside>

    <main class="exam-main">
      <div class="q-header">
        <span class="q-counter" id="q-counter-label">Question 1 of 90</span>
        <button class="flag-toggle" id="flag-btn" onclick="toggleFlag()">‚öë Flag</button>
      </div>
      <div class="question-stem" id="q-stem"></div>
      <div class="code-snippet hidden" id="q-code"></div>
      <div class="options-list" id="q-options"></div>
      <div class="explanation hidden" id="q-explanation"></div>
      <div class="q-footer">
        <button class="btn-nav" id="btn-prev" onclick="navTo(currentQ-1)">‚Üê Prev</button>
        <div class="q-status-mid" id="q-status-mid"></div>
        <button class="btn-nav primary" id="btn-next" onclick="navTo(currentQ+1)">Next ‚Üí</button>
      </div>
    </main>
  </div>
</div>

<!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê RESULTS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->
<div id="screen-results" class="screen"></div>

<!-- Submit Modal -->
<div class="modal-overlay" id="modal-submit">
  <div class="modal">
    <h2>Submit Exam?</h2>
    <p id="modal-submit-msg">Are you sure you want to submit?</p>
    <div class="modal-actions">
      <button class="btn-modal-cancel" onclick="closeModal('modal-submit')">Go Back</button>
      <button class="btn-modal-confirm" onclick="submitExam()">Submit Now</button>
    </div>
  </div>
</div>

<!-- Time Up Modal -->
<div class="modal-overlay" id="modal-timeup">
  <div class="modal">
    <h2>‚è± Time's Up!</h2>
    <p>Your 120 minutes are up. Your exam is being submitted now.</p>
    <div class="modal-actions">
      <button class="btn-modal-confirm" onclick="submitExam()">View Results</button>
    </div>
  </div>
</div>

<script>// questions.js ‚Äî 380+ questions across all 5 exam domains
const ALL_QUESTIONS = [

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// DOMAIN 1: Databricks Lakehouse Platform (~24%)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{domain:"Lakehouse Platform",q:"Which of the following best describes the Databricks Lakehouse architecture?",opts:["A managed cloud data warehouse that stores data in proprietary columnar format","An architecture combining the flexibility of data lakes with ACID capabilities and data management of warehouses","A streaming-only platform built on Apache Kafka and Flink","A unified serving layer replacing both OLTP and OLAP systems"],ans:1,exp:"The Lakehouse combines data lake cost/flexibility with warehouse reliability. Open formats (Delta) on cheap object storage with full ACID, governance, and BI support."},
{domain:"Lakehouse Platform",q:"What is the primary role of Delta Lake in the Databricks Lakehouse?",opts:["A BI visualization layer on top of Parquet files","An open-source storage layer providing ACID transactions, scalable metadata, and unified batch/streaming","A columnar in-memory cache for speeding up SQL queries","A proprietary format that replaces Parquet"],ans:1,exp:"Delta Lake is an open-source storage format adding ACID transactions, schema enforcement, time travel, and scalable metadata handling on top of Parquet files in cloud object storage."},
{domain:"Lakehouse Platform",q:"A data engineer needs low-latency interactive SQL queries for BI tools. Which Databricks compute is most appropriate?",opts:["Standard job cluster","SQL Warehouse (Serverless or Pro)","Single-node all-purpose cluster","Streaming cluster with micro-batch triggers"],ans:1,exp:"SQL Warehouses are purpose-built for BI and SQL analytics. They feature Photon acceleration, auto-scaling, result caching, and concurrency handling optimized for many simultaneous BI queries."},
{domain:"Lakehouse Platform",q:"What is the key difference between an all-purpose cluster and a job cluster?",opts:["All-purpose clusters support Python only; job clusters support all languages","All-purpose clusters are persistent and interactive; job clusters are ephemeral, terminated after job completion","Job clusters have unlimited memory; all-purpose clusters are capped","All-purpose clusters cannot run notebooks"],ans:1,exp:"All-purpose clusters are manually started for interactive development and shared use. Job clusters are auto-created when a job runs and auto-terminated when done ‚Äî cost-efficient for production workloads."},
{domain:"Lakehouse Platform",q:"Which file format stores the actual data in Delta Lake tables?",opts:["ORC","Avro","Parquet","JSON"],ans:2,exp:"Delta Lake stores data as Parquet files. The Delta layer adds the transaction log (_delta_log) directory containing JSON commit files and Parquet checkpoints on top of the raw Parquet data."},
{domain:"Lakehouse Platform",q:"Which feature allows sharing Delta tables with external Spark clusters without copying data?",opts:["Delta Sharing","Unity Catalog External Tables","DBFS Mount Points","Databricks Connect"],ans:0,exp:"Delta Sharing is an open protocol for secure, cross-platform data sharing. It shares read access to Delta tables via REST API without data duplication, accessible by non-Databricks clients."},
{domain:"Lakehouse Platform",q:"What does DBFS (Databricks File System) provide?",opts:["A proprietary binary format replacing Parquet","A distributed file system abstracting cloud object storage with unified path interface","An on-disk NFS mount on the driver node only","A metadata catalog tracking all files across workspaces"],ans:1,exp:"DBFS is a distributed filesystem mounted to Databricks workspaces. It abstracts S3/ADLS/GCS behind familiar paths like /mnt/ or dbfs:/, enabling consistent file access across the cluster."},
{domain:"Lakehouse Platform",q:"What does the Photon engine provide in Databricks?",opts:["GPU-accelerated deep learning training","A native vectorized C++ query engine significantly speeding up SQL and DataFrame operations","Real-time streaming with sub-millisecond latency","Automatic table partitioning based on query patterns"],ans:1,exp:"Photon is a native vectorized execution engine written in C++. It processes data in columnar batches using SIMD instructions, delivering major speedups for SQL and DataFrame operations."},
{domain:"Lakehouse Platform",q:"A data engineer drops an External Table in Unity Catalog. What happens to the underlying data files?",opts:["Both metadata and data files are deleted","Only metadata is removed; data files in cloud storage remain intact","Files move to a recycle bin for 30 days","The operation fails ‚Äî External Tables cannot be dropped"],ans:1,exp:"Dropping an External Table removes only the table definition from the metastore. Data files at the external location are preserved. This contrasts with Managed Tables where dropping deletes data too."},
{domain:"Lakehouse Platform",q:"Which table type in Unity Catalog stores data in Databricks-managed storage?",opts:["External Table","Managed Table","View","Foreign Table"],ans:1,exp:"Managed Tables store data files in the Unity Catalog-managed storage location. Dropping a Managed Table deletes both the metadata and the underlying data files."},
{domain:"Lakehouse Platform",q:"What is the purpose of Databricks Repos?",opts:["Deploy ML models to production","Integrate with Git providers to version-control notebooks and files","Share Delta tables across workspaces","Create serverless cloud functions"],ans:1,exp:"Databricks Repos provides Git integration within workspaces. Engineers can clone repos, commit, branch, and do pull requests ‚Äî enabling software engineering best practices for notebook-based work."},
{domain:"Lakehouse Platform",q:"A query filters on columns in a Delta table. Which Delta feature automatically skips reading irrelevant data files?",opts:["Z-Order clustering","Data skipping using per-file min/max statistics stored in the transaction log","Partition pruning only","Bloom filter indexes"],ans:1,exp:"Delta automatically collects min/max statistics per column per file during writes. At query time, the engine compares filter predicates against these statistics to skip files that definitely don't contain matching data."},
{domain:"Lakehouse Platform",q:"What is the role of the _delta_log directory in a Delta table?",opts:["Stores raw Parquet data files","Maintains the transaction log as JSON commit files and Parquet checkpoint files","Caches query results for faster reads","Holds schema evolution metadata only"],ans:1,exp:"The _delta_log contains one JSON file per commit (recording operations, schema, statistics) and periodic Parquet checkpoint files summarizing history. This is what gives Delta its ACID, time travel, and audit capabilities."},
{domain:"Lakehouse Platform",q:"How can a Delta table stored on a cloud path be queried without metastore registration?",opts:["It cannot ‚Äî Delta tables must be registered","spark.table() with the full cloud path","spark.read.format('delta').load('path') or SELECT * FROM delta.`path`","By first running CREATE TABLE USING DELTA LOCATION"],ans:2,exp:"Delta tables can be queried directly by path: spark.read.format('delta').load('s3://...') in the API or SELECT * FROM delta.`s3://...` in SQL. No metastore registration is required for ad-hoc access."},
{domain:"Lakehouse Platform",q:"Which Databricks feature allows running notebook code on a local machine while using remote cluster compute?",opts:["Databricks Repos","Databricks Connect","MLflow Projects","Delta Sharing"],ans:1,exp:"Databricks Connect lets you run Spark code from your local IDE (VS Code, PyCharm) against a remote Databricks cluster. This enables local development with full cluster-scale compute."},
{domain:"Lakehouse Platform",q:"What happens when a cluster is configured with auto-scaling?",opts:["The cluster automatically upgrades to faster hardware when under load","The cluster adds or removes worker nodes dynamically based on workload demand within configured min/max bounds","Memory is automatically allocated from a shared pool","The cluster duplicates itself during peak hours"],ans:1,exp:"Auto-scaling monitors pending tasks and stage metrics. When tasks are queued, it adds workers up to the max. When tasks complete and workers are idle, it removes them down to the min, optimizing cost and performance."},
{domain:"Lakehouse Platform",q:"What is a Databricks Secret Scope used for?",opts:["Defining network security groups for clusters","Securely storing credentials (passwords, API keys, tokens) accessible via dbutils.secrets.get()","Encrypting Delta table data at rest","Restricting which notebooks can be run by specific users"],ans:1,exp:"Secret Scopes store sensitive credentials in a secure vault (Databricks-managed or Azure Key Vault-backed). Values are accessed with dbutils.secrets.get(scope, key) and never displayed in output ‚Äî they appear as [REDACTED]."},
{domain:"Lakehouse Platform",q:"Which statement about Unity Catalog's metastore is TRUE?",opts:["Each workspace has its own separate metastore","A single metastore can be shared across multiple workspaces in the same region","Metastores are limited to 100 catalogs each","The metastore stores the actual data files alongside metadata"],ans:1,exp:"Unity Catalog uses a single metastore per cloud region/account shared across all workspaces. This enables centralized governance, cross-workspace data sharing, and consistent access control."},
{domain:"Lakehouse Platform",q:"A data engineer wants to query a Hive-registered table from Python. Which is the correct approach?",opts:["spark.sql('SELECT * FROM db.table')","pd.read_sql('SELECT * FROM db.table', conn)","spark.hive.read('db.table')","dbutils.fs.ls('dbfs:/user/hive/warehouse/')"],ans:0,exp:"spark.sql() executes SQL against the Spark SQL engine which resolves table names via the configured metastore (Hive or Unity Catalog). This is the standard way to query registered tables from PySpark."},
{domain:"Lakehouse Platform",q:"What is the three-level namespace in Unity Catalog?",opts:["workspace.database.table","catalog.schema.table","metastore.catalog.table","environment.catalog.schema"],ans:1,exp:"Unity Catalog uses catalog.schema.table (e.g., main.sales.orders). USE CATALOG and USE SCHEMA can set defaults. This three-level hierarchy enables multi-tenant governance across workspaces."},

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// DOMAIN 2: ELT with Apache Spark & Python (~29%)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{domain:"ELT with Spark & Python",q:"When is a Spark DataFrame transformation actually executed?",opts:["Immediately after each transformation call","After the first transformation returning a non-null result","Only when an action such as count(), show(), or write() is called","When the DataFrame is assigned to a variable"],ans:2,exp:"Spark uses lazy evaluation. Transformations build a DAG logical plan. Execution is deferred until an action (count, collect, write, show) triggers the computation."},
{domain:"ELT with Spark & Python",q:"What is the difference between a narrow and a wide transformation?",opts:["Narrow transformations involve sorting; wide do not","Narrow require a shuffle; wide do not","Narrow process data within a single partition; wide require shuffling data across partitions","Wide read from multiple DataFrames; narrow from one"],ans:2,exp:"Narrow transformations (filter, select, map) process each partition independently with no data movement. Wide transformations (groupBy, join, sort) require shuffling data across partitions, marking stage boundaries."},
{domain:"ELT with Spark & Python",q:"Which Spark function applies a custom Python function row-by-row to create a new column?",opts:["df.apply()","df.withColumn() with a UDF","df.map()","df.transform()"],ans:1,exp:"UDFs registered with @udf or spark.udf.register() can be applied in withColumn(). Note: UDFs serialize data to Python (crossing the JVM boundary), so they're slower than built-in Spark functions."},
{domain:"ELT with Spark & Python",q:"Which join type returns all rows from the left DataFrame and matching rows from the right (nulls where no match)?",opts:["inner","left","right","outer"],ans:1,exp:"Left join keeps all rows from the left DataFrame. Where a right match exists, right columns are populated. Where no match exists, right columns are null. Right join is the mirror image."},
{domain:"ELT with Spark & Python",q:"What does df.persist(StorageLevel.MEMORY_AND_DISK) do?",opts:["Immediately writes the DataFrame to disk","Marks the DataFrame to cache in memory (spilling to disk if needed) on the next action","Saves the DataFrame as a managed Delta table","Broadcasts the DataFrame to all executors"],ans:1,exp:"persist() is lazy ‚Äî it marks the DataFrame for caching at the specified storage level. The first action triggers materialization. Subsequent actions reuse cached data, avoiding recomputation of the DAG."},
{domain:"ELT with Spark & Python",q:"What does the explode() function do?",opts:["Splits a string column by a delimiter into multiple columns","Transforms each element of an array/map into a separate row","Increases partition count by splitting large partitions","Unnests nested structs into flat schema"],ans:1,exp:"explode() takes an array or map column and creates one row per element. A row with array [A,B,C] becomes 3 rows. This is fundamental for processing nested/semi-structured data like JSON arrays."},
{domain:"ELT with Spark & Python",q:"A data engineer notices a Spark job has one task running 50x longer than others. What is the most likely cause?",opts:["Insufficient RAM on the cluster","Data skew: one partition key has disproportionately more records","The driver is a bottleneck","The DataFrame has too many columns"],ans:1,exp:"Data skew means one partition has far more data than others. Solutions: enable AQE skew join (spark.sql.adaptive.skewJoin.enabled=true), salt the skewed key, or separate the skewed key into a broadcast join."},
{domain:"ELT with Spark & Python",q:"After a groupBy().agg() operation, the result always has exactly 200 partitions. Why?",opts:["groupBy always outputs 200 rows","spark.sql.shuffle.partitions defaults to 200, controlling output partitions after shuffles","200 is the maximum Spark partition count","The driver limits partitions to 200 for safety"],ans:1,exp:"spark.sql.shuffle.partitions (default 200) sets the number of partitions created after any shuffle operation (groupBy, join, sort). Enable AQE (spark.sql.adaptive.enabled=true) to auto-tune this."},
{domain:"ELT with Spark & Python",q:"Which join strategy should be used when one DataFrame is small enough to fit in executor memory?",opts:["Sort-Merge Join","Broadcast Hash Join","Shuffle Hash Join","Cross Join"],ans:1,exp:"Broadcast Hash Join sends the small DataFrame to every executor, eliminating the shuffle of the large table. Triggered automatically when the smaller side is below spark.sql.autoBroadcastJoinThreshold (default 10MB, configurable)."},
{domain:"ELT with Spark & Python",q:"What does df.explain(True) display?",opts:["Execution time per stage","All four query plans: Parsed, Analyzed, Optimized, and Physical","Records processed per executor","Schema of all DataFrames in the session"],ans:1,exp:"explain(True) / explain(mode='extended') shows all plan stages. The Optimized Plan reveals what the Catalyst optimizer changed; the Physical Plan shows join strategies, scan types, and aggregation implementations."},
{domain:"ELT with Spark & Python",q:"What is the difference between repartition() and coalesce()?",opts:["repartition() only reduces; coalesce() can increase or decrease","repartition() always shuffles and can change in any direction; coalesce() only reduces with a minimal shuffle","Both are identical but repartition() uses hash partitioning","coalesce() triggers an action; repartition() is lazy"],ans:1,exp:"repartition() does a full shuffle ‚Äî it can increase or decrease partition count. coalesce() moves data from fewer executors to reduce partitions without a full shuffle, making it efficient for reducing partition count."},
{domain:"ELT with Spark & Python",q:"A data engineer runs df.dropDuplicates(['customer_id','order_date']). What happens?",opts:["Removes rows where customer_id or order_date is null","Keeps one row per unique combination of customer_id and order_date","Drops the customer_id and order_date columns","Raises an error if duplicates exist"],ans:1,exp:"dropDuplicates(subset) deduplicates based on the specified columns only. For each unique key combination, one row is kept (non-deterministic choice). Combine with orderBy() to control which duplicate survives."},
{domain:"ELT with Spark & Python",q:"Which Spark function generates unique monotonically increasing (but not necessarily consecutive) IDs?",opts:["row_number()","monotonically_increasing_id()","rank()","uuid()"],ans:1,exp:"monotonically_increasing_id() generates unique 64-bit integers encoding partition ID in upper bits. They're monotonically increasing across partitions but have gaps between partitions. row_number() needs a window spec."},
{domain:"ELT with Spark & Python",q:"How do you compute a 7-day rolling average per product in Spark?",opts:["groupBy('product').agg(avg('sales'))","Window function with rowsBetween(-6,0) over date-ordered, product-partitioned window","A self-join on date range","A UDF reading the last 7 rows per product"],ans:1,exp:"Window functions with rowsBetween(-6, 0) define a frame of the current + 6 preceding rows. Combined with Window.partitionBy('product').orderBy('date'), this computes a 7-row rolling average without data movement."},
{domain:"ELT with Spark & Python",q:"What is the risk of spark.read.format('csv').option('inferSchema','true')?",opts:["inferSchema is not supported for CSV","Schema inference requires a full scan and can produce incorrect types on production data","inferSchema always returns StringType for all columns","Result has no column names"],ans:1,exp:"Schema inference requires reading the entire file to determine types. For large files this is expensive. Also, inferred types can be wrong (e.g., an integer column with one null row). Always specify an explicit StructType in production."},
{domain:"ELT with Spark & Python",q:"Which function converts a Unix timestamp (seconds since epoch) to a readable timestamp?",opts:["to_timestamp()","from_unixtime()","cast('timestamp')","unix_timestamp()"],ans:1,exp:"from_unixtime() converts numeric seconds-since-epoch to a timestamp string. to_timestamp() parses a formatted string. unix_timestamp() does the reverse ‚Äî converts timestamps to seconds."},
{domain:"ELT with Spark & Python",q:"What does df.pivot('quarter').agg(sum('revenue')) accomplish after groupBy('year')?",opts:["Unpivots quarters into rows","Reshapes data so each distinct quarter value becomes a column with summed revenue","Creates a multi-index DataFrame","Filters data to a specific quarter"],ans:1,exp:"pivot() with groupBy and an aggregation transforms long-format data to wide format. Each unique value in the pivot column becomes a separate output column, filled with the aggregated values."},
{domain:"ELT with Spark & Python",q:"A data engineer writes df.write.mode('overwrite').partitionBy('year','month').format('delta').save(path). Data exists. What happens?",opts:["Raises AnalysisException","Appends new data to existing partitions","Replaces only partitions present in df; other partitions are untouched","Deletes all data and writes only new data"],ans:2,exp:"Delta's dynamic partition overwrite (default behavior with partitionBy + overwrite mode) replaces only the partitions contained in the new DataFrame. Other partitions are not affected, preventing unintended data loss."},
{domain:"ELT with Spark & Python",q:"Which approach efficiently adds a column 'full_name' by concatenating first_name and last_name with a space?",opts:["col('first_name') + ' ' + col('last_name')","concat(col('first_name'), lit(' '), col('last_name'))","col('first_name').concat(col('last_name'))","format_string('%s%s', first_name, last_name)"],ans:1,exp:"concat() is the correct built-in function for concatenating string columns. lit(' ') creates a literal space Column object. Operator + doesn't work the same way on Column objects in PySpark."},
{domain:"ELT with Spark & Python",q:"What does Adaptive Query Execution (AQE) do when spark.sql.adaptive.enabled=true?",opts:["Pre-compiles all SQL to native code","Re-optimizes query plans at runtime based on statistics from completed stages","Auto-partitions data by column cardinality","Enables vectorized execution on GPU"],ans:1,exp:"AQE collects runtime statistics from completed shuffle map stages and uses them to dynamically re-optimize: coalescing too many small shuffle partitions, handling skew joins, and switching join strategies based on actual data sizes."},
{domain:"ELT with Spark & Python",q:"Which Spark API is preferred for structured data processing due to Catalyst optimizer integration?",opts:["RDD API with mapPartitions()","DataFrame/Dataset API","Accumulators with manual aggregation","DStream API"],ans:1,exp:"DataFrame/Dataset API goes through the Catalyst optimizer (predicate pushdown, column pruning, join reordering) and Tungsten (memory management, code generation). RDDs bypass these optimizations entirely."},
{domain:"ELT with Spark & Python",q:"How do you read data from a Delta table into a streaming DataFrame?",opts:["spark.read.format('delta').stream(table)","spark.readStream.format('delta').table('table_name')","spark.read.stream().delta('table_name')","delta.readStream('table_name')"],ans:1,exp:"spark.readStream.format('delta').table() or .load(path) reads a Delta table as a streaming source. The stream incrementally processes new files added to the table since the last checkpoint."},
{domain:"ELT with Spark & Python",q:"A data engineer uses broadcast(lookup_df) in a join. When is this beneficial?",opts:["When the lookup_df has more rows than the fact table","When lookup_df is small enough to fit in executor memory, avoiding a shuffle of the large table","When both DataFrames are too large for memory","When using sort-merge join strategy"],ans:1,exp:"Broadcast join sends the small table to every executor. The large table's partitions can be scanned locally without shuffling. This is most beneficial when one side fits in memory (configurable threshold)."},
{domain:"ELT with Spark & Python",q:"What does coalesce(col('region'), lit('UNKNOWN')) return?",opts:["Concatenates region with UNKNOWN","Returns UNKNOWN only if region is empty string","Returns the first non-null value: region if not null, else UNKNOWN","Always returns UNKNOWN"],ans:2,exp:"coalesce() returns the first non-null argument. If region is not null, it returns region. If null, it returns 'UNKNOWN'. It's the idiomatic Spark way to replace nulls with defaults."},
{domain:"ELT with Spark & Python",q:"A Spark job reads a 10TB Delta table and filters to 5% of data. How can partition pruning help?",opts:["It caches the filtered result in memory","It reads only partitions matching the filter predicate, skipping 95% of data","It splits the query into 100 parallel jobs","It converts the filter to a broadcast join"],ans:1,exp:"If the Delta table is partitioned on the filtered column, Spark reads only matching partitions from cloud storage. Combined with Delta's data skipping (min/max statistics), this can dramatically reduce I/O."},
{domain:"ELT with Spark & Python",q:"Which function checks if a column value is null?",opts:["col('x') == None","col('x').isNull()","col('x').equals(null)","isnull(col('x')) ‚Äî both B and D work"],ans:3,exp:"Both col('x').isNull() (method) and isnull(col('x')) (function) work in PySpark to check for null. col('x') == None uses Python equality which doesn't translate correctly to SQL null semantics."},
{domain:"ELT with Spark & Python",q:"What is a common way to read a table from a SQL query into a DataFrame?",opts:["spark.sql('SELECT ...')","spark.table('SELECT ...')","df.query('SELECT ...')","spark.read.sql('SELECT ...')"],ans:0,exp:"spark.sql() executes any valid SQL and returns a DataFrame. This is the most direct way to run SQL queries, subqueries, CTEs, and window functions within PySpark code."},
{domain:"ELT with Spark & Python",q:"How do you add a column with the current timestamp to a DataFrame?",opts:["df.withColumn('ts', datetime.now())","df.withColumn('ts', current_timestamp())","df.withColumn('ts', now())","df.withColumn('ts', lit(time.time()))"],ans:1,exp:"current_timestamp() is a Spark SQL function that returns the current timestamp as a Column. It's evaluated at query execution time on each executor, not at Python code definition time."},
{domain:"ELT with Spark & Python",q:"A data engineer writes df.select('a', 'b', explode('tags').alias('tag')). What does this produce?",opts:["A column named 'tag' that concatenates all tags","One row per element in the tags array, repeated for each original row's a and b values","An error because explode cannot be used in select","A struct column containing all tags"],ans:1,exp:"explode() in select produces one row per array element. The original row's other columns (a, b) are repeated for each tag value. This is the standard way to 'flatten' array columns in Spark."},
{domain:"ELT with Spark & Python",q:"What is the purpose of the Spark UI's Stages tab?",opts:["Shows running notebooks in the workspace","Displays each stage's task details, shuffle read/write, input/output bytes, and task duration distribution for diagnosing bottlenecks","Lists all DataFrames currently in memory","Shows cluster resource utilization over time"],ans:1,exp:"The Stages tab breaks execution into stages separated by shuffle boundaries. For each stage you can see task counts, input/output sizes, shuffle metrics, and task distribution ‚Äî essential for diagnosing skew, spill, and slow tasks."},
{domain:"ELT with Spark & Python",q:"Which method writes a DataFrame to a Delta table, creating it if it doesn't exist and appending if it does?",opts:["df.write.mode('overwrite').delta(path)","df.write.mode('append').format('delta').save(path)","df.write.saveAsTable('table')","df.insertInto('table')"],ans:1,exp:"mode('append') adds new rows to an existing table without modifying existing data. If the table doesn't exist, it creates it. format('delta').save(path) writes as a Delta table at the specified path."},
{domain:"ELT with Spark & Python",q:"What happens when you call df.cache() and then modify the source data?",opts:["The cache auto-invalidates and re-reads fresh data","The cached version becomes stale ‚Äî subsequent actions use the cached (old) data until you call df.unpersist()","Cache modification raises an error","Spark automatically merges the cache with new data"],ans:1,exp:"Once cached, Spark reuses the stored data until explicitly unpersisted. If source data changes (e.g., new Delta commits), the cache will be stale. Call df.unpersist() to invalidate and re-read fresh data."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to convert a struct column 'address' with fields 'city' and 'zip' into separate columns. Which approach works?",opts:["df.expand('address')","df.select('address.city', 'address.zip') or df.select(col('address.*'))","df.flatten('address')","df.withColumn('city', col('address').city)"],ans:1,exp:"Nested struct fields are accessed with dot notation: col('address.city'). To select all fields from a struct, use col('address.*'). This produces one column per struct field."},

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// DOMAIN 3: Incremental Data Processing (~22%)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{domain:"Incremental Data Processing",q:"Which Delta Lake command physically removes data files no longer referenced by the transaction log?",opts:["OPTIMIZE","VACUUM","ZORDER BY","PURGE"],ans:1,exp:"VACUUM deletes files not referenced by any current or recent Delta version. By default it preserves files newer than 7 days (retention threshold). OPTIMIZE compacts small files but leaves old ones for time travel."},
{domain:"Incremental Data Processing",q:"What is the default Delta Lake time travel retention period before VACUUM can delete old files?",opts:["1 day","7 days","30 days","Indefinitely"],ans:1,exp:"The default retention is 7 days (168 hours), configurable via delta.deletedFileRetentionDuration. VACUUM respects this ‚Äî it won't delete files newer than the threshold to protect active time travel queries."},
{domain:"Incremental Data Processing",q:"What does the following DLT decorator do? @dlt.expect_or_drop('valid_id', 'id IS NOT NULL')",opts:["Raises an exception and stops the pipeline if id is null","Warns on null ids but keeps all rows","Drops rows where id is null and logs the count as a metric","Filters rows before writing to Silver layer"],ans:2,exp:"@dlt.expect_or_drop removes rows violating the constraint and records metrics (expected vs. actual counts). Use @dlt.expect to warn, @dlt.expect_or_fail to halt the pipeline on any violation."},
{domain:"Incremental Data Processing",q:"Which Auto Loader mechanism is more scalable for high-volume cloud storage ingestion?",opts:["Directory listing (default)","File notification via cloud events (SNS/SQS, Event Grid)","JDBC polling of an inventory table","Kafka subscription for file events"],ans:1,exp:"File notification mode uses cloud storage events to detect new files immediately without scanning the directory. It's more scalable and lower-latency than directory listing, especially for buckets with millions of files."},
{domain:"Incremental Data Processing",q:"A streaming job fails after 3 hours. Which mechanism ensures it resumes without reprocessing?",opts:["Kafka consumer group offsets alone","The checkpointLocation specified in writeStream options","Spark driver memory recovery","The Delta transaction log of the output table"],ans:1,exp:"The checkpoint location persists the stream's progress: source offsets and committed output versions. On restart, Spark reads this and resumes exactly where it left off, providing end-to-end exactly-once semantics."},
{domain:"Incremental Data Processing",q:"Which output mode writes only new rows added since the last trigger in Structured Streaming?",opts:["Complete","Append","Update","Delta"],ans:1,exp:"Append mode outputs only new rows added to the result since the last trigger. Complete rewrites the full result table. Update outputs only changed rows. Delta is not a streaming output mode."},
{domain:"Incremental Data Processing",q:"What does withWatermark('event_time', '10 minutes') do?",opts:["Triggers processing every 10 minutes","Sets a threshold: events more than 10 minutes late are dropped from aggregations","Buffers events for 10 minutes before processing","Creates a 10-minute fixed window"],ans:1,exp:"Watermarking bounds late data handling. Events where event_time < (max seen event_time - 10 minutes) are dropped. This allows the engine to finalize window results and clean up state within bounded memory."},
{domain:"Incremental Data Processing",q:"What does trigger(availableNow=True) do?",opts:["Runs the stream continuously","Processes all currently available data and stops ‚Äî batch semantics with streaming guarantees","Triggers one micro-batch every available second","Enables exactly-once processing"],ans:1,exp:"availableNow=True is the modern replacement for once=True. It processes all data available at startup in potentially multiple micro-batches for efficiency, then stops. Checkpointing ensures no data is reprocessed."},
{domain:"Incremental Data Processing",q:"Which Delta table feature allows incremental processing of changes using a streaming source?",opts:["DESCRIBE HISTORY","readStream with startingVersion","Time Travel TIMESTAMP AS OF","VACUUM RETAIN 0 HOURS"],ans:1,exp:"Reading Delta as a streaming source with spark.readStream.format('delta').option('startingVersion', N) picks up all new commits (inserts, updates, deletes if CDF enabled) from version N, enabling incremental CDC processing."},
{domain:"Incremental Data Processing",q:"What does APPLY CHANGES INTO accomplish in Delta Live Tables?",opts:["Compacts small files in the target table","Processes CDC events (inserts/updates/deletes) and applies them to maintain a current-state target table","Computes incremental aggregations","Performs schema evolution"],ans:1,exp:"APPLY CHANGES INTO is DLT's CDC primitive. It processes events with a change type (INSERT/UPDATE/DELETE) from a source and applies them in order to a target table, maintaining the current state correctly. Supports SCD Type 1 and 2."},
{domain:"Incremental Data Processing",q:"In DLT, what is the difference between a STREAMING TABLE and a MATERIALIZED VIEW?",opts:["Streaming tables are recomputed fully each run; materialized views process only new data","Streaming tables process only new records incrementally; materialized views are recomputed fully each run","Both behave identically but streaming tables require Kafka","Materialized views support APPLY CHANGES; streaming tables do not"],ans:1,exp:"STREAMING TABLES use dlt.read_stream() and process only new/unprocessed records incrementally. MATERIALIZED VIEWS use dlt.read() and are fully recomputed each pipeline run. Choose based on source type and latency needs."},
{domain:"Incremental Data Processing",q:"A data engineer needs to deduplicate streaming events within a 1-hour window. Which approach is correct?",opts:["df.dropDuplicates(['event_id'])","df.withWatermark('event_time','1 hour').dropDuplicates(['event_id','event_time'])","df.distinct() in streaming","Not supported in Structured Streaming"],ans:1,exp:"Streaming deduplication requires watermarking to bound the state size (how long to remember seen IDs). Without watermark, the state grows indefinitely. With watermark, IDs older than 1 hour are evicted from state."},
{domain:"Incremental Data Processing",q:"What is the purpose of cloudFiles.schemaLocation in Auto Loader?",opts:["Specifies where to write schema as a Delta table","Persists inferred schema across runs, enabling detection and handling of schema evolution","Defines the target schema that files must match","Points to an external Glue catalog"],ans:1,exp:"schemaLocation persists the inferred schema between runs. When new columns appear in incoming files, Auto Loader detects the change and can evolve the schema (add new columns) based on cloudFiles.schemaEvolutionMode."},
{domain:"Incremental Data Processing",q:"Which DLT table definition uses dlt.read_stream() and creates a streaming table?",opts:["@dlt.view decorated function using dlt.read_stream()","@dlt.table decorated function using dlt.read_stream()","@dlt.materialized_view using dlt.read_stream()","Any function using spark.readStream"],ans:1,exp:"A @dlt.table function that uses dlt.read_stream() creates a STREAMING TABLE. It processes only new data from the source. Using dlt.read() instead would create a MATERIALIZED VIEW that recomputes fully."},
{domain:"Incremental Data Processing",q:"After running OPTIMIZE, what additional step is needed to reclaim storage from old small files?",opts:["ANALYZE TABLE","VACUUM","REFRESH TABLE","DESCRIBE DETAIL"],ans:1,exp:"OPTIMIZE creates new compacted files but marks old small files as deleted in the transaction log (soft delete). VACUUM physically removes these files after the retention period, reclaiming cloud storage space."},
{domain:"Incremental Data Processing",q:"A DLT pipeline is set to DEVELOPMENT mode. Key difference from PRODUCTION mode?",opts:["Development uses more expensive compute","Development reuses the cluster across runs; Production terminates after each run","Development disables expectations","Production limits to 10 tables"],ans:1,exp:"Development mode keeps the cluster alive between pipeline runs for faster iteration (skipping ~5 min cluster startup). Production mode terminates the cluster after each run for cost savings and runs automatic error recovery."},
{domain:"Incremental Data Processing",q:"What is the purpose of MERGE INTO in Delta Lake?",opts:["Merges two Delta tables into one file","Performs upsert: insert new records, update existing ones, and optionally delete ‚Äî in one atomic operation","Appends data and deduplicates based on primary key","Refreshes the table metadata from storage"],ans:1,exp:"MERGE INTO (upsert) is a single atomic operation. It matches source rows against target rows using a condition, then applies different actions (INSERT, UPDATE, DELETE) based on whether a match was found."},
{domain:"Incremental Data Processing",q:"What does enabling Change Data Feed (CDF) on a Delta table provide?",opts:["Automatic streaming of inserts only","Row-level change tracking (insert/update/delete) queryable via table_changes() with a start version","Synchronous writes that block until committed","Automatic MERGE of new data"],ans:1,exp:"CDF writes change records to _change_data directory with _change_type column (insert, update_preimage, update_postimage, delete). Query with spark.read.format('delta').option('readChangeFeed','true').option('startingVersion', N)."},
{domain:"Incremental Data Processing",q:"Which statement about stream-stream joins is TRUE?",opts:["Both streams must have the same partition count","Watermarks must be defined on both streams to bound join state","They always use broadcast join strategy","They require checkpoint location set to MEMORY"],ans:1,exp:"Stream-stream joins buffer events from both sides waiting for matches. Without watermarks on both streams, this state grows indefinitely. Watermarks bound the state and allow late data to be dropped, preventing OOM."},
{domain:"Incremental Data Processing",q:"A data engineer runs OPTIMIZE with ZORDER BY (city, date). What is the expected result?",opts:["Rows sorted globally by city then date","Small files compacted; data co-located by city and date within files, improving data skipping","B-tree index created on city and date","Duplicate rows with same city/date removed"],ans:1,exp:"ZORDER BY performs multi-dimensional locality clustering within compacted files. Files contain smaller ranges of city/date values, allowing Delta's min/max statistics to skip more files when queries filter on those columns."},
{domain:"Incremental Data Processing",q:"Which command reverts a Delta table to a previous version?",opts:["RESTORE TABLE my_table TO VERSION AS OF 10","SELECT * FROM my_table VERSION AS OF 10 INTO my_table","ROLLBACK TABLE my_table VERSION 10","ALTER TABLE my_table RESTORE 10"],ans:0,exp:"RESTORE TABLE ... TO VERSION AS OF N creates a new commit that restores the table to its state at version N. The operation is recorded in the transaction log ‚Äî all version history is preserved."},
{domain:"Incremental Data Processing",q:"What happens to streaming state when a query with watermark processes an event?",opts:["State is stored in HDFS permanently","State is stored in the executor memory/local disk; after watermark delay, expired state is evicted","State is broadcast to all executors every micro-batch","State is stored in the Delta checkpoint only"],ans:1,exp:"Structured Streaming maintains state (e.g., partial aggregations, dedup seen-IDs) in executor memory backed by state store (RocksDB by default in newer Spark). Watermark expiry triggers state eviction, keeping state bounded."},
{domain:"Incremental Data Processing",q:"Which DESCRIBE command shows the complete transaction history of a Delta table?",opts:["DESCRIBE TABLE EXTENDED","DESCRIBE HISTORY my_table","SHOW HISTORY my_table","EXPLAIN my_table"],ans:1,exp:"DESCRIBE HISTORY returns all commits in the Delta transaction log: operation type, timestamp, user, cluster ID, parameters, and metrics. LIMIT N restricts to the N most recent versions."},

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// DOMAIN 4: Production Pipelines (~16%)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{domain:"Production Pipelines",q:"A Databricks Job has tasks A ‚Üí B ‚Üí C. Task B fails. What happens by default?",opts:["All tasks retry automatically","Task C is skipped; job is marked Failed","Task B retries indefinitely","Job sends alert and waits for manual intervention"],ans:1,exp:"When a task fails, downstream dependent tasks are skipped and the job is marked Failed. You configure per-task retries and set up notifications. The job can be restarted from the failed task without rerunning A."},
{domain:"Production Pipelines",q:"Which Databricks feature enables CI/CD for notebooks, jobs, and pipelines using YAML config files in Git?",opts:["Databricks Repos","Databricks Asset Bundles (DABs)","MLflow Projects","Databricks Connect"],ans:1,exp:"DABs define all Databricks resources (jobs, DLT pipelines, serving endpoints) in YAML using the Databricks CLI. Resources are version-controlled in Git and deployed with 'databricks bundle deploy' to any environment."},
{domain:"Production Pipelines",q:"Which Databricks Workflows feature allows Task B to run only if Task A failed?",opts:["Not possible ‚Äî tasks only run after successful dependencies","Set Task B's 'Run if' condition to 'If not succeeded' on Task A","Add Task B as a failure handler in job config","Use try/catch in Task A's notebook"],ans:1,exp:"Databricks Workflows supports conditional execution via 'Run if' conditions: 'If succeeded', 'If failed', 'If not succeeded' (failed or skipped), or 'All done'. This enables cleanup, alerting, or compensation tasks."},
{domain:"Production Pipelines",q:"What is the recommended way to pass sensitive credentials to Databricks jobs?",opts:["Hardcode in notebook and restrict access","Store in Delta table with column encryption","Use Databricks Secrets and access via dbutils.secrets.get()","Store in JSON config file in DBFS"],ans:2,exp:"Databricks Secrets store credentials securely. They're referenced with dbutils.secrets.get(scope, key) and appear as [REDACTED] in output. Supports Databricks-managed secret scopes and Azure Key Vault-backed scopes."},
{domain:"Production Pipelines",q:"A Databricks Job has max_retries=3 and min_retry_interval=60. Task A fails with OOM. What happens?",opts:["Job immediately fails ‚Äî OOM is non-retriable","Task retries up to 3 times with at least 60-second gaps","Cluster is auto-resized and task retried once","Retries only apply to network errors"],ans:1,exp:"Databricks retries apply to any failure unless the job is configured otherwise. With max_retries=3 and min_retry_interval=60s, the task retries up to 3 times. For persistent OOM, also increase cluster memory or optimize the code."},
{domain:"Production Pipelines",q:"Which trigger type in Databricks Jobs starts a job when a new file arrives in cloud storage?",opts:["Cron schedule","File arrival trigger","Webhook trigger","API trigger"],ans:1,exp:"File arrival triggers (for S3, ADLS, GCS) start a job automatically when a new file matching the specified path pattern is detected. This is more responsive and efficient than polling via a frequent cron schedule."},
{domain:"Production Pipelines",q:"What is the best practice for sharing common transformation logic across multiple Databricks workflows?",opts:["Copy-paste logic into each task notebook","Package as a Python wheel and install as cluster library","Use %run to include another notebook","Hardcode logic in a SQL warehouse procedure"],ans:1,exp:"Python wheels provide versioned, testable, reusable code packages. They're installed on clusters and importable in any notebook or job. This follows software engineering best practices better than %run or copy-paste."},
{domain:"Production Pipelines",q:"How can a Databricks Job notify a Slack channel on failure?",opts:["Print Slack webhook URL in the notebook","Configure webhook notification in the Job's notification settings","Create a separate monitoring job polling the Jobs API","Slack notifications are unsupported ‚Äî email only"],ans:1,exp:"Databricks Jobs support email and webhook (Slack, Teams, PagerDuty) notifications for job start, success, failure, and duration warnings. Configure in the Job UI under Notifications without any custom code."},
{domain:"Production Pipelines",q:"What is a key advantage of Delta Live Tables over manually orchestrated Spark notebooks?",opts:["DLT always runs faster than equivalent Spark code","DLT auto-manages orchestration, error recovery, data quality enforcement, and lineage tracking","DLT supports more data sources","DLT pipelines cannot be scheduled"],ans:1,exp:"DLT reduces operational overhead: it handles dependency ordering, retries, cluster lifecycle, data quality metrics, lineage, and automatic recovery. Engineers declare data transformations declaratively; DLT handles execution."},
{domain:"Production Pipelines",q:"A data engineer wants to run a Databricks Job on a fixed schedule every day at 2 AM UTC. Which option is used?",opts:["Cron expression: 0 2 * * *","Schedule: daily at 02:00","TimeInterval: 86400","JobTrigger: DAILY_2AM"],ans:0,exp:"Databricks Jobs use Quartz cron syntax. '0 2 * * *' means 'at 02:00 every day'. Configure in the job's Schedule section using either the UI cron editor or by entering the cron expression directly."},
{domain:"Production Pipelines",q:"In a Databricks workflow, when a task has 'depends_on: [task_a, task_b]', when does it execute?",opts:["Immediately at job start","Only after both task_a AND task_b succeed","After either task_a OR task_b completes first","After task_a regardless of task_b"],ans:1,exp:"Multiple dependencies require ALL listed tasks to complete successfully. This enables DAG-structured workflows: parallel tasks can run simultaneously, converging at a downstream task that waits for all of them."},
{domain:"Production Pipelines",q:"What is init script used for on a Databricks cluster?",opts:["Run a notebook when a user connects","Install OS packages, configure system settings, or install Python libraries at cluster startup before Spark starts","Pre-load frequently used DataFrames into memory","Configure Spark SQL settings for all queries"],ans:1,exp:"Init scripts run on each node during cluster initialization before Spark starts. They install system dependencies, configure environment variables, or install Python packages not available via the cluster libraries UI."},
{domain:"Production Pipelines",q:"Which method in foreachBatch helps achieve idempotent writes even if a micro-batch is retried?",opts:["Using append mode with ignoreDuplicates=True","Using the batchId to detect and skip already-committed batches","Setting spark.streaming.kafka.exactly.once=True","Writing to temp table first then merging"],ans:1,exp:"foreachBatch provides a unique, monotonically increasing batchId. By storing committed batchIds (e.g., in a Delta table) and checking before writing, you detect retried batches and skip them ‚Äî achieving idempotent, exactly-once semantics."},

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// DOMAIN 5: Data Governance (~9%)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{domain:"Data Governance",q:"Minimum privileges for a user to SELECT from a Unity Catalog table?",opts:["SELECT on the table only","USE CATALOG + USE SCHEMA + SELECT on the table","ALL PRIVILEGES on the catalog","DATA_ACCESS on external location + SELECT on table"],ans:1,exp:"Unity Catalog requires all three: USE CATALOG (to enter the catalog), USE SCHEMA (to enter the schema), and SELECT (to read table data). Missing any one results in permission denied."},
{domain:"Data Governance",q:"Which Unity Catalog object stores credentials for accessing cloud storage?",opts:["External Table","Storage Credential","External Location","Metastore Admin Role"],ans:1,exp:"Storage Credentials hold cloud IAM credentials (AWS IAM role ARN, Azure Service Principal, GCP service account). External Locations reference a Storage Credential to define accessible cloud storage paths with fine-grained access control."},
{domain:"Data Governance",q:"Which feature automatically masks credit card numbers for non-privileged users in Unity Catalog?",opts:["Row Filters","Column Masks","Dynamic Views","Table Tags"],ans:1,exp:"Column Masks attach a SQL masking function to a column. When queried, the function is applied per-user at the engine level. Privileged users see real values; others see masked output. No table duplication needed."},
{domain:"Data Governance",q:"What does Unity Catalog lineage automatically capture?",opts:["Only table-to-table lineage within one notebook","Column-level lineage across notebooks, jobs, and DLT pipelines across workspaces","Only within a single workspace","Only for Delta tables, not views"],ans:1,exp:"Unity Catalog captures fine-grained column-level lineage automatically. It tracks data flow from source columns through transformations to target columns across notebooks, SQL queries, jobs, and DLT pipelines ‚Äî even cross-workspace."},
{domain:"Data Governance",q:"What is the correct GDPR right-to-erasure approach for a Delta table?",opts:["DROP TABLE and recreate excluding the customer","DELETE FROM table WHERE customer_id='X' followed by VACUUM after retention period","VACUUM RETAIN 0 HOURS immediately","Archive to cold storage and remove reference"],ans:1,exp:"DELETE removes the record from current reads immediately (logical delete). VACUUM after the retention period physically removes old Parquet files containing the record. This completes the erasure at the storage level."},
{domain:"Data Governance",q:"A Unity Catalog Row Filter is applied to 'sales'. What happens when a user queries a VIEW built on 'sales'?",opts:["View bypasses the filter ‚Äî returns all rows","Row filter is applied through the view, enforcing restriction at table level","View owner's permissions determine visible rows","Row filters only apply to direct table queries"],ans:1,exp:"Unity Catalog enforces Row Filters at the table level transparently for all access patterns ‚Äî direct queries, views, subqueries, joins, and DLT pipelines. The querying user's filter is always applied, regardless of the view definition."},
{domain:"Data Governance",q:"What do table tags in Unity Catalog provide?",opts:["Automatic PII column encryption","Searchable metadata labels for classification and discovery without affecting query behavior","Restricting SELECT to users with matching tag attributes","Triggering automatic VACUUM when PII tag is set"],ans:1,exp:"Tags are key-value metadata labels applied to catalogs, schemas, tables, and columns. They enable data classification (PII, PCI, confidential), asset discovery, and governance workflows. They have no effect on query execution."},
{domain:"Data Governance",q:"Which privilege allows creating tables in a schema but NOT reading existing tables?",opts:["SELECT","MODIFY","CREATE TABLE","USE SCHEMA"],ans:2,exp:"CREATE TABLE grants permission to create new tables in a schema. It doesn't grant SELECT (read), MODIFY (write), or any access to existing tables. Unity Catalog uses fine-grained, additive privilege model."},
{domain:"Data Governance",q:"What is a Unity Catalog External Location?",opts:["A table stored outside of Databricks","A mapping of a cloud storage path to a Storage Credential, enabling governed access to files at that path","A connection to an external database via JDBC","An auto-loader configuration for cloud files"],ans:1,exp:"External Locations combine a cloud storage URL prefix with a Storage Credential. Users/groups granted CREATE EXTERNAL TABLE or READ FILES on the External Location can work with data at that path within Unity Catalog's governance framework."},
{domain:"Data Governance",q:"How does Unity Catalog handle data access across multiple Databricks workspaces?",opts:["Each workspace has a separate independent metastore with no sharing","A single Unity Catalog metastore is shared across workspaces, providing centralized governance and consistent permissions","Workspaces share data via Delta Sharing only","Cross-workspace access requires VPC peering"],ans:1,exp:"Unity Catalog's architecture is one metastore per region shared across all workspaces. Permissions, lineage, auditing, and data sharing are centrally managed, providing consistent governance across teams and workspaces."},

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// MORE DELTA LAKE / ADVANCED QUESTIONS
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{domain:"Incremental Data Processing",q:"Which Delta Lake table property enables automatic data compaction during writes?",opts:["delta.autoOptimize.optimizeWrite = true","delta.optimize.enabled = true","delta.autoCompact = true","delta.write.optimize = true"],ans:0,exp:"delta.autoOptimize.optimizeWrite=true enables Optimized Writes, which coalesces small files into larger files (targeting ~128MB) during writes. This reduces the small file problem without requiring manual OPTIMIZE runs."},
{domain:"Incremental Data Processing",q:"A Delta table's DESCRIBE DETAIL shows numFiles=10,000 and sizeInBytes=500MB. What does this indicate?",opts:["The table is too large and needs archiving","There are 10,000 small files averaging 50KB each ‚Äî a small file problem requiring OPTIMIZE","The table has 10,000 partitions","The Delta log has 10,000 transactions"],ans:1,exp:"10,000 files averaging 50KB is a severe small file problem. Small files increase metadata overhead, slow query planning, and create excessive cloud storage API calls. Running OPTIMIZE (and optionally Z-ORDER) will compact these files."},
{domain:"ELT with Spark & Python",q:"What does the following code do?\n\ndf.write.option('mergeSchema','true').mode('append').format('delta').save(path)",opts:["Overwrites data and adds any new columns to the schema","Appends data and automatically adds any new columns present in df to the target schema","Raises SchemaEvolutionException","Drops columns in target not present in df"],ans:1,exp:"mergeSchema=true enables schema evolution on write. New columns in the DataFrame are automatically added to the target Delta table schema. Existing columns not in df are set to null for newly written rows."},
{domain:"ELT with Spark & Python",q:"How do you register a Python UDF for use in Spark SQL?",opts:["spark.udf.register('my_udf', my_func, returnType=StringType())","@udf decorator on the function only","spark.sql.registerFunction('my_udf', my_func)","spark.createUDF('my_udf', my_func)"],ans:0,exp:"spark.udf.register() makes a Python function available as a SQL UDF by name. After registration, you can call it in spark.sql() queries. @udf creates a Column-API UDF but doesn't register it by name for SQL."},
{domain:"Lakehouse Platform",q:"What is Databricks Runtime (DBR)?",opts:["The cluster management layer that schedules tasks","An optimized version of Apache Spark bundled with libraries, optimizations (Photon-ready), and Databricks-specific features","The UI layer for notebooks and workflows","A specialized runtime for ML training only"],ans:1,exp:"Databricks Runtime is a pre-built Spark distribution with tuned configurations, Delta Lake, MLflow, and commonly used libraries. Different DBR versions include specific Spark versions, Python, and optional ML libraries."},
{domain:"ELT with Spark & Python",q:"A data engineer uses spark.read.csv(path) without specifying a schema. All columns come in as StringType. How should this be fixed for production?",opts:["Add .option('inferSchema','true')","Define an explicit StructType schema and pass it with .schema(schema)","Use df.cast() after reading","Enable spark.sql.csv.autoSchemaDetect"],ans:1,exp:"Defining an explicit StructType schema guarantees correct types, avoids the full-file scan of inferSchema, and fails fast if the source data doesn't match expectations ‚Äî all critical for production data pipelines."},
{domain:"Production Pipelines",q:"Which Databricks Jobs feature enables a single parameterized job to run with different configurations (e.g., different tables or dates)?",opts:["Job Templates","Job Parameters / Widget values passed to notebooks","Task Libraries","Cluster Policies"],ans:1,exp:"Databricks Jobs supports passing widget parameters to notebooks. The job definition includes parameter key-value pairs; the notebook reads them with dbutils.widgets.get(). This enables one job definition for many use cases."},
{domain:"Incremental Data Processing",q:"What is the purpose of cloudFiles.schemaEvolutionMode = 'addNewColumns' in Auto Loader?",opts:["Drops columns not present in incoming files","Automatically adds new columns detected in incoming files to the inferred schema","Enforces strict schema ‚Äî rejects files with new columns","Renames columns to match existing schema"],ans:1,exp:"addNewColumns mode evolves the schema by adding newly discovered columns from incoming files. The schemaLocation persists this across runs. Rows from older files get null for the new columns."},
{domain:"Data Governance",q:"A data engineer creates a view that filters rows: CREATE VIEW my_view AS SELECT * FROM sales WHERE region = current_user(). What is this an example of?",opts:["A Materialized View","A Dynamic View implementing row-level security based on the current user","A Row Filter in Unity Catalog","A Column Mask"],ans:1,exp:"Dynamic Views use functions like current_user() or is_member() in WHERE clauses to implement row-level security based on the querying user's identity. Unity Catalog Row Filters achieve the same but are applied at the table level without needing a view."},
{domain:"ELT with Spark & Python",q:"Which built-in Spark function computes the number of days between two date columns?",opts:["date_sub()","datediff()","date_diff()","timestampdiff()"],ans:1,exp:"datediff(end_date, start_date) returns the number of days between two date expressions. date_sub() subtracts a fixed number of days from a date. These are built-in Spark SQL functions optimized by the Catalyst optimizer."},
{domain:"ELT with Spark & Python",q:"What does spark.range(1000).repartition(10).mapPartitions(process_partition).count() guarantee?",opts:["process_partition is called once for all 1000 rows","process_partition is called exactly 10 times, once per partition","process_partition is called 1000 times, once per row","The count returns 10"],ans:1,exp:"mapPartitions() calls the function once per partition, passing an iterator of all rows in that partition. With 10 partitions, it's called 10 times. This is more efficient than map() for expensive per-partition setup (e.g., DB connections)."},
{domain:"Lakehouse Platform",q:"Which statement about cluster policies in Databricks is TRUE?",opts:["Cluster policies allow users to create any cluster configuration","Cluster policies restrict cluster configurations to approved options, enforcing cost controls and compliance","Cluster policies are applied after cluster creation to resize","Cluster policies require admin approval for each cluster creation"],ans:1,exp:"Cluster policies define allowed configurations (instance types, max DBUs, auto-termination) that users can choose from. They prevent over-provisioning, enforce tagging for cost attribution, and ensure compliance with organizational standards."},
{domain:"Production Pipelines",q:"A data engineer wants to monitor average query duration for SQL Warehouse queries over time. Where is this data available?",opts:["Spark UI stages tab","SQL Warehouse Query History and System Tables (system.query.history)","DBFS audit logs","Cluster event log"],ans:1,exp:"SQL Warehouse Query History in the Databricks UI shows all executed queries with duration, status, and cluster details. System Tables (system.query.history) provide programmatic access for building custom monitoring dashboards."},
{domain:"ELT with Spark & Python",q:"What is the difference between df.filter() and df.where()?",opts:["filter() is for DataFrames; where() is for SQL only","They are aliases ‚Äî both filter rows based on a condition; where() is the SQL-style alias for filter()","filter() evaluates lazily; where() evaluates eagerly","where() supports only column conditions; filter() also supports Python expressions"],ans:1,exp:"df.filter() and df.where() are exact aliases in PySpark. Both accept Column expressions, SQL string conditions, or Python boolean expressions. There is no functional difference ‚Äî use whichever is more readable."},
{domain:"Incremental Data Processing",q:"A streaming query writes to a Delta table. What happens if the same checkpoint is used for two different streaming queries?",opts:["Both queries share progress tracking efficiently","The second query will fail or produce incorrect results ‚Äî each query must have a unique checkpoint location","Spark automatically creates a sub-directory per query","The checkpoint is overwritten by the most recently started query"],ans:1,exp:"Checkpoint locations are tied to a specific query and track its exact read position and state. Using the same checkpoint for two different queries causes conflicts, incorrect progress tracking, and potential data duplication or loss."},
{domain:"ELT with Spark & Python",q:"Which approach should be used to handle a schema with deeply nested JSON (multiple levels of arrays and structs)?",opts:["Use inferSchema and cast all columns afterward","Use schema_of_json() to detect the schema, then from_json() with an explicit schema to parse","Read as text and process with Python json library on the driver","Use flatten() to eliminate all nested structures first"],ans:1,exp:"schema_of_json() can infer JSON schema from a sample. from_json() then parses the JSON string column using an explicit schema, producing typed struct/array columns that can be navigated with dot notation and explode()."},
{domain:"Data Governance",q:"What is the purpose of Unity Catalog's audit log?",opts:["Stores query results for faster repeat access","Records all data access events (who accessed what, when, from where) for compliance and security monitoring","Tracks Delta table versions and schema changes","Logs cluster startup and shutdown events only"],ans:1,exp:"Unity Catalog audit logs capture every access event: user identity, accessed object (catalog/schema/table/column), operation (SELECT, INSERT, etc.), timestamp, IP, and workspace. This is critical for GDPR, HIPAA, and SOC2 compliance."},
{domain:"ELT with Spark & Python",q:"A data engineer chains multiple withColumn() calls. What is a better approach for adding many columns at once?",opts:["Use a for loop with withColumn()","Use select() with all column expressions including the new ones, or use a single withColumns() call (Spark 3.3+)","Use createDataFrame() with the new columns","Use registerTempView() and a SQL statement"],ans:1,exp:"Multiple withColumn() calls each create a new plan node, which can slow down plan analysis for many columns. Using select() with all columns at once (or withColumns() dict in Spark 3.3+) produces a more efficient plan."},
{domain:"Lakehouse Platform",q:"What does auto-termination on an all-purpose cluster do?",opts:["Automatically upgrades the cluster runtime","Terminates the cluster after a specified period of inactivity, saving cost","Restarts the cluster when a new notebook attaches","Terminates the cluster after a fixed wall-clock time regardless of activity"],ans:1,exp:"Auto-termination terminates an idle all-purpose cluster after the configured inactivity period (default often 120 minutes). This prevents clusters from running overnight when forgotten, saving significant cost."},
{domain:"ELT with Spark & Python",q:"What is a Stage in Spark execution?",opts:["A single task running on one executor","A set of transformations that can run without a shuffle, separated from other stages by shuffle boundaries","A complete job from read to write","A single node's work in the cluster"],ans:1,exp:"Stages are bounded by shuffle operations. Within a stage, transformations execute in a pipelined fashion with data flowing from one task to the next. A shuffle creates a new stage boundary, outputting data to intermediate storage before the next stage reads it."},
{domain:"Production Pipelines",q:"Which approach allows a Databricks notebook to accept parameters when run as part of a Job?",opts:["Environment variables set at cluster level","dbutils.widgets ‚Äî widgets defined in the notebook can receive Job parameters","Hardcoded variables that are overridden by Job configuration","sys.argv command-line arguments"],ans:1,exp:"Databricks Widgets (dbutils.widgets.get/text/dropdown) receive parameters passed by Jobs, Repos CI/CD, or %run calls. In Job configuration, you specify key-value parameter pairs that map to widget names in the notebook."},
{domain:"Incremental Data Processing",q:"What does the following achieve?\n\ndf.writeStream.trigger(processingTime='30 seconds').start()",opts:["Processes data in real-time with no delay","Triggers one micro-batch every 30 seconds regardless of data availability","Processes each event individually with 30-second timeout","Delays stream start by 30 seconds"],ans:1,exp:"processingTime trigger sets a fixed interval between micro-batches. The engine waits at least 30 seconds between batch starts. If a batch takes longer than 30 seconds, the next starts immediately after completion."},
{domain:"ELT with Spark & Python",q:"What is the result of df.groupBy('dept').agg(collect_list('name'))?",opts:["One row per dept with a count of employees","One row per dept with an array of all employee names in that dept","A pivot table of departments vs names","One row per name with the dept as a separate column"],ans:1,exp:"collect_list() aggregates all non-null values of a column within each group into an array. The result has one row per unique dept value, with the names column containing all employees in that dept as an array."},
{domain:"Data Governance",q:"A data engineer needs to ensure that a column 'email' is always masked for users not in the 'data_scientists' group. Which Unity Catalog feature handles this?",opts:["Row Filter with is_member() check","Column Mask with a function that checks is_member('data_scientists')","Dynamic View with CASE WHEN","Table ACL with column-level DENY"],ans:1,exp:"A Column Mask function can use is_member('group_name') or current_user() to conditionally return the real value or a masked value based on the querying user's group membership. Applied transparently to all access patterns."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to read multiple CSV files from a directory where new files are added daily. Which approach handles both existing and future files?",opts:["spark.read.csv(path) ‚Äî re-run daily","spark.readStream.format('cloudFiles').option('cloudFiles.format','csv').load(path) with Auto Loader","spark.read.csv(f'{path}/{today}/*')","Schedule a job that moves files and re-reads all"],ans:1,exp:"Auto Loader (cloudFiles) tracks which files have been processed using a checkpoint. It processes existing files on first run, then only new files on subsequent runs ‚Äî perfect for incrementally growing directories."},
{domain:"Lakehouse Platform",q:"What is the maximum number of Databricks clusters that can share a single Unity Catalog metastore?",opts:["1 per cluster type","Limited only by the number of workspaces in the region","100 clusters maximum","Depends on the Databricks plan"],ans:1,exp:"Unity Catalog's metastore is a regional service shared by all workspaces and all clusters within those workspaces in the same region. There's no cluster-count limit ‚Äî it scales to thousands of clusters accessing the same metastore."},
{domain:"Production Pipelines",q:"What is the purpose of job cluster libraries (cluster-scoped) vs. notebook-scoped libraries?",opts:["No difference ‚Äî libraries are always cluster-scoped","Cluster libraries are available to all notebooks on the cluster; notebook libraries (%pip install) are only available in that notebook's session and restart when session ends","Notebook libraries persist across cluster restarts","Cluster libraries are for Python only; notebook libraries support all languages"],ans:1,exp:"Cluster-scoped libraries (installed via UI, init scripts, or Repos requirements.txt) are available to all users on the cluster. %pip install in a notebook installs for that session only and is lost on detach/reattach."},
{domain:"Incremental Data Processing",q:"Which statement about Delta Lake schema enforcement is TRUE?",opts:["Delta allows any schema to be written by default","Delta rejects writes with columns not in the existing table schema (schema on write)","Schema enforcement applies only to streaming writes","You must drop and recreate to change schema"],ans:1,exp:"Delta Lake enforces schema on write. Writes with extra columns or incompatible types are rejected. To add new columns, use mergeSchema=true on write or ALTER TABLE ADD COLUMN. To allow type widening, use schema evolution settings."},
{domain:"ELT with Spark & Python",q:"How do you read only the most recent partition of a Delta table partitioned by date?",opts:["spark.read.format('delta').option('latestPartition','true').load(path)","spark.read.format('delta').load(path).filter(col('date') == spark.sql('SELECT MAX(date) FROM my_table').collect()[0][0])","Both A and a direct SQL query MAX subquery work; no special Auto Loader option exists for this","Use DESCRIBE HISTORY to find latest version then read that version"],ans:1,exp:"There's no built-in 'latest partition' option. The standard approach is to read the table and filter to the max date ‚Äî either by computing max date first or using a SQL subquery. Partition pruning will then only scan that partition."},
{domain:"Lakehouse Platform",q:"A data engineer wants to use a SQL Warehouse for BI queries. What feature reduces query latency for repeated queries?",opts:["Photon vectorized engine","Result cache ‚Äî identical queries return cached results without cluster computation","Auto-scaling workers","Delta data skipping"],ans:1,exp:"SQL Warehouses have a result cache that stores query results for identical queries. If the underlying data hasn't changed and the query is the same, subsequent executions return cached results in milliseconds with no cluster compute."},
{domain:"ELT with Spark & Python",q:"What happens if you call df.collect() on a 100GB DataFrame?",opts:["Spark automatically samples 1% for the driver","All 100GB is transferred to the driver memory ‚Äî likely causing an OOM error on the driver","Collect() automatically writes to disk instead of memory","An error is raised if the DataFrame exceeds 1GB"],ans:1,exp:"collect() brings all data to the driver node as a Python list. For large DataFrames this causes driver OOM. Use show() for sampling, write() to save results, or take(n) for small previews. Reserve collect() for small DataFrames only."},
{domain:"Data Governance",q:"In Unity Catalog, which privilege grants a user the ability to read and write files in an External Location but not create tables?",opts:["READ FILES only","READ FILES + WRITE FILES","CREATE EXTERNAL TABLE","ALL PRIVILEGES on the External Location"],ans:1,exp:"READ FILES and WRITE FILES are file-level privileges on External Locations for reading/writing raw files without table creation. CREATE EXTERNAL TABLE allows creating table metadata pointing to files at the location."},
{domain:"Production Pipelines",q:"A DLT pipeline processes data in scheduled mode. What triggers a pipeline run?",opts:["Any write to the source Delta table","The configured schedule (cron) or manual trigger","A new Databricks cluster being started","A new notebook being attached to the cluster"],ans:1,exp:"DLT pipelines in scheduled mode run on the configured cron schedule or when manually triggered. Continuous mode runs the pipeline 24/7, processing new data as it arrives. Triggered mode is for scheduled batch pipelines."},
{domain:"ELT with Spark & Python",q:"Which statement about Spark's Catalyst optimizer is TRUE?",opts:["Catalyst only optimizes SQL queries ‚Äî DataFrame API bypasses it","Catalyst optimizes both SQL and DataFrame API operations through logical plan analysis and physical plan selection","Catalyst requires hints to optimize join order","Catalyst is only available in Scala, not PySpark"],ans:1,exp:"Catalyst works on the unified logical plan regardless of whether code uses SQL or DataFrame API ‚Äî both compile to the same logical plan. Catalyst applies rule-based and cost-based optimizations to produce an efficient physical plan."},
{domain:"Incremental Data Processing",q:"What is the purpose of Delta Lake checkpoints in the _delta_log directory?",opts:["Store query results for caching","Provide a compact summary of the transaction log at regular intervals to speed up log replay on table open","Mark files for VACUUM deletion","Track schema version history"],ans:1,exp:"Checkpoints (every 10 commits by default) are Parquet files summarizing all active files, statistics, and schema at that point. When opening a table, Spark reads the latest checkpoint + only subsequent JSON files, avoiding replaying the entire log."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to find all rows where a string column contains the word 'error'. Which expression is correct?",opts:["col('msg') == 'error'","col('msg').contains('error')","col('msg').like('error')","col('msg').startswith('error')"],ans:1,exp:"contains() checks if the string contains the substring anywhere. like() uses SQL LIKE pattern matching with % wildcards. startswith/endswith check position-specific matches. For case-insensitive search, use lower(col('msg')).contains('error')."},
{domain:"Lakehouse Platform",q:"What is the benefit of using instance pools in Databricks?",opts:["Instance pools share memory between clusters","Pre-allocated idle instances reduce cluster start time by keeping VMs ready","Instance pools automatically select the cheapest instance type","Instance pools enable multi-cloud deployment"],ans:1,exp:"Instance pools maintain a set of pre-warmed idle VM instances. When a cluster is created using a pool, it acquires instances from the pool (already booted) instead of provisioning from scratch, reducing start time from minutes to seconds."},
{domain:"ELT with Spark & Python",q:"What is the result of spark.sql('SHOW TABLES IN my_schema')?",opts:["Returns column metadata for all tables","Returns a DataFrame listing all tables in my_schema with their names and types","Drops all temporary views in the schema","Returns the DDL for each table"],ans:1,exp:"SHOW TABLES IN schema returns a DataFrame with columns: namespace, tableName, isTemporary. It lists all registered tables and views in the specified schema, including both Delta tables and views."},
{domain:"Production Pipelines",q:"Which Databricks feature provides automated ML experiment tracking including parameters, metrics, and artifacts?",opts:["Databricks Repos","MLflow Tracking","Unity Catalog","Databricks AutoML"],ans:1,exp:"MLflow Tracking (integrated in Databricks) automatically logs parameters, metrics, and model artifacts. mlflow.autolog() enables automatic logging for popular frameworks. The MLflow UI in Databricks shows all experiments and runs."},
{domain:"Data Governance",q:"A data engineer creates a table with sensitive columns and wants lineage to show what transformed those columns. Which Databricks feature provides this?",opts:["Delta DESCRIBE HISTORY","Unity Catalog automated column-level lineage","MLflow artifact tracking","Spark UI event log"],ans:1,exp:"Unity Catalog automatically captures column-level lineage for queries run through Databricks. It shows which source columns feed into which target columns across notebooks, jobs, and DLT pipelines without any manual instrumentation."},
{domain:"ELT with Spark & Python",q:"What does the following return? spark.read.format('delta').option('versionAsOf','5').load(path)",opts:["The 5 most recent versions merged","The Delta table as it was at version 5 (time travel)","The 5th partition of the Delta table","An error ‚Äî versionAsOf is not a valid option"],ans:1,exp:"versionAsOf enables Delta time travel by version number. This returns the table state exactly as it was when commit 5 was written. Equivalent to SQL: SELECT * FROM delta.`path` VERSION AS OF 5."},
{domain:"Lakehouse Platform",q:"What is a Databricks SQL Dashboard?",opts:["A cluster monitoring interface","A collection of visualizations built on SQL queries executed against SQL Warehouses for BI reporting","A notebook with widgets for interactive analysis","An ML model monitoring interface"],ans:1,exp:"Databricks SQL Dashboards combine multiple visualizations (charts, tables, counters) built from SQL queries. They auto-refresh, support parameters via widgets, and can be shared with stakeholders ‚Äî providing a BI layer on Lakehouse data."},
{domain:"ELT with Spark & Python",q:"A data engineer notices that after reading a Parquet file, many tasks are created (e.g., 2000) for a small file (100MB). What is the likely cause and fix?",opts:["Parquet has 2000 row groups ‚Äî use repartition(20) after read","The default parallelism is too high ‚Äî use coalesce(20) after read to reduce tasks","Too many executors ‚Äî reduce cluster size","2000 tasks is correct for 100MB Parquet files"],ans:1,exp:"Parquet files are split into row groups, each becoming a Spark task. If the file has many small row groups (e.g., due to being written with too many partitions), you get many small tasks. coalesce(N) after reading reduces partitions without a full shuffle."},
{domain:"Incremental Data Processing",q:"What is the effect of setting delta.targetFileSize = 134217728 (128MB) on a Delta table?",opts:["VACUUM will only delete files larger than 128MB","OPTIMIZE will target 128MB file sizes when compacting","New writes will split files at 128MB boundaries","Auto-compaction triggers when files exceed 128MB"],ans:1,exp:"delta.targetFileSize configures the target file size for OPTIMIZE. Files are compacted toward this target (default ~1GB). Setting it lower (128MB) produces more, smaller files ‚Äî useful for tables with many concurrent readers that benefit from smaller files."},
{domain:"ELT with Spark & Python",q:"How do you write a Spark DataFrame to a specific Delta table partition only (e.g., only date='2024-01-01')?",opts:["df.write.format('delta').partitionOverwrite(col('date') == '2024-01-01').save(path)","df.filter(col('date') == '2024-01-01').write.mode('overwrite').format('delta').option('replaceWhere','date = \"2024-01-01\"').save(path)","df.write.format('delta').where('date=\"2024-01-01\"').save(path)","df.write.partition('date=2024-01-01').format('delta').save(path)"],ans:1,exp:"replaceWhere allows atomic replacement of specific data ranges in a Delta table. The engine verifies that all new data satisfies the replaceWhere predicate and replaces only matching data, preserving all other partitions."},

// ‚îÄ‚îÄ ADDITIONAL QUESTIONS TO REACH 380+ ‚îÄ‚îÄ
{domain:"Lakehouse Platform",q:"What does the term 'open format' mean in the context of the Databricks Lakehouse?",opts:["The data can be read by anyone on the internet without authentication","Data is stored in open, non-proprietary formats (Parquet, Delta) that can be read by any engine (Spark, Presto, Hive, Pandas)","The metadata schema is publicly documented","Databricks open-sources all its platform code"],ans:1,exp:"Open formats mean data isn't locked in proprietary formats. Delta, Parquet, and ORC can be read by Spark, Presto, Hive, Trino, and even Pandas directly. This prevents vendor lock-in and enables multi-engine architectures."},
{domain:"ELT with Spark & Python",q:"Which Spark function computes the standard deviation of a numeric column?",opts:["variance()","std()","stddev()","Both stddev() and std() are valid aliases"],ans:3,exp:"Both stddev() and std() compute the sample standard deviation. stddev_pop() computes population standard deviation. variance() computes sample variance. These are all built-in aggregate functions in Spark SQL."},
{domain:"Incremental Data Processing",q:"What is the purpose of the readChangeFeed option when reading a Delta table as a streaming source?",opts:["Reads only inserts, ignoring updates and deletes","Enables reading the Change Data Feed to get row-level changes (inserts/updates/deletes) incrementally","Reads the table feed in reverse order","Enables schema evolution during streaming reads"],ans:1,exp:"readChangeFeed=true with startingVersion reads row-level changes from the CDF (_change_data directory). Each row has a _change_type column (insert, update_preimage, update_postimage, delete). Requires CDF to be enabled on the table."},
{domain:"Production Pipelines",q:"A data engineer needs to test a notebook with different parameter values before productionizing it. Which Databricks tool is appropriate?",opts:["Run > Run All Cells with different widget values","Databricks Experiments","dbutils.notebook.run() called programmatically with different parameter dictionaries","Both A and C are valid testing approaches"],ans:3,exp:"For iterative testing, changing widget values manually (A) works for interactive testing. For automated testing of multiple parameter combinations, dbutils.notebook.run() (C) programmatically runs the notebook with different parameters and captures output."},
{domain:"ELT with Spark & Python",q:"What does from pyspark.sql.functions import * import?",opts:["All Python standard library functions","All built-in Spark SQL functions (col, lit, when, count, avg, etc.) into the current namespace","Only string manipulation functions","The Spark SQL execution engine"],ans:1,exp:"pyspark.sql.functions contains all built-in Spark SQL functions ‚Äî aggregations (sum, avg, count), string (concat, upper), date (date_add, datediff), mathematical (round, log), and many more. Wildcard import brings all into scope."},
{domain:"Data Governance",q:"Which system table in Unity Catalog provides query audit information programmatically?",opts:["system.access.audit","system.query.history","system.operational_data.queries","databricks.audit.query_log"],ans:1,exp:"system.query.history (and system.access.audit for access events) provides programmatic access to query execution history including user, query text, duration, warehouse, and status. Use for building monitoring dashboards and compliance reports."},
{domain:"Lakehouse Platform",q:"A workspace admin wants to enforce that all new clusters must use an auto-termination of 30 minutes. Which feature enforces this?",opts:["Workspace configuration file","Cluster Policy with autoTerminationMinutes fixed at 30","Cluster Tags with key 'terminate' = '30'","Organization-wide Databricks setting"],ans:1,exp:"Cluster Policies can fix specific cluster settings (autoTerminationMinutes, instance types, DBR version) to required values. Admins create policies and assign them to users/groups, ensuring organizational standards are enforced."},
{domain:"ELT with Spark & Python",q:"A data engineer writes spark.sql('CACHE TABLE my_table'). How is this different from df.cache()?",opts:["CACHE TABLE stores data in HDFS; df.cache() uses memory only","CACHE TABLE caches the table eagerly in memory across all sessions; df.cache() is lazy and session-scoped","They are identical","CACHE TABLE works only for Delta tables"],ans:1,exp:"CACHE TABLE eagerly materializes and caches a table in Spark SQL's in-memory columnar cache, accessible across sessions using the table name. df.cache() lazily caches a specific DataFrame reference within the current session only."},
{domain:"Incremental Data Processing",q:"What happens if a streaming job's checkpoint directory is accidentally deleted?",opts:["The stream resumes from where it left off using the output table","The stream restarts from the beginning of the source, potentially reprocessing all historical data","Spark reconstructs the checkpoint from the executor logs","The stream fails permanently and cannot be restarted"],ans:1,exp:"Without the checkpoint, Spark loses all progress information (source offsets). Depending on source configuration, it may restart from the earliest available offset (Kafka) or the beginning of the Delta table ‚Äî potentially causing duplicate processing."},
{domain:"ELT with Spark & Python",q:"What does the following produce? df.select(when(col('age') >= 18, 'adult').when(col('age') >= 13, 'teen').otherwise('child').alias('age_group'))",opts:["Error: when() requires a separate filter call","A new column 'age_group' with values 'adult', 'teen', or 'child' based on age","A filtered DataFrame with only one age group","Three separate DataFrames"],ans:1,exp:"when().when().otherwise() is Spark's equivalent of SQL CASE WHEN. Conditions are evaluated in order: first match wins. otherwise() provides the default. alias() names the resulting column. This is a standard conditional column transformation."},
{domain:"Production Pipelines",q:"A data engineer wants to store ML model artifacts alongside experiment metadata. Which Databricks feature handles both?",opts:["Delta Lake with binary columns for artifacts","MLflow ‚Äî experiment tracking for metadata and Model Registry for versioned model artifacts","Unity Catalog only","Databricks Feature Store"],ans:1,exp:"MLflow provides end-to-end ML lifecycle: Tracking records parameters/metrics/artifacts per run, and the Model Registry provides versioned model management with staging/production transitions and lineage to training experiments."},
{domain:"Lakehouse Platform",q:"What is the benefit of serverless compute in Databricks SQL?",opts:["Unlimited query parallelism with no configuration","Instant startup (no cluster provisioning), automatic scaling, and pay-per-query pricing","Free SQL queries up to 100GB/month","Dedicated hardware not shared with other customers"],ans:1,exp:"Serverless SQL Warehouses eliminate cluster management ‚Äî compute starts instantly (pre-warmed infrastructure), scales automatically to query demand, and you pay only for queries executed. No idle cluster costs."},
{domain:"ELT with Spark & Python",q:"A data engineer wants to apply different aggregations to different columns in one groupBy. Which approach is most efficient?",opts:["Multiple separate groupBy().agg() calls combined with join","df.groupBy('dept').agg(sum('salary'), avg('age'), count('*'), max('hire_date'))","A UDF that applies all aggregations at once","Pivot on all aggregation columns"],ans:1,exp:"Multiple aggregations can be passed to a single .agg() call. This computes all aggregations in a single scan with one shuffle ‚Äî far more efficient than separate groupBy calls which each require a separate full shuffle."},
{domain:"Data Governance",q:"A company needs to know which jobs changed a critical Delta table last week. Which feature provides this information?",opts:["Delta DESCRIBE HISTORY ‚Äî shows operations, timestamps, and user/job info for each commit","Unity Catalog lineage ‚Äî shows table-level data flow","Spark UI ‚Äî shows all recent queries","DBFS audit log"],ans:0,exp:"DESCRIBE HISTORY returns the full audit trail for a Delta table: each row shows the commit version, timestamp, operation (WRITE, MERGE, DELETE), user, notebook, job ID, and cluster info ‚Äî exactly what's needed for audit investigations."},
{domain:"ELT with Spark & Python",q:"How do you efficiently sample 10% of a large DataFrame without replacement?",opts:["df.limit(df.count() * 0.1)","df.sample(fraction=0.1, withReplacement=False)","df.head(int(df.count() * 0.1))","df.randomSplit([0.1, 0.9])[0]"],ans:1,exp:"df.sample(fraction, withReplacement, seed) applies reservoir sampling. It's a distributed operation ‚Äî each partition independently samples approximately 10% of its rows. Much more efficient than count() + limit() which scans all data twice."},
{domain:"Incremental Data Processing",q:"A DLT pipeline fails due to a data quality expectation violation with @dlt.expect_or_fail. What must you do to resume?",opts:["The pipeline auto-retries indefinitely until data is clean","Fix the upstream data quality issue causing the violation, then re-trigger the pipeline","Delete the expectation and re-run","Downgrade to @dlt.expect which only warns"],ans:1,exp:"@dlt.expect_or_fail halts the pipeline on any violation. The expectation is designed for 'data must always be valid' scenarios. Fix the bad data in the source, then re-trigger. Check expectation metrics to understand failure rates."},
{domain:"ELT with Spark & Python",q:"What does the window function rank() produce compared to dense_rank()?",opts:["rank() and dense_rank() are identical","rank() skips ranks for ties (1,1,3); dense_rank() doesn't skip (1,1,2)","dense_rank() skips ranks; rank() doesn't","rank() uses tie-breaking; dense_rank() is non-deterministic"],ans:1,exp:"For tied values: rank() assigns the same rank and then skips (1,1,3 for a 3-way tie at rank 1 and 2). dense_rank() assigns the same rank without skipping (1,1,2). Use dense_rank() when you need consecutive rank numbers."},
{domain:"Lakehouse Platform",q:"A data engineer wants to create a database in Databricks with a custom location for its tables. Which SQL is correct?",opts:["CREATE DATABASE mydb AT 's3://bucket/path'","CREATE DATABASE mydb LOCATION 's3://bucket/path'","CREATE SCHEMA mydb WITH PATH 's3://bucket/path'","CREATE DATABASE mydb STORAGE 's3://bucket/path'"],ans:1,exp:"CREATE DATABASE (or CREATE SCHEMA) with LOCATION sets the default storage path for managed tables in that database. Tables created in the database without explicit LOCATION store data under this path."},
{domain:"ELT with Spark & Python",q:"What does the Spark function get_json_object(col, '$.address.city') return?",opts:["The entire JSON object as a string","The value at the JSON path $.address.city extracted from the JSON string column","An error if the JSON path doesn't exist","A struct type with address and city fields"],ans:1,exp:"get_json_object() extracts a scalar value from a JSON string column using a JSONPath expression. $.address.city navigates to the city field inside the address object. Returns null if the path doesn't exist."},
{domain:"Production Pipelines",q:"A team runs 20 Databricks Jobs simultaneously. They notice job start times are delayed. What is the most likely cause?",opts:["Python version conflicts between jobs","Cluster instance quota limits ‚Äî not enough VMs available to start all clusters simultaneously","The Databricks API is rate-limited to 20 requests/minute","The Spark UI is overloaded"],ans:1,exp:"Cloud providers have VM quota limits per account/region. If 20 jobs simultaneously request clusters, quota may be exhausted, causing new clusters to wait. Solutions: increase quotas, use instance pools, or stagger job schedules."},
{domain:"ELT with Spark & Python",q:"Which of the following correctly reads a parquet file from DBFS?",opts:["spark.read.parquet('dbfs:/data/file.parquet')","spark.read.parquet('/dbfs/data/file.parquet')","Both A and B work ‚Äî dbfs:/ and /dbfs/ are equivalent paths in Databricks","spark.read.format('parquet').dbfs('/data/file.parquet').load()"],ans:2,exp:"Both dbfs:/ (Spark path syntax) and /dbfs/ (FUSE mount path for local file APIs) point to the same DBFS location in Databricks. Use dbfs:/ in Spark APIs and /dbfs/ when using Python file I/O (open(), pandas.read_csv(), etc.)."},
{domain:"Data Governance",q:"What happens when a Unity Catalog managed table is renamed using ALTER TABLE my_table RENAME TO new_table?",opts:["Data files are moved to match the new table name","Only the metadata (table definition) is renamed; data files remain in the same location","A new table is created and old one is dropped","Rename is not supported for managed tables"],ans:1,exp:"Renaming a managed table updates only the metadata in the Unity Catalog metastore. The underlying Parquet/Delta files remain at the same storage path. Only the logical name changes."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to read only specific columns from a large Delta table. How does Spark optimize this?",opts:["Spark reads all columns and drops the unneeded ones in memory","Column pruning: Spark reads only the requested columns from Parquet files, skipping bytes for other columns","Spark creates a secondary index for the requested columns","The optimization requires explicit hints: df.hint('prune', ['col1','col2'])"],ans:1,exp:"Parquet is a columnar format ‚Äî each column is stored separately. Spark's column pruning only reads the bytes for requested columns from each Parquet file, dramatically reducing I/O for queries selecting a few columns from wide tables."},
{domain:"Incremental Data Processing",q:"A streaming application reads from Kafka and has been running for 6 hours. The Kafka offset for a topic partition is 1000. What does this mean?",opts:["1000 messages are waiting to be processed","1000 messages have been processed from that partition up to message at offset 999","The lag is 1000 messages behind real-time","1000 partitions exist in the Kafka topic"],ans:1,exp:"In Kafka, an offset is a sequential ID assigned to each message in a partition. An offset of 1000 means messages 0-999 have been committed. The streaming checkpoint stores this offset so that on restart, reading continues from offset 1000."},
{domain:"Lakehouse Platform",q:"What is the primary purpose of Databricks Workflows (formerly Jobs) compared to simple scheduled notebook execution?",opts:["Workflows support only batch processing; scheduled notebooks support streaming","Workflows provide multi-task DAG orchestration, dependency management, conditional execution, retries, notifications, and audit logs for complex production pipelines","Workflows are for ML pipelines only; notebooks are for ETL","Workflows execute faster because they use dedicated hardware"],ans:1,exp:"Databricks Workflows orchestrate multi-task pipelines with dependencies (DAG), conditional execution (run-if), retries, timeout management, external service integration (webhooks), audit trails, and SLA monitoring ‚Äî far beyond simple notebook scheduling."},
{domain:"ELT with Spark & Python",q:"How should a data engineer handle read errors (malformed JSON rows) gracefully in production?",opts:["Let the job fail and fix manually","Use PERMISSIVE mode (default) with a badRecordsPath to capture malformed rows without failing the job","Use DROPMALFORMED mode to silently drop all bad records","Read as text and parse manually"],ans:1,exp:"PERMISSIVE mode (default for JSON/CSV) tries to parse records and nulls out bad values. Adding badRecordsPath writes unparsable records to a specified path for later investigation. DROPMALFORMED silently drops bad records ‚Äî bad for production visibility."},
{domain:"Data Governance",q:"What does the GRANT USAGE ON CATALOG privilege do in Unity Catalog?",opts:["Grants SELECT on all tables in the catalog","Allows the grantee to navigate into the catalog and see its schemas, but doesn't grant access to any data inside","Grants the ability to create schemas in the catalog","Grants admin rights on the catalog"],ans:1,exp:"USAGE (or USE CATALOG) allows navigating into the catalog ‚Äî seeing its schemas. Without this, users can't even list schemas. It's the first tier of the privilege hierarchy before granting schema-level and table-level access."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to write a streaming query that sends each micro-batch to an external REST API. Which sink type enables this?",opts:["console sink","delta sink","foreachBatch with custom write logic","memory sink"],ans:2,exp:"foreachBatch provides the micro-batch DataFrame and batch ID, allowing you to implement custom write logic including REST API calls, multi-sink writes, deduplication, and MERGE INTO operations ‚Äî anything a batch DataFrame supports."},
{domain:"Lakehouse Platform",q:"What is the difference between dbutils.fs.ls() and %fs ls in Databricks?",opts:["dbutils.fs.ls() works only with DBFS; %fs ls works with cloud storage","They access the same filesystem; dbutils.fs.ls() returns a list of FileInfo objects in Python; %fs ls is a magic command displaying output as text","%fs ls is deprecated; dbutils.fs.ls() is the current standard","dbutils.fs.ls() is 10x faster than %fs ls"],ans:1,exp:"Both access DBFS (and cloud mounts). dbutils.fs.ls() returns a list of FileInfo objects programmatically usable in Python. %fs ls is a notebook magic command for quick interactive directory browsing. Same underlying API, different interfaces."},
{domain:"Incremental Data Processing",q:"A data engineer wants to ensure a streaming query processes events exactly once end-to-end (reading from Kafka, writing to Delta). What combination of settings achieves this?",opts:["enable.auto.commit=true in Kafka consumer","checkpointLocation set + Delta as output sink ‚Äî Delta's transactional writes + checkpoint guarantees exactly-once","Increase Kafka replication factor to 3","Use FAILSAFE mode in the stream writer"],ans:1,exp:"Exactly-once semantics require: (1) a checkpoint location (for idempotent source offset tracking) and (2) a transactional sink (Delta supports atomic batch commits). Together, retried micro-batches produce the same result without duplicates."},
{domain:"ELT with Spark & Python",q:"What does df.na.fill({'age': 0, 'city': 'Unknown'}) do?",opts:["Fills all null values with 0 and Unknown","Fills null values in 'age' with 0 and null values in 'city' with 'Unknown', leaving other columns unchanged","Drops rows where age=0 or city=Unknown","Renames null values to the specified strings"],ans:1,exp:"na.fill() with a dict fills nulls column-specifically. Only null values in the specified columns are replaced with the corresponding values. Other columns and other types of nulls are untouched."},
{domain:"Production Pipelines",q:"A data engineer needs to run a Python script (not a notebook) as a Databricks Job task. Which task type is used?",opts:["Notebook task with the script as inline code","Python script task ‚Äî pointing to a .py file in DBFS or a Git repo","JAR task with the Python packaged as a JAR","SQL task with a Python stored procedure"],ans:1,exp:"Databricks Jobs supports a Python script task type that runs a .py file directly on the cluster. The file can be stored in DBFS, a workspace path, or a Git repository (when using Git-backed jobs)."},
{domain:"Lakehouse Platform",q:"What is the purpose of a Databricks cluster tag?",opts:["Categorizes notebooks in the workspace UI","Adds metadata to clusters for cloud cost allocation, billing attribution, and organization-level tracking","Restricts which users can attach to the cluster","Enables automatic cluster termination by tag name"],ans:1,exp:"Cluster tags propagate to cloud resource tags (AWS resource tags, Azure resource tags). This enables cloud billing analysis by team/project, chargeback models, and organizational cost attribution in cloud cost management tools."},
{domain:"ELT with Spark & Python",q:"Which method converts a DataFrame to a pandas DataFrame?",opts:["df.to_pandas()","df.toPandas()","df.collect_pandas()","pandas.from_spark(df)"],ans:1,exp:"df.toPandas() collects all data to the driver and converts to a pandas DataFrame. Use only for small DataFrames ‚Äî large DataFrames will cause driver OOM. For large-scale pandas operations, consider pandas API on Spark (df.to_pandas_on_spark())."},
{domain:"Incremental Data Processing",q:"A Delta table is partitioned by 'country' and 'year'. Which query benefits MOST from partition pruning?",opts:["SELECT * FROM t WHERE name = 'Alice'","SELECT * FROM t WHERE country = 'US' AND year = 2024","SELECT COUNT(*) FROM t","SELECT * FROM t ORDER BY sales DESC LIMIT 10"],ans:1,exp:"Query B filters on both partition columns, so Spark reads only the directory country=US/year=2024, skipping all other countries and years. Query A filters on a non-partition column ‚Äî no partition pruning possible, full scan required."},
{domain:"Data Governance",q:"Which Delta Lake feature, combined with Unity Catalog, enables GDPR 'right to be forgotten' without full table rewrite?",opts:["VACUUM with immediate deletion","DELETE FROM + VACUUM after retention period, with Unity Catalog auditing the delete operation","DROP TABLE and recreate excluding the user","Delta CDF to stream deletions to downstream systems"],ans:1,exp:"DELETE FROM performs a logical delete (immediately invisible to reads). VACUUM after the retention period physically removes old files containing the deleted data. Unity Catalog audits the DELETE operation with user, timestamp, and affected rows for compliance."},
{domain:"ELT with Spark & Python",q:"What does schema.fields return for a StructType schema?",opts:["A list of column names as strings","A list of StructField objects containing name, dataType, and nullable for each column","A dict mapping column names to data types","The DDL string representation of the schema"],ans:1,exp:"schema.fields returns a list of StructField objects. Each StructField has .name (column name), .dataType (e.g., StringType(), IntegerType()), and .nullable (boolean). Useful for programmatic schema inspection and transformation."},
{domain:"Lakehouse Platform",q:"A data engineer creates a view in Unity Catalog: CREATE VIEW v AS SELECT * FROM t. Who can query this view by default?",opts:["All workspace users","Only the creator and workspace admins","No one ‚Äî SELECT on the view must be explicitly granted","Anyone with SELECT on the underlying table t"],ans:1,exp:"By default, only the creator and workspace admins can access a newly created object in Unity Catalog. Access must be explicitly granted: GRANT SELECT ON VIEW v TO user_or_group. The view's access is independent of access to the underlying table."},
{domain:"ELT with Spark & Python",q:"What is the output of df.groupBy('dept').agg(countDistinct('employee_id'))?",opts:["One row with the total count of distinct employee IDs across all departments","One row per dept with the count of unique employee IDs in that dept","A list of unique employee IDs per dept","An error ‚Äî countDistinct cannot be used with groupBy"],ans:1,exp:"groupBy('dept').agg(countDistinct('col')) produces one row per unique dept, with countDistinct counting unique non-null values of employee_id within each department. This is equivalent to SQL: SELECT dept, COUNT(DISTINCT employee_id) FROM t GROUP BY dept."},
{domain:"Production Pipelines",q:"A data engineer wants to validate that a DLT pipeline produces the expected number of output rows. Which approach is best?",opts:["Add a print() statement at the end of the pipeline","Use DLT expectations with @dlt.expect to assert minimum row counts","Check the pipeline's metrics in the DLT UI and set up alerts based on output table row count via system tables","Add a separate validation notebook run after the pipeline"],ans:2,exp:"DLT Expectations (expect, expect_or_drop, expect_or_fail) capture quality metrics. For pipeline-level validation, monitoring the pipeline event log and output metrics via system tables or DLT UI alerts provides ongoing validation without manual checking."},
{domain:"ELT with Spark & Python",q:"How do you read a Delta table from a specific path and convert it to a temporary view for SQL queries?",opts:["spark.sql('CREATE VIEW v AS SELECT * FROM delta.`path`')","spark.read.format('delta').load(path).createOrReplaceTempView('v')","delta.read(path).toTempView('v')","spark.load(path).view('v')"],ans:1,exp:"createOrReplaceTempView() creates a session-scoped temporary view accessible via spark.sql(). This pattern bridges the DataFrame API and SQL API: read with DataFrame API, query with SQL, combine both as needed."},
{domain:"Incremental Data Processing",q:"What is the SCD (Slowly Changing Dimension) Type 2 behavior in DLT APPLY CHANGES INTO?",opts:["Overwrites the existing row with new values (no history)","Creates a new row for each change, preserving full history with effective date columns","Deletes the old row and inserts a new one","Updates only the changed columns, nulling unchanged ones"],ans:1,exp:"SCD Type 2 preserves full history. APPLY CHANGES INTO with STORED AS SCD TYPE 2 creates new rows for each change, adding __START_AT and __END_AT columns. The current record has null __END_AT; historical records have both dates populated."},
{domain:"Lakehouse Platform",q:"What is the purpose of the Databricks Partner Connect feature?",opts:["Connects Databricks to partner hardware vendors for specialized compute","Provides one-click integrations with BI tools (Tableau, Power BI), ETL tools (Fivetran, dbt), and other data ecosystem partners","Enables cross-cloud data sharing between Databricks partners","Provides a marketplace for buying Databricks-certified datasets"],ans:1,exp:"Partner Connect offers pre-built integrations with 30+ data partners ‚Äî BI tools, ingestion tools, governance, and ML platforms. One-click setup configures the partner tool to connect to your Databricks workspace automatically."},
{domain:"ELT with Spark & Python",q:"A data engineer uses df.write.format('delta').save(path) on an existing Delta path. What happens?",opts:["Overwrites existing data","Raises AnalysisException: table already exists","Appends data to the existing table (default mode is ErrorIfExists)","Merges with existing data based on primary key"],ans:2,exp:"Default write mode is ErrorIfExists ‚Äî it raises an error if the target path already has data. You must explicitly set mode('overwrite'), mode('append'), or mode('ignore') to change this behavior. This prevents accidental data overwrites."},
{domain:"Data Governance",q:"Which SQL command grants a group read access to all current AND future tables in a schema?",opts:["GRANT SELECT ON TABLE *.* TO GROUP analysts","GRANT SELECT ON ALL TABLES IN SCHEMA main.sales TO GROUP analysts","GRANT SELECT ON FUTURE TABLES IN SCHEMA main.sales TO GROUP analysts -- plus on current tables","GRANT SELECT ON SCHEMA main.sales TO GROUP analysts"],ans:2,exp:"To cover both existing and future tables: (1) GRANT SELECT ON ALL TABLES IN SCHEMA for current tables, (2) GRANT SELECT ON FUTURE TABLES IN SCHEMA for tables created afterward. Schema-level GRANT doesn't automatically include table-level SELECT."},
{domain:"Incremental Data Processing",q:"What causes a 'SparkException: Task failed while writing rows' error during a Delta write?",opts:["Too many executors are writing simultaneously","Common causes include constraint violations (NOT NULL, CHECK), schema mismatches, or storage permission issues preventing writes to the target path","The DataFrame has too many columns","The Delta log is corrupt"],ans:1,exp:"This error wraps various write-time failures: NOT NULL constraint violations, schema mismatches (incompatible types), target storage permission denied, disk space exhaustion, or network issues reaching cloud storage. Inspect the full stack trace for the root cause."},
{domain:"ELT with Spark & Python",q:"What does df.summary() display compared to df.describe()?",opts:["They are identical","summary() includes additional statistics: count, mean, stddev, min, 25%, 50%, 75%, max; describe() shows count, mean, stddev, min, max only","describe() is deprecated in favor of summary()","summary() works on string columns; describe() works on numeric columns"],ans:1,exp:"df.describe() returns: count, mean, stddev, min, max for numeric columns and count, unique counts for strings. df.summary() extends this with quartiles (25%, 50%, 75%) for a more complete distributional view."},
{domain:"Lakehouse Platform",q:"What does configuring an external Hive metastore for a Databricks cluster allow?",opts:["Using Hive on-premises compute alongside Databricks","Sharing table definitions with other Spark environments (EMR, on-prem Spark) that connect to the same external metastore","Replacing Unity Catalog with the Hive metastore","Using Hive QL exclusively for queries"],ans:1,exp:"An external Hive metastore (MySQL, RDS, Azure SQL) can be shared between Databricks and other Spark environments. Tables registered in one environment are visible in others. Unity Catalog is the modern replacement for cross-platform governance."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to write test cases for a PySpark transformation. Which testing approach is recommended?",opts:["Run tests manually in a notebook and inspect output visually","Use pytest with SparkSession in test files; use chispa for DataFrame comparison or spark.createDataFrame() for test fixtures","Mock all Spark operations with unittest.mock","Test only via Databricks Repos CI/CD pipelines"],ans:1,exp:"pytest with a shared SparkSession fixture is the standard approach. chispa provides assertDataFrameEqual for column-order-independent DataFrame comparison. pyspark.testing.assertSchemaEqual validates schemas. Tests can run locally or in CI."},
{domain:"Production Pipelines",q:"A DLT pipeline has multiple tables with complex interdependencies. How does DLT determine execution order?",opts:["Tables execute alphabetically","The engineer specifies execution order explicitly in a config file","DLT automatically builds a DAG from dlt.read() / dlt.read_stream() dependencies and executes in topological order","Tables always execute in the order they are defined in the notebook"],ans:2,exp:"DLT automatically constructs a dependency DAG by analyzing dlt.read() and dlt.read_stream() calls. It then executes tables in topological order ‚Äî dependencies before dependents ‚Äî automatically, without any manual ordering required."},
{domain:"ELT with Spark & Python",q:"What does the Spark broadcast variable allow you to do efficiently?",opts:["Broadcast a streaming DataFrame to all executors","Share a read-only value (e.g., a large Python dict or list) with all tasks without serializing it with each task","Broadcast writes to multiple output sinks simultaneously","Make a DataFrame available in SQL queries automatically"],ans:1,exp:"sc.broadcast(value) distributes a read-only variable to all executors once and caches it there. Tasks reference the broadcast variable instead of the value being serialized with each task closure ‚Äî crucial for large lookup tables or config dicts."},
{domain:"Incremental Data Processing",q:"When does Auto Loader switch from directory listing to file notification mode and what is the benefit?",opts:["Switch happens automatically when file count exceeds 1000","It must be manually configured with cloudFiles.useNotifications=true; benefit is real-time file detection without full directory scans","Switch happens automatically when latency exceeds 60 seconds","File notification mode is only available on Azure, not AWS"],ans:1,exp:"File notification mode requires explicit configuration and cloud setup (SQS/SNS on AWS, Event Grid on Azure). Benefit: files are detected within seconds of landing without scanning the entire directory. Critical for high-throughput or latency-sensitive ingestion."},
{domain:"Data Governance",q:"A data engineer wants to grant the 'etl_team' group the ability to INSERT data into a table but not SELECT from it. Which grant achieves this?",opts:["GRANT INSERT ON TABLE t TO GROUP etl_team","GRANT MODIFY ON TABLE t TO GROUP etl_team","GRANT WRITE ON TABLE t TO GROUP etl_team","GRANT INSERT is not a Unity Catalog privilege; use GRANT ALL"],ans:1,exp:"MODIFY privilege in Unity Catalog covers INSERT, UPDATE, DELETE, and MERGE operations on a table. It does NOT grant SELECT (read) access. To give write-only access, grant MODIFY without SELECT."},
{domain:"ELT with Spark & Python",q:"Which Spark operation would you use to convert a DataFrame of (key, value) pairs into a Python dict on the driver?",opts:["df.toPandas().to_dict()","dict(df.collect()) or df.rdd.collectAsMap()","df.toDict()","spark.sql('SELECT MAP(key, value) FROM t').collect()"],ans:1,exp:"dict(df.collect()) collects all rows as Row objects and creates a dict (works for 2-column DFs). rdd.collectAsMap() is more explicit for key-value pair RDDs. Both bring data to driver ‚Äî use only for small DataFrames."},
{domain:"Lakehouse Platform",q:"What does the 'single user' access mode for a cluster mean in Unity Catalog?",opts:["Only one user can create the cluster, but all can use it","The cluster is assigned to one user and provides full Unity Catalog governance features including data isolation","Single user clusters are 10x faster than shared clusters","Single user mode disables all authentication checks"],ans:1,exp:"Single user access mode (assigned to one user) supports all Unity Catalog features including credential passthrough and fine-grained access control. Shared access mode supports multiple users with Unity Catalog but has some feature restrictions."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to parse timestamps in format 'MM/dd/yyyy HH:mm:ss'. Which function and option are needed?",opts:["to_timestamp(col, 'MM/dd/yyyy HH:mm:ss')","from_timestamp(col, 'MM/dd/yyyy HH:mm:ss')","cast(col, 'timestamp')","unix_timestamp(col, 'MM/dd/yyyy HH:mm:ss').cast('timestamp')"],ans:0,exp:"to_timestamp(column, format) parses a string column with the specified Java datetime pattern. 'MM/dd/yyyy HH:mm:ss' maps to month/day/year hour:minute:second. This is the primary function for parsing non-ISO timestamp strings."},
{domain:"Production Pipelines",q:"A data engineer needs to ensure a Databricks Job retries only on specific exceptions (e.g., not on data quality failures). How can this be achieved?",opts:["Databricks Job retry config cannot be conditioned on exception type","Use a custom retry wrapper in the notebook that catches specific exceptions and calls dbutils.notebook.exit() with different codes; configure job to retry only on specific exit codes","Set exception_type in the job's retry_on_timeout config","Use @retry(exceptions=[SparkException]) decorator"],ans:1,exp:"Databricks Job tasks can be configured to retry on specific exit codes (for Python scripts) or by catching exceptions in notebook code and using dbutils.notebook.exit(error_code) to signal retriable vs non-retriable failures."},
{domain:"ELT with Spark & Python",q:"What is the purpose of unpersist() in Spark?",opts:["Deletes the DataFrame and frees its memory from the execution plan","Removes the DataFrame from the Spark cache (memory/disk), freeing resources for other operations","Clears all cached DataFrames in the session","Writes the cached DataFrame to DBFS and removes from memory"],ans:1,exp:"unpersist() explicitly releases the cached data for a DataFrame, freeing executor memory/disk. Spark will also evict cached data based on LRU policy when memory pressure occurs, but explicit unpersist() is better for fine-grained control."},
{domain:"Incremental Data Processing",q:"A data engineer notices DLT pipeline events showing 'WAITING_FOR_RESOURCES'. What does this indicate?",opts:["The pipeline is paused waiting for manual approval","The cluster is starting up or waiting for available cloud instances before the pipeline can execute","A data quality expectation is blocking progress","The pipeline source is empty"],ans:1,exp:"WAITING_FOR_RESOURCES means the DLT pipeline is waiting for cluster compute to become available ‚Äî either the cluster is starting up (cold start) or cloud instance quota is exhausted. In Development mode, cluster reuse eliminates most of this wait time."},
{domain:"ELT with Spark & Python",q:"What does window.unboundedPreceding as the lower bound of a window frame mean?",opts:["The window includes only the previous row","The window includes all rows from the beginning of the partition up to the current row (running total)","The window has no lower bound ‚Äî must use rowsBetween to define it","The window starts from a negative offset"],ans:1,exp:"Window.unboundedPreceding as lower bound means the frame starts from the very first row in the partition. Combined with Window.currentRow as upper bound, this creates a running total (cumulative sum, running max, etc.) from the start of the partition."},
];

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// ADDITIONAL QUESTIONS SET 2 (150+ more)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
const ADDITIONAL_QUESTIONS = [

// LAKEHOUSE PLATFORM (25 new)
{domain:"Lakehouse Platform",q:"What is the key difference between a Databricks All-Purpose Cluster and a Job Cluster?",opts:["All-Purpose is faster; Job Cluster is cheaper","All-Purpose stays running between notebook sessions; Job Clusters are created for a specific run and terminated automatically when done","Job Clusters support more concurrent users","All-Purpose Clusters use spot instances by default"],ans:1,exp:"All-Purpose Clusters persist between sessions and are shared by multiple users for interactive workloads. Job Clusters are ephemeral ‚Äî created when a job starts and automatically terminated when the job completes, making them more cost-efficient for automated workloads."},
{domain:"Lakehouse Platform",q:"Which Databricks concept allows you to define a 'virtual' schema that maps to an external Hive metastore or HMS?",opts:["External Catalog","External Location","Legacy Hive metastore","Federated Catalog"],ans:0,exp:"An External Catalog in Unity Catalog is a federated reference to an external Hive metastore (or Glue). It lets Unity Catalog govern and query data registered in those external systems without migrating the underlying data."},
{domain:"Lakehouse Platform",q:"What is the Databricks Lakehouse Federation feature?",opts:["Sharing Delta tables across cloud regions","Querying external databases (MySQL, PostgreSQL, Snowflake, Redshift) directly from Databricks without data movement","Federating ML model serving endpoints","Combining data from multiple Unity Catalog metastores"],ans:1,exp:"Lakehouse Federation enables read-only querying of external database systems via Unity Catalog connections and foreign catalogs. Data stays in the source system; Databricks pushes down queries using optimized connectors."},
{domain:"Lakehouse Platform",q:"In Databricks, what is a 'Shared' cluster access mode?",opts:["A cluster shared by two workspaces","A cluster where multiple users can run concurrent workloads with Unity Catalog row/column security enforced per user","A cluster that shares compute with other Databricks accounts","A pre-shared cluster template"],ans:1,exp:"Shared access mode allows multiple users to run on the same cluster with full Unity Catalog security isolation. Each user's queries are checked against their specific permissions. Contrast with Single User mode where one user has full access."},
{domain:"Lakehouse Platform",q:"What is Databricks Delta Sharing?",opts:["Sharing Delta table schemas between teams","An open protocol for securely sharing Delta Lake tables with other organizations or platforms without copying data","Automatically mirroring Delta tables across regions","A feature for sharing Databricks notebooks publicly"],ans:1,exp:"Delta Sharing is an open REST protocol for sharing live Delta tables with external recipients (even non-Databricks environments like pandas, Spark, or Power BI) securely. The provider controls access; recipients query the data without copying it."},
{domain:"Lakehouse Platform",q:"What does the DESCRIBE DETAIL command return for a Delta table?",opts:["Column names and data types only","Table-level metadata: format, location, numFiles, sizeInBytes, partitionColumns, properties, and more","The last 10 commit operations","The CREATE TABLE DDL statement"],ans:1,exp:"DESCRIBE DETAIL provides rich operational metadata: storage location, file count, total size, partition columns, table properties (CDF enabled, etc.), min/max reader/writer version requirements, and the table's creation timestamp."},
{domain:"Lakehouse Platform",q:"A Databricks workspace is configured with a Unity Catalog metastore. Where is managed table data stored by default?",opts:["In DBFS root at /user/hive/warehouse","In the default storage location associated with the Unity Catalog metastore (typically a cloud bucket configured by the admin)","In the workspace's local disk","In a Databricks-managed S3 bucket shared globally"],ans:1,exp:"When a Unity Catalog metastore is created, a default storage location (cloud storage bucket/container) is specified. Managed tables in that metastore default to this location. Each catalog and schema can override with their own storage location."},
{domain:"Lakehouse Platform",q:"Which statement best describes the Photon engine in Databricks?",opts:["A separate query engine for Python workloads only","A C++ vectorized query engine that replaces Spark's JVM-based execution for SQL and DataFrame operations, dramatically improving throughput","A GPU-accelerated training engine for ML models","A specialized engine for streaming workloads only"],ans:1,exp:"Photon is a native vectorized execution engine written in C++ that replaces Spark's JVM-based execution layer for SQL and DataFrame operations. It processes multiple rows simultaneously using SIMD instructions, achieving 2-10x speedups for data-intensive workloads."},
{domain:"Lakehouse Platform",q:"What are Databricks Repos used for?",opts:["Storing binary data files and models","Integrating with Git providers (GitHub, GitLab, Bitbucket, Azure DevOps) to version-control notebooks, files, and libraries directly in the workspace","Managing secret scopes","Hosting ML model artifacts"],ans:1,exp:"Databricks Repos connects your workspace to Git repositories. You can clone, pull, push, create branches, and commit changes from the Repos UI or CLI. It enables CI/CD workflows, code review, and multi-developer collaboration on Databricks code."},
{domain:"Lakehouse Platform",q:"Which compute type in Databricks is best suited for ad-hoc SQL analytics by BI teams?",opts:["Interactive All-Purpose Cluster","Serverless SQL Warehouse","Job Cluster with SQL task","GPU Cluster"],ans:1,exp:"SQL Warehouses (formerly SQL Endpoints) are purpose-built for BI/SQL analytics. They support JDBC/ODBC connectivity for BI tools, provide a result cache, serverless auto-scaling, and are optimized for concurrent SQL queries from many users."},
{domain:"Lakehouse Platform",q:"What is the purpose of Unity Catalog's system schema?",opts:["Stores internal Unity Catalog configuration","Provides system tables for querying audit logs, billing data, query history, and lineage programmatically via SQL","A schema for storing temporary results","Reserved for Databricks-internal use only, not user-accessible"],ans:1,exp:"The system schema in Unity Catalog provides system tables: system.access.audit (access events), system.query.history (query logs), system.billing.usage (DBU consumption), and system.lineage.* tables. These enable programmatic monitoring and compliance reporting."},
{domain:"Lakehouse Platform",q:"What happens to the underlying data when you DROP a Unity Catalog managed table?",opts:["Only metadata is deleted; files remain for 30 days","Both table metadata and underlying data files are deleted","Only the table definition is dropped; data files are retained indefinitely","Data is archived to cheaper storage tier"],ans:1,exp:"Dropping a managed table in Unity Catalog deletes both the metadata (in the metastore) and the underlying data files in cloud storage. This is the key distinction from external tables, where only metadata is dropped and files are preserved."},
{domain:"Lakehouse Platform",q:"What is the Databricks File System (DBFS)?",opts:["A distributed filesystem that replaces HDFS in Databricks, abstracting cloud storage (S3, ADLS, GCS) with a unified interface","A local SSD filesystem on each cluster node","A proprietary binary file format for Databricks","A network filesystem for sharing notebooks"],ans:1,exp:"DBFS is an abstraction layer over cloud object storage (S3, ADLS, GCS) that presents it as a Unix-like filesystem. It enables path-based access (dbfs:/ or /dbfs/), persists data beyond cluster lifetimes, and provides a unified interface to cloud storage."},
{domain:"Lakehouse Platform",q:"A data engineer wants to enable autoscaling on a Databricks cluster. What must be configured?",opts:["Set spark.dynamicAllocation.enabled=true in Spark config","Set min and max workers in the cluster configuration; autoscaling handles the rest automatically","Install the autoscaling library on the cluster","Autoscaling is only available for Job Clusters, not All-Purpose Clusters"],ans:1,exp:"Databricks autoscaling requires specifying min and max worker counts. The cluster starts at min workers and automatically scales up (when there are pending tasks) or down (when workers are idle). Both All-Purpose and Job Clusters support autoscaling."},
{domain:"Lakehouse Platform",q:"What is the relationship between Databricks and Apache Spark?",opts:["Databricks created Spark and it is proprietary to Databricks","Databricks was founded by Spark's original creators; it provides a managed, optimized cloud platform for Spark with additional features like Delta Lake, MLflow, and Unity Catalog","Databricks is a pure competitor to Spark","Databricks uses a fork of Spark incompatible with open-source Spark"],ans:1,exp:"Databricks was founded by the creators of Apache Spark (from UC Berkeley's AMPLab). Databricks Runtime includes an optimized, tested Spark distribution, contributing upstream to Apache Spark while adding proprietary features on top for production use."},
{domain:"Lakehouse Platform",q:"What does spark.conf.set('spark.sql.shuffle.partitions', 200) control?",opts:["The number of partitions in the source data","The number of output partitions after a shuffle operation (sort, join, groupBy)","The maximum parallel tasks per stage","The number of RDD partitions created from file reads"],ans:1,exp:"spark.sql.shuffle.partitions (default 200) controls the number of partitions produced by shuffle operations. For small datasets, reduce it (e.g., 4-20) to avoid overhead; for large shuffles, increase it. It doesn't affect file-read parallelism (controlled by input file size)."},
{domain:"Lakehouse Platform",q:"Which feature enables a Databricks cluster to access data in an S3 bucket without embedding IAM credentials in code?",opts:["DBFS mount with encrypted credentials","Instance profiles (IAM role attached to the cluster's EC2 instances) or Unity Catalog storage credentials","Environment variables in the cluster config","Databricks secret scope with S3 keys"],ans:1,exp:"Instance profiles attach an IAM role to the EC2 instances running the cluster. Spark jobs assume this role for S3 access without embedding credentials. Unity Catalog storage credentials are the modern replacement for fine-grained access control."},
{domain:"Lakehouse Platform",q:"What is a Databricks workspace?",opts:["A single notebook environment","A shared environment where teams access notebooks, clusters, jobs, data, and other Databricks assets; workspace assets are isolated per workspace","The physical server cluster running Databricks","A single user's private notebook collection"],ans:1,exp:"A workspace is a Databricks deployment with its own URL. It provides isolated environments for teams with their own clusters, notebooks, jobs, and access controls. Multiple workspaces can be attached to a single Unity Catalog metastore for cross-workspace governance."},
{domain:"Lakehouse Platform",q:"What does the Databricks Community Edition offer compared to the full product?",opts:["Full enterprise features for open-source contributors","A limited free tier for learning with a single-node cluster, basic notebook functionality, but no Jobs, SQL Warehouses, Unity Catalog, or team features","A cloud-agnostic version with no AWS/Azure/GCP dependencies","Free enterprise features for academic institutions"],ans:1,exp:"Community Edition is a free learning environment with a small single-node cluster, notebook access, and basic Spark/Delta capabilities. It lacks enterprise features: Jobs orchestration, SQL Warehouses, Unity Catalog, MLflow Model Registry, and collaboration features."},
{domain:"Lakehouse Platform",q:"Which feature in Databricks allows running SQL queries directly on Delta tables using standard JDBC/ODBC connectivity?",opts:["Databricks Connect","SQL Warehouses with JDBC/ODBC endpoints","Interactive cluster SQL magic commands","Delta REST API"],ans:1,exp:"SQL Warehouses expose standard JDBC and ODBC endpoints compatible with BI tools (Tableau, Power BI, Looker, etc.) and SQL clients. Tools connect using the warehouse's server hostname, HTTP path, and a personal access token."},
{domain:"Lakehouse Platform",q:"What is Delta Lake?",opts:["A proprietary Databricks file format","An open-source storage layer on top of Parquet that adds ACID transactions, schema enforcement, time travel, and scalable metadata to data lakes","A cloud-native database system","A stream processing framework"],ans:1,exp:"Delta Lake is an open-source storage framework (hosted by the Linux Foundation) that adds ACID transactions, scalable metadata handling, schema enforcement/evolution, time travel, and streaming/batch unification to existing Parquet-based data lakes."},
{domain:"Lakehouse Platform",q:"What is the maximum time travel retention in Delta Lake by default before VACUUM removes old files?",opts:["24 hours","7 days","30 days","Unlimited until VACUUM is run with RETAIN 0 HOURS"],ans:1,exp:"By default, Delta retains files for 7 days (604800 seconds), configurable via delta.deletedFileRetentionDuration. VACUUM removes files older than this threshold. Running VACUUM with RETAIN 0 HOURS (override safety) can delete immediately."},
{domain:"Lakehouse Platform",q:"A data engineer sets spark.databricks.delta.optimizeWrite.enabled=true at the cluster level. What is the effect?",opts:["Enables Optimized Writes for all new Delta writes on the cluster, automatically coalescing small files","Enables OPTIMIZE to run automatically after each write","Enables Photon for all write operations","Partitions data automatically by date"],ans:0,exp:"Optimized Writes (also settable per-table) coalesces small Spark task output files into fewer, larger files (~128MB) before committing to Delta. This reduces the small file problem without requiring explicit OPTIMIZE runs after each write."},
{domain:"Lakehouse Platform",q:"What type of join strategy does Databricks automatically apply when one table is small enough to fit in memory?",opts:["Sort-Merge Join","Broadcast Hash Join","Shuffle Hash Join","Cross Join"],ans:1,exp:"When a table is smaller than spark.sql.autoBroadcastJoinThreshold (default 10MB), Spark automatically uses Broadcast Hash Join: the small table is sent to all executors, enabling the large table to be joined without any shuffle. No data movement for the large side."},
{domain:"Lakehouse Platform",q:"What information does the Spark UI's 'SQL / DataFrame' tab provide?",opts:["Python variable values during execution","The full execution plan (parsed, analyzed, optimized, physical) and per-stage metrics for SQL queries and DataFrame operations","Memory usage per executor","A list of all loaded Python libraries"],ans:1,exp:"The SQL/DataFrame tab shows the query execution DAG with physical plan stages, timing per node, rows processed, bytes shuffled, and cache hit statistics. It's the primary tool for diagnosing slow queries and understanding Catalyst optimization decisions."},

// ELT WITH SPARK & PYTHON (30 new)
{domain:"ELT with Spark & Python",q:"What does df.explain(True) display?",opts:["The DataFrame schema only","All four logical and physical plans: Parsed Logical, Analyzed Logical, Optimized Logical, and Physical Plan","Only the Physical Plan","The Spark SQL query equivalent"],ans:1,exp:"explain(True) (or explain('all')) shows all plan stages: (1) Parsed Logical Plan (initial parse), (2) Analyzed Logical Plan (types resolved), (3) Optimized Logical Plan (Catalyst rules applied), (4) Physical Plan (execution strategy). Essential for debugging performance."},
{domain:"ELT with Spark & Python",q:"Which PySpark method selects all columns except specified ones?",opts:["df.drop('col1','col2')","df.select_except('col1','col2')","df.exclude('col1','col2')","df.remove_cols(['col1','col2'])"],ans:0,exp:"df.drop('col1','col2') returns a new DataFrame with the specified columns removed. It's the clean way to exclude columns without listing all others. Also accepts a list: df.drop(*['col1','col2'])."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to pivot rows to columns. The data has columns (store, product, sales). What does df.groupBy('store').pivot('product').sum('sales') produce?",opts:["One row per product with stores as columns","One row per store, one column per unique product value, with sum of sales as cell values","A flattened array of all sales per store","A cross join of stores and products"],ans:1,exp:"pivot() transposes values of the pivot column into individual columns. GroupBy('store').pivot('product').sum('sales') creates one row per store, with a column for each unique product containing the sum of sales for that store-product combination."},
{domain:"ELT with Spark & Python",q:"What is the purpose of the lit() function in PySpark?",opts:["Creates a literal string for print output","Creates a Column object from a constant value, allowing it to be used in DataFrame transformations","Converts null to empty string","Parses JSON literals"],ans:1,exp:"lit(value) wraps a constant Python value (string, number, boolean, None) into a Spark Column object so it can be used alongside other Column expressions: df.withColumn('constant', lit(42)) adds a column of value 42 to every row."},
{domain:"ELT with Spark & Python",q:"What does df.select(expr('price * quantity AS total')) demonstrate?",opts:["Using Python arithmetic on DataFrame columns","Using SQL expression syntax within the DataFrame API via expr(), which parses a SQL string as a Column expression","A type cast operation","String interpolation in SQL"],ans:1,exp:"expr() takes a SQL expression string and returns a Column object. This allows mixing SQL syntax within the DataFrame API. Equivalent to: df.select((col('price') * col('quantity')).alias('total')). Useful for complex expressions more naturally written in SQL."},
{domain:"ELT with Spark & Python",q:"Which method converts a DataFrame column of JSON strings into a struct using a defined schema?",opts:["json_parse()","from_json(col, schema)","parse_json(col)","struct_from_json(col, schema)"],ans:1,exp:"from_json(col, schema) parses a JSON string column into a StructType column. The schema must be provided (as a StructType or DDL string). Use schema_of_json() on a sample to infer the schema, then pass it to from_json()."},
{domain:"ELT with Spark & Python",q:"What is the result of df.select(explode('array_col').alias('item'))?",opts:["A new array column with all arrays merged","One row per element in array_col, with the element value in the 'item' column","A count of elements per array","An error if arrays have different lengths"],ans:1,exp:"explode() expands an array (or map) column so that each element becomes a separate row. All other column values from the original row are duplicated. explode_outer() handles nulls by producing a single null row instead of dropping the row."},
{domain:"ELT with Spark & Python",q:"A data engineer writes spark.read.format('jdbc').option('url', url).option('dbtable', 'orders').load(). What is a key limitation?",opts:["JDBC reads are not supported in Databricks","By default the entire table is read in a single partition ‚Äî add partitionColumn, lowerBound, upperBound, numPartitions options for parallel reads","JDBC reads require Unity Catalog storage credentials","The result cannot be written to Delta"],ans:1,exp:"Without partitioning options, JDBC reads data in a single task. For large tables, configure partitionColumn (numeric column), lowerBound, upperBound, and numPartitions to split the read into parallel tasks, dramatically improving performance."},
{domain:"ELT with Spark & Python",q:"What does the coalesce(n) transformation do compared to repartition(n)?",opts:["Both are identical ‚Äî both can increase or decrease partitions","coalesce(n) reduces partitions without a full shuffle (merges existing partitions); repartition(n) always shuffles all data, can increase or decrease partitions","repartition is lazy; coalesce is eager","coalesce can only increase partitions"],ans:1,exp:"coalesce(n) is efficient for reducing partitions ‚Äî it combines existing partitions without a full shuffle. repartition(n) performs a full shuffle that evenly distributes data across all n partitions. Use coalesce when reducing, repartition when redistributing/increasing."},
{domain:"ELT with Spark & Python",q:"Which Spark function creates a new column by applying a condition: return 'active' if status='A' else 'inactive'?",opts:["df.withColumn('label', iif(col('status')=='A','active','inactive'))","df.withColumn('label', when(col('status')=='A','active').otherwise('inactive'))","df.withColumn('label', case(col('status')==lit('A'), 'active', 'inactive'))","df.withColumn('label', col('status').map({'A':'active'}))"],ans:1,exp:"when(condition, value).otherwise(default) implements conditional logic. when() can be chained for multiple conditions: when(c1, v1).when(c2, v2).otherwise(default). The first matching condition wins."},
{domain:"ELT with Spark & Python",q:"What does spark.sql.adaptive.enabled = true enable?",opts:["Automatic Python to Scala translation","Adaptive Query Execution (AQE): dynamically re-optimizes the physical plan at runtime based on actual data statistics, adjusting join strategies and partition counts","Automatic DataFrame caching","Dynamic schema inference"],ans:1,exp:"AQE (enabled by default in Spark 3.x) re-plans at runtime: converts sort-merge joins to broadcast joins when one side turns out small, coalesces post-shuffle partitions to avoid many small tasks, and handles skewed joins by splitting large partitions."},
{domain:"ELT with Spark & Python",q:"A DataFrame has a column 'event_time' as StringType. How do you cast it to TimestampType?",opts:["col('event_time').asTimestamp()","col('event_time').cast('timestamp') or col('event_time').cast(TimestampType())","df.withColumn('event_time', timestamp('event_time'))","spark.sql('CAST event_time AS TIMESTAMP')"],ans:1,exp:".cast() converts column types. For timestamps, cast('timestamp') assumes ISO 8601 format. For other formats, use to_timestamp(col, format) which accepts Java datetime format patterns like 'yyyy-MM-dd HH:mm:ss'."},
{domain:"ELT with Spark & Python",q:"What is the difference between .agg(count('*')) and .agg(count('column_name'))?",opts:["They always return identical results","count('*') counts all rows including nulls; count('column_name') counts only non-null values in that column","count('*') is faster","count('column_name') works only on numeric columns"],ans:1,exp:"count('*') counts every row. count(col_name) counts rows where col_name IS NOT NULL. This distinction matters for columns with nulls: count('*') might be 1000 while count('email') is only 750 if 250 emails are null."},
{domain:"ELT with Spark & Python",q:"What does df.checkpoint() do differently from df.cache()?",opts:["checkpoint() is faster but uses more memory","checkpoint() truncates the lineage by writing the DataFrame to disk (HDFS/DBFS) and loading from that checkpoint, while cache() just persists in memory/disk without breaking lineage","They are identical ‚Äî checkpoint is an alias for cache","checkpoint() encrypts the data before saving"],ans:1,exp:"Checkpoint writes data to reliable storage and severs the computation lineage. This prevents stack overflows from long lineage chains (e.g., iterative ML algorithms). cache() keeps the full lineage ‚Äî if cache is lost, Spark recomputes from scratch."},
{domain:"ELT with Spark & Python",q:"Which function can be used to read a Delta table as a streaming source?",opts:["spark.readStream.format('delta').load(path)","spark.read.stream.delta(path)","DeltaTable.stream(path)","spark.readDelta.streaming(path)"],ans:0,exp:"spark.readStream.format('delta').load(path) reads a Delta table as a continuous stream. New data committed to the table appears as new micro-batches. Use option('startingVersion', n) or option('startingTimestamp', ts) to control where to start."},
{domain:"ELT with Spark & Python",q:"What does array_contains(col('tags'), 'python') return?",opts:["The index of 'python' in the array","True if 'python' is in the array, False otherwise, null if the array is null","The count of 'python' occurrences","A filtered array with only 'python' elements"],ans:1,exp:"array_contains(array_col, value) returns True if the array contains the specified value, False if not, and null if the array is null. It's a built-in function that works with ArrayType columns."},
{domain:"ELT with Spark & Python",q:"What is the purpose of Spark's mapPartitions() vs. map()?",opts:["mapPartitions processes one column; map processes one row","mapPartitions calls the function once per partition (with an iterator); map calls the function once per row ‚Äî mapPartitions is more efficient for costly per-partition setup","map is for RDDs only; mapPartitions works on DataFrames","They produce different output types"],ans:1,exp:"mapPartitions() is efficient when you have expensive initialization (DB connection, ML model load) that should happen once per partition, not once per row. The function receives an iterator of rows and returns an iterator ‚Äî enables batching and resource reuse."},
{domain:"ELT with Spark & Python",q:"A data engineer needs to read the latest version of a Delta table that was written to 30 minutes ago. Which is the correct approach?",opts:["spark.read.format('delta').option('timestampAsOf', '30 minutes ago').load(path)","spark.read.format('delta').option('timestampAsOf', timestamp_30min_ago).load(path) using a proper timestamp string","spark.read.format('delta').option('version', -1).load(path)","The latest version is always read by default ‚Äî no special option needed"],ans:3,exp:"spark.read.format('delta').load(path) (without time travel options) always reads the latest committed version of the table. Time travel options (versionAsOf, timestampAsOf) are only needed to access historical versions."},
{domain:"ELT with Spark & Python",q:"What does the following accomplish? df.write.partitionBy('year','month').format('delta').mode('append').save(path)",opts:["Creates a Delta table partitioned by year and month, with data physically organized in year=X/month=Y directory structure","Partitions the DataFrame in memory without affecting storage layout","Creates a sorted index on year and month","Renames the year and month columns to partition keys"],ans:0,exp:"partitionBy() organizes files in a Hive-style directory hierarchy (path/year=2024/month=01/). This enables partition pruning: queries filtering on year and/or month skip entire directories, dramatically reducing I/O for date-range queries."},
{domain:"ELT with Spark & Python",q:"Which command shows all currently active streaming queries in a Spark session?",opts:["spark.streams.active","spark.streaming.running","df.writeStream.status()","spark.getActiveStreams()"],ans:0,exp:"spark.streams.active returns a list of active StreamingQuery objects. Each can be inspected (query.status, query.recentProgress, query.lastProgress) or managed (query.stop(), query.awaitTermination()). Useful for monitoring multiple concurrent streams."},
{domain:"ELT with Spark & Python",q:"What is the result of df.select(size('array_col'))?",opts:["The total byte size of the column","The number of elements in each array_col value (one count per row)","The number of distinct values across all arrays","The memory usage of the column"],ans:1,exp:"size(array_col) returns the number of elements in each array (or the number of key-value pairs in a map). Returns -1 for null values. Produces one integer value per row ‚Äî useful for filtering rows with arrays of specific sizes."},
{domain:"ELT with Spark & Python",q:"How do you perform a cross join (cartesian product) in PySpark?",opts:["df1.join(df2)","df1.crossJoin(df2) or df1.join(df2, how='cross')","df1.join(df2, on=None)","Cross joins are not supported in Spark for safety reasons"],ans:1,exp:"df1.crossJoin(df2) creates a cartesian product with all combinations of rows. Since this is usually a mistake, Spark requires spark.sql.crossJoin.enabled=True or the explicit crossJoin() method. The result has df1.count() * df2.count() rows."},
{domain:"ELT with Spark & Python",q:"What does the pandas_udf (Pandas UDF / Vectorized UDF) offer over a regular Python UDF?",opts:["pandas_udf supports SQL but regular UDF does not","pandas_udf operates on pandas Series/DataFrames instead of one row at a time, using Apache Arrow for serialization ‚Äî dramatically faster (10-100x) than row-at-a-time Python UDFs","pandas_udf can return multiple columns; regular UDF cannot","regular UDF is deprecated in Spark 3.0"],ans:1,exp:"pandas_udf uses Apache Arrow for columnar serialization (avoiding row-at-a-time Python pickle overhead) and operates on batches (pandas Series/DataFrame). This enables vectorized operations and numpy/pandas optimizations, typically 10-100x faster than regular Python UDFs."},
{domain:"ELT with Spark & Python",q:"A data engineer wants to apply a Python function to groups of rows. Which approach uses distributed pandas operations efficiently?",opts:["Collect data to driver, apply pandas groupby, distribute back","Use applyInPandas() with groupBy(), which applies a pandas function to each group on the executor","Use mapPartitions() with manual group aggregation","Convert to RDD and use groupByKey()"],ans:1,exp:"groupBy().applyInPandas(func, schema) distributes groups to executors where the function receives a pandas DataFrame and returns one. This keeps data distributed ‚Äî no collect/redistribute needed. Use for complex grouped operations where Spark's built-in aggregations are insufficient."},
{domain:"ELT with Spark & Python",q:"What is the difference between repartition(col('date')) and repartitionByRange(col('date'))?",opts:["They are identical","repartition uses hash partitioning (same date ‚Üí same partition); repartitionByRange sorts dates into ranges (useful for time-based bucketing with contiguous ranges)","repartitionByRange is deprecated","repartition supports multiple columns; repartitionByRange supports only one"],ans:1,exp:"repartition() by column uses hash partitioning: same value ‚Üí same partition, but different values may share a partition. repartitionByRange() distributes rows based on sorted ranges of the column, ensuring contiguous ranges are co-located ‚Äî better for range queries and sorted writes."},
{domain:"ELT with Spark & Python",q:"Which method returns the schema of a DataFrame as a StructType object?",opts:["df.schema","df.schema()","df.getSchema()","df.dtypes"],ans:0,exp:"df.schema is a property (not a method) that returns the StructType schema. df.dtypes returns a list of (column_name, type_string) tuples. df.printSchema() prints a human-readable tree format. All come from the same underlying schema."},
{domain:"ELT with Spark & Python",q:"A data engineer uses df.write.mode('overwrite').saveAsTable('my_table'). What happens to the existing table?",opts:["An error is raised ‚Äî saveAsTable doesn't support overwrite","The existing table is dropped and recreated with new data and the new schema","New data is appended to the table","Only rows that match existing data are updated"],ans:1,exp:"mode('overwrite') with saveAsTable drops the existing managed table (or truncates it if overwriteSchema=False) and writes the new data. For Delta tables with dynamic partition overwrite, it replaces only partitions present in the new data."},
{domain:"ELT with Spark & Python",q:"What is the purpose of the foreachBatch sink in Structured Streaming?",opts:["Writes each micro-batch to a separate file","Calls a user-defined function with each micro-batch as a DataFrame, enabling arbitrary write logic including MERGE, multi-sink writes, and conditional processing","Runs foreach on every row in real-time","Writes to a batch job trigger"],ans:1,exp:"foreachBatch(func) gives you full control over writing each micro-batch. The function receives (batchDF, batchId), letting you: run MERGE INTO for upserts, write to multiple destinations, apply deduplication, or call external APIs ‚Äî anything batch DataFrames support."},
{domain:"ELT with Spark & Python",q:"How do you read a single CSV file where the first row is not a header?",opts:["spark.read.csv(path, header=False)","spark.read.option('header','false').csv(path)","Both A and B work (equivalent syntax)","spark.read.csv(path).withoutHeader()"],ans:2,exp:"Both forms are equivalent. Python keyword argument (header=False) and .option('header','false') produce the same result. When header=False, Spark names columns _c0, _c1, _c2, etc. Always provide an explicit schema or rename columns afterward."},
{domain:"ELT with Spark & Python",q:"What does df.na.drop(subset=['email','phone']) do?",opts:["Drops columns email and phone from the DataFrame","Drops rows where email OR phone (or both) are null","Drops rows where BOTH email AND phone are null","Replaces nulls in email and phone with empty string"],ans:1,exp:"na.drop(subset=[col_list]) drops rows where ANY of the specified columns are null. To drop only when ALL specified columns are null, use na.drop(how='all', subset=[col_list]). Default behavior (how='any') drops if any specified column is null."},

// INCREMENTAL DATA PROCESSING (30 new)  
{domain:"Incremental Data Processing",q:"What is the default trigger behavior if no trigger is specified in a writeStream?",opts:["Batch mode ‚Äî processes once and stops","processingTime='0 seconds' ‚Äî runs micro-batches as fast as possible with minimal delay between them","availableNow=True ‚Äî processes all current data","A trigger must always be specified or an error is raised"],ans:1,exp:"Without a trigger specification, Structured Streaming runs in continuous mode: as soon as one micro-batch completes, the next starts immediately. This maximizes throughput for latency-sensitive workloads but uses continuous compute."},
{domain:"Incremental Data Processing",q:"What does OPTIMIZE with ZORDER do to a Delta table's transaction log?",opts:["Creates a new separate index file","Records the OPTIMIZE operation as a new commit in the transaction log, with the new compacted files replacing the old ones","Deletes the transaction log and recreates it","Adds a ZORDER index entry to each existing transaction log entry"],ans:1,exp:"OPTIMIZE is recorded as an atomic commit in the Delta transaction log. The commit adds new compacted files and marks old files as removed (logically deleted). The table's min/max statistics are updated to reflect the new file layout for data skipping."},
{domain:"Incremental Data Processing",q:"A DLT pipeline reads from a Bronze table and writes to Silver. Which DLT concept represents the Silver table transformation?",opts:["A view","A STREAMING TABLE or MATERIALIZED VIEW reading from the Bronze table using dlt.read() or dlt.read_stream()","A checkpoint table","A metadata table"],ans:1,exp:"In DLT, Bronze‚ÜíSilver transformations are defined as STREAMING TABLEs (for incremental processing from Bronze streaming table) or MATERIALIZED VIEWs (for full recomputation). DLT manages dependencies automatically based on dlt.read()/dlt.read_stream() calls."},
{domain:"Incremental Data Processing",q:"What does the Delta table property delta.enableChangeDataFeed = true enable?",opts:["Enables automatic streaming of the table to Kafka","Records insert, update, and delete operations in a _change_data directory alongside the table, queryable with readChangeFeed option","Enables automatic merging of incoming changes","Enables CDC replication to an external system"],ans:1,exp:"When CDF is enabled, Delta records row-level change data (with _change_type, _commit_version, _commit_timestamp) in _change_data files alongside regular data files. These are queryable with .option('readChangeFeed','true').option('startingVersion', N)."},
{domain:"Incremental Data Processing",q:"What is the 'Trigger.Once' equivalent in modern Databricks Structured Streaming?",opts:["trigger(once=True) ‚Äî still the recommended approach","trigger(availableNow=True) ‚Äî processes all available data in possibly multiple micro-batches then stops","trigger(processingTime='1 run')","trigger(maxBatches=1)"],ans:1,exp:"trigger(once=True) processed exactly one micro-batch and stopped. trigger(availableNow=True) is its modern replacement ‚Äî it processes all currently available data (potentially multiple micro-batches for efficiency) then stops, with better resource utilization and the same 'batch semantics, streaming guarantees' behavior."},
{domain:"Incremental Data Processing",q:"A Delta table has 500 files of 1MB each after many incremental writes. What command should be run and what is the expected result?",opts:["VACUUM ‚Äî removes small files and reduces count","OPTIMIZE ‚Äî compacts the 500 small files into fewer, larger files (~128MB each), reducing to ~4-5 files","ANALYZE TABLE ‚Äî updates statistics to improve query performance","REFRESH TABLE ‚Äî reloads file metadata"],ans:1,exp:"OPTIMIZE (bin-packing compaction) reads the 500 small files and rewrites them into fewer, larger files. With 500MB total and ~128MB target, you'd get ~4 files. This dramatically reduces file listing overhead and improves read performance."},
{domain:"Incremental Data Processing",q:"What causes 'STATE_STORE_PROVIDER' errors in Structured Streaming?",opts:["Missing checkpoint directory","A Kafka topic was deleted","State store corruption, version mismatch, or resource exhaustion ‚Äî Spark cannot read/write the streaming state properly","Python UDF raised an exception in the streaming context"],ans:2,exp:"State store errors indicate problems with how Spark persists aggregation/join state across micro-batches. Common causes: checkpoint directory corruption, Spark version upgrade breaking state format compatibility, disk space exhaustion, or excessive state size causing OOM."},
{domain:"Incremental Data Processing",q:"A streaming query uses groupBy('user_id').count() without watermark. What happens over time?",opts:["The count resets every micro-batch","State accumulates indefinitely ‚Äî the engine tracks all user_ids ever seen, growing memory usage without bound","Only the last 1000 user_ids are tracked","The query automatically drops users inactive for 24 hours"],ans:1,exp:"Without watermark, stateful streaming operations (aggregations, deduplication, stream-stream joins) keep state for ALL keys ever seen. This causes unbounded state growth that will eventually OOM the executor. Always use withWatermark() for stateful streaming."},
{domain:"Incremental Data Processing",q:"What does spark.readStream.format('delta').option('maxFilesPerTrigger', 10).load(path) control?",opts:["Number of streaming queries running in parallel","Limits the number of new Delta files processed per micro-batch ‚Äî useful for controlling throughput and backpressure","Maximum number of files in the Delta table","Number of partitions per micro-batch"],ans:1,exp:"maxFilesPerTrigger limits how many new Delta files (commits) are processed per micro-batch, controlling data ingestion rate. This is useful for backpressure management or smooth catches-up after downtime. maxBytesPerTrigger does the same for byte-based control."},
{domain:"Incremental Data Processing",q:"Which DLT expectation fails the entire pipeline if ANY record violates the constraint?",opts:["@dlt.expect","@dlt.expect_or_drop","@dlt.expect_or_fail","@dlt.expect_all"],ans:2,exp:"@dlt.expect_or_fail raises a pipeline failure immediately on the first constraint violation. Use this for critical invariants where bad data must never pass. @dlt.expect logs violations as metrics without dropping. @dlt.expect_or_drop removes bad rows and logs counts."},
{domain:"Incremental Data Processing",q:"What is the purpose of Delta Lake's _delta_log directory?",opts:["Stores raw data files","Contains JSON transaction log files and Parquet checkpoint files that record all changes (adds, removes, schema changes, table properties) to the table","Stores streaming checkpoints","Contains OPTIMIZE and VACUUM operation logs"],ans:1,exp:"The _delta_log directory is the transaction log (WAL - Write-Ahead Log). Each commit is a JSON file with add/remove file entries, statistics, and metadata. Every 10 commits a Parquet checkpoint summarizes the state. This enables ACID, time travel, and schema enforcement."},
{domain:"Incremental Data Processing",q:"How does Auto Loader handle schema inference when cloudFiles.inferColumnTypes = true?",opts:["Scans all files on first run to infer types","Infers types from a sample of files and persists the schema to schemaLocation; evolves schema when new columns or type changes are detected","Requires manual schema specification even when set to true","Infers schema from file names only"],ans:1,exp:"Auto Loader samples incoming files to infer column types. With cloudFiles.inferColumnTypes=true and a schemaLocation, the inferred schema persists between runs. Schema evolution is handled according to cloudFiles.schemaEvolutionMode (default: addNewColumns)."},
{domain:"Incremental Data Processing",q:"What is a DLT pipeline's 'Enhanced Autoscaling' feature?",opts:["Automatically adds GPU nodes when ML workloads are detected","Scales cluster workers based on streaming backlog and query complexity, separate from Databricks Enhanced Autoscaling for batch","Enables automatic DLT pipeline scheduling","Scales SQL Warehouse nodes for DLT SQL queries"],ans:1,exp:"DLT Enhanced Autoscaling monitors pipeline backlog and data volume, scaling workers up when the pipeline is behind and down when caught up. It's optimized for streaming workloads with variable data rates, independent of standard Databricks Enhanced Autoscaling."},
{domain:"Incremental Data Processing",q:"A MERGE INTO statement in Delta Lake ‚Äî what does WHEN NOT MATCHED BY SOURCE THEN DELETE do?",opts:["Deletes source rows not present in target","Deletes target rows that have no matching source row ‚Äî useful for full sync CDC where source represents complete current state","Deletes all rows from both tables","This clause syntax is invalid in Delta Lake"],ans:1,exp:"WHEN NOT MATCHED BY SOURCE THEN DELETE removes target rows that don't appear in the source. This implements a full sync: the target becomes an exact copy of the source after the merge. Available in Delta Lake 2.0+/Spark 3.4+."},
{domain:"Incremental Data Processing",q:"What is the minimum spark.databricks.delta.retentionDurationCheck.enabled = false used for?",opts:["Disabling all VACUUM operations","Overriding the safety check that prevents VACUUM RETAIN 0 HOURS, allowing immediate deletion of files older than the retention period","Disabling Delta's schema enforcement","Disabling transaction log retention"],ans:1,exp:"By default, Databricks prevents VACUUM with retention < 7 days to avoid breaking active time travel queries. Setting retentionDurationCheck.enabled=false disables this safety check, allowing VACUUM RETAIN 0 HOURS to immediately delete all unreferenced files. Use with caution."},
{domain:"Incremental Data Processing",q:"What does delta.logRetentionDuration = '30 days' control?",opts:["How long data files are retained before VACUUM removes them","How long transaction log files (_delta_log) are retained, controlling how far back time travel queries can go","The retention period for CDF change data files","How long streaming checkpoints are retained"],ans:1,exp:"logRetentionDuration controls how long Delta keeps transaction log files. Longer retention enables longer time travel windows. Note: this is separate from file retention (delta.deletedFileRetentionDuration). For time travel, both log files AND data files must be retained."},
{domain:"Incremental Data Processing",q:"A streaming job reads from Kafka. Which configuration ensures messages aren't skipped if the stream restarts after being down for more than Kafka's retention period?",opts:["Set auto.offset.reset=latest","Set auto.offset.reset=earliest and ensure checkpoint is preserved; if checkpoint is lost with expired offsets, set to earliest to restart from beginning","Kafka retention cannot be configured ‚Äî use Delta as source instead","Increase Spark micro-batch interval to match Kafka retention"],ans:1,exp:"If a streaming job is down longer than Kafka's retention period, the checkpointed offsets may no longer be available in Kafka. Setting failOnDataLoss=false causes Spark to skip to the earliest available offset rather than failing. Plan for this scenario by either increasing Kafka retention or using Delta as a more durable source."},
{domain:"Incremental Data Processing",q:"What is the purpose of Delta Lake's min/max statistics stored per file?",opts:["Used by OPTIMIZE to prioritize which files to compact","Enable data skipping: the query engine uses per-file min/max values for filter columns to skip files that cannot contain matching rows","Provide information to VACUUM for deletion ordering","Used by Auto Loader to detect new files"],ans:1,exp:"Delta stores min/max (and null count) statistics for each data file's columns. When a query has filter predicates, the engine checks these statistics to skip files whose min/max range excludes the filter value. ZORDER improves skipping by clustering related values together in files."},
{domain:"Incremental Data Processing",q:"In DLT, what is a 'pipeline run' vs. a 'pipeline update'?",opts:["They are the same thing","A pipeline run is the cluster lifecycle; a pipeline update is a single execution of the pipeline's tables and transformations","Pipeline runs are for production; updates are for development","Updates process only new data; runs process all data"],ans:1,exp:"A DLT pipeline update is one execution cycle that processes data and updates tables. Multiple updates can happen within the same pipeline run (cluster lifecycle). In Development mode, you can trigger multiple updates without restarting the cluster (pipeline run stays alive)."},
{domain:"Incremental Data Processing",q:"What does the CLONE command do in Delta Lake?",opts:["Creates a shallow copy of a table's schema only","Creates either a shallow clone (references original files) or deep clone (copies all files) of a table at a specific version, creating an independent Delta table","Duplicates the entire cluster and its data","Replicates a Delta table to another cloud region"],ans:1,exp:"CREATE TABLE target CLONE source [VERSION AS OF N] creates a Delta table clone. Shallow clone references the source's files (fast, space-efficient, but tied to source files). Deep clone copies all files (fully independent). Both clones support independent writes using Delta's transaction protocol."},
{domain:"Incremental Data Processing",q:"What is the relationship between the Delta transaction log and ACID properties?",opts:["Delta achieves ACID only through file locking","The transaction log is a write-ahead log: each commit is atomically written as a new JSON file. Readers see a consistent snapshot; writers use optimistic concurrency control ‚Äî together providing Atomicity, Consistency, Isolation, and Durability","ACID is provided by the underlying cloud storage, not Delta","Delta only guarantees atomicity, not isolation"],ans:1,exp:"Delta's transaction log implements ACID: Atomicity (a commit is a single atomic file write), Consistency (schema/constraints enforced), Isolation (readers see only committed versions; writers use optimistic concurrency with conflict detection), Durability (log files in durable cloud storage)."},
{domain:"Incremental Data Processing",q:"When running MERGE INTO with both WHEN MATCHED UPDATE and WHEN NOT MATCHED INSERT, what happens if the source has duplicate keys?",opts:["The last matching source row wins","By default, Delta raises an error ‚Äî a MERGE source must not have duplicate join keys for the same target row (produces MERGE conflict)","All matching source rows are applied in sequence","Spark automatically deduplicates source rows before MERGE"],ans:0,exp:"If multiple source rows match the same target row, Delta raises an error (AnalysisException). MERGE semantics require each target row to match at most one source row. Deduplicate the source before MERGE using dropDuplicates() or a CTE with ROW_NUMBER()."},
{domain:"Incremental Data Processing",q:"What is the Delta Lake 'writer version' and 'reader version' in DESCRIBE DETAIL?",opts:["The number of writers and readers currently connected","Protocol version numbers that indicate which Delta Lake features are required to read/write the table; tables with higher versions require newer Delta Lake clients","The Spark version used to write/read the table","The number of table schema versions in history"],ans:1,exp:"Delta protocol versions define required feature sets. A table with writerVersion=2 requires a writer that supports Delta protocol 2 features (e.g., delta writer protocol enabling certain features). Tables automatically upgrade their protocol version when advanced features are enabled."},
{domain:"Incremental Data Processing",q:"Which Auto Loader option would you set to handle incoming files that contain data for previously processed time windows?",opts:["cloudFiles.allowOutOfOrderFiles=true","cloudFiles.backfillInterval with a cloudFiles.includeExistingFiles setting","This cannot be handled by Auto Loader","Use Structured Streaming with Kafka instead"],ans:1,exp:"cloudFiles.backfillInterval (e.g., '1 day') periodically triggers a file listing to discover and process files that may have been backdated or uploaded late. Combined with checkpointing, Auto Loader tracks which files have been processed to avoid reprocessing."},
{domain:"Incremental Data Processing",q:"What is a key difference between a DLT STREAMING TABLE and a regular Structured Streaming writeStream?",opts:["STREAMING TABLEs cannot use Kafka as source","DLT manages checkpoints, recovery, data quality, and lineage automatically; manual writeStream requires you to manage these yourself","STREAMING TABLEs use different Spark APIs internally","Manual writeStream is faster than DLT"],ans:1,exp:"DLT abstracts operational complexity: automatic checkpoint management, intelligent retry/recovery, built-in data quality expectations, lineage tracking, and pipeline orchestration. Manual Structured Streaming gives more control but requires you to handle all of this explicitly."},
{domain:"Incremental Data Processing",q:"What is the effect of setting delta.autoOptimize.autoCompact = true on a Delta table?",opts:["Triggers OPTIMIZE after every write","Automatically compacts small files to target size when the total number of small files exceeds a threshold, during write operations","Enables OPTIMIZE to run on a cron schedule","Disables manual OPTIMIZE commands"],ans:1,exp:"Auto Compaction (delta.autoOptimize.autoCompact=true) coalesces small files incrementally after writes when the number of small files exceeds a threshold. It's less aggressive than Optimized Writes but reduces small file accumulation over time without requiring manual OPTIMIZE runs."},
{domain:"Incremental Data Processing",q:"Which command shows the current streaming query progress including input rows per second and processing throughput?",opts:["spark.streams.active[0].explain()","query.lastProgress or query.recentProgress ‚Äî returns a dict/JSON with inputRowsPerSecond, processedRowsPerSecond, and trigger timing","spark.streams.metrics()","query.status.progress"],ans:1,exp:"query.lastProgress returns a Python dict with detailed progress metrics from the last micro-batch: inputRowsPerSecond, processedRowsPerSecond, numInputRows, trigger timing, and state operator stats. query.recentProgress returns the last several batches."},
{domain:"Incremental Data Processing",q:"What is the purpose of withEventTimeOrder in DLT APPLY CHANGES INTO?",opts:["Sorts the output table by event time","Ensures CDC changes are applied in event time order rather than arrival order, preventing out-of-order updates from overwriting newer records","Forces chronological file processing","Enables time travel on the target table"],ans:1,exp:"APPLY CHANGES INTO with SEQUENCE BY processes CDC events in the order specified by the sequence column (often an event timestamp). Events with lower sequence values for the same key are ignored if a newer value has already been applied, preventing out-of-order CDC from corrupting data."},

// PRODUCTION PIPELINES (25 new)
{domain:"Production Pipelines",q:"What is the purpose of Databricks Asset Bundles (DABs)?",opts:["A UI tool for managing cluster bundles","A framework for defining Databricks resources (jobs, pipelines, permissions, clusters) as YAML configuration files, enabling infrastructure-as-code and CI/CD deployment","A package manager for Databricks notebooks","A bundle format for distributing Databricks notebooks"],ans:1,exp:"DABs use YAML to define all Databricks workspace resources. Resources can be version-controlled in Git and deployed to multiple environments (dev/staging/prod) using 'databricks bundle validate/deploy/run'. Supports parameterization via target-specific variables."},
{domain:"Production Pipelines",q:"In Databricks Workflows, what does setting a task's 'Run if dependency' to 'At least one succeeded' enable?",opts:["The task runs only if all previous tasks succeeded","The task runs if at least one upstream dependency succeeded ‚Äî even if some failed ‚Äî useful for partial results processing","The task runs immediately without waiting for dependencies","The task runs if none of the dependencies failed"],ans:1,exp:"Conditional execution in Databricks Workflows: 'At least one succeeded' means the task proceeds if 1+ of its direct dependencies completed successfully. Other options: 'All succeeded', 'All done' (regardless of outcome), 'None failed', 'If not succeeded'."},
{domain:"Production Pipelines",q:"Which Databricks feature allows you to repair and re-run only failed tasks in a Databricks Job?",opts:["Job cloning","Repair Run ‚Äî re-executes only failed/skipped tasks using the original job parameters without re-running successful tasks","Task-level retry configuration","A new job run must always be triggered from the beginning"],ans:1,exp:"Repair Run (in the Jobs UI or via API: jobs/runs/repair) lets you re-run failed or skipped tasks in an existing run without re-running successfully completed tasks. The repaired run reuses the original run's context, parameters, and cluster config for failed tasks."},
{domain:"Production Pipelines",q:"What is the recommended approach for deploying Databricks Jobs to a production environment using CI/CD?",opts:["Manual copy of notebooks from staging workspace","Use Databricks Asset Bundles with a CI/CD pipeline (GitHub Actions, Azure DevOps) that runs 'databricks bundle deploy' on merge to main branch","Export job JSON and import manually","Use %run to include notebooks from a Git repo directly"],ans:1,exp:"DABs + CI/CD pipeline is the recommended approach. PR ‚Üí automated tests ‚Üí merge to main ‚Üí pipeline runs 'databricks bundle deploy' to production workspace. Environment-specific variables (cluster sizes, catalog names) are defined in bundle targets."},
{domain:"Production Pipelines",q:"A Databricks Job task fails with exit code 1. The task has max_retries=2. What happens next?",opts:["Job is immediately marked as failed","The task retries up to 2 more times (total 3 attempts) with the configured retry interval; if all retries fail, downstream tasks are skipped and the job is marked Failed","The task retries indefinitely until manually stopped","Only non-zero exit codes above 10 trigger retries"],ans:1,exp:"Any non-zero exit code (except codes configured as non-retryable) triggers retries. With max_retries=2, the task attempts once initially plus up to 2 retries. If all fail, dependent downstream tasks are skipped and the overall job is marked Failed."},
{domain:"Production Pipelines",q:"What is the key benefit of using a SQL task type in Databricks Workflows over a Notebook task with SQL cells?",opts:["SQL tasks run 10x faster","SQL tasks run on a SQL Warehouse (serverless or classic), providing better isolation, BI-grade performance, and no cluster management; Notebook SQL runs on a Spark cluster","SQL tasks support Unity Catalog; Notebook SQL does not","SQL tasks can return results directly to the user"],ans:1,exp:"SQL task type connects to a SQL Warehouse. This separates SQL ETL from Spark cluster workloads, uses a cost-optimized SQL engine (Photon), and supports concurrent queries without blocking the cluster. Ideal for BI-style SQL transformations in a workflow."},
{domain:"Production Pipelines",q:"How does Databricks Workflows handle time zones for scheduled jobs?",opts:["All jobs run in UTC only","The cron schedule can be configured with a specific time zone, so '0 9 * * *' runs at 9 AM in the specified local time zone","Time zones are set at the workspace level and apply to all jobs","Jobs automatically adjust for daylight saving time"],ans:1,exp:"Databricks Job schedules support specifying a timezone (e.g., 'America/New_York'). This ensures the job runs at the correct local time even across daylight saving time transitions. The underlying cron expression is still in the specified timezone."},
{domain:"Production Pipelines",q:"What is a Databricks Job's 'concurrent runs' setting?",opts:["Number of parallel tasks within the job","Controls whether multiple runs of the same job can execute simultaneously (allow) or whether new triggers are skipped/queued if a run is already active (skip or queue)","Number of clusters used by the job","Number of retries allowed per run"],ans:1,exp:"Concurrent runs policy: 'Allow' lets multiple runs execute simultaneously (default for API triggers). 'Skip' cancels new triggers if a run is already active. 'Queue' queues new triggers up to the max queue size. Important for preventing overlapping ETL jobs."},
{domain:"Production Pipelines",q:"Which approach is best for passing the output of one Databricks Job task as input to the next task?",opts:["Write to a shared DBFS path and read in the next task","Use task values: dbutils.jobs.taskValues.set(key, value) in the first task and dbutils.jobs.taskValues.get(taskKey, key) in the next task","Use global Python variables","Pass output through a Delta table always"],ans:1,exp:"Task values allow passing small data (strings, numbers, lists up to 48KB) between tasks without external storage. The producing task calls taskValues.set(); consuming tasks call taskValues.get(taskKey='upstream_task', key='my_key'). For large data, use Delta tables."},
{domain:"Production Pipelines",q:"What is a Databricks Job's 'run_if' condition and when is it useful?",opts:["Determines if the job should run based on calendar date","Sets the condition under which a task runs based on the outcome of its dependencies ‚Äî enabling error-handling tasks, cleanup tasks, and conditional branching","Controls which cluster the task runs on","Determines if results should be saved"],ans:1,exp:"run_if conditions: 'ALL_SUCCESS' (default), 'AT_LEAST_ONE_SUCCESS', 'NONE_FAILED', 'ALL_DONE', 'AT_LEAST_ONE_FAILED', 'ALL_FAILED'. Use to: run cleanup regardless of failures ('ALL_DONE'), create error-handling branches ('ALL_FAILED'), or run final steps even with partial failures."},
{domain:"Production Pipelines",q:"A data engineer needs to schedule a pipeline that should not start if the previous run hasn't finished. Which setting prevents overlapping runs?",opts:["Set task retries to 0","Set concurrent runs policy to 'Skip new run' or 'Queue'","Set a pre-check condition to verify no active runs","This requires external orchestration ‚Äî Databricks doesn't support this natively"],ans:1,exp:"Setting concurrent runs to 'Skip new run' (trigger is dropped if a run is active) or 'Queue' (trigger waits up to queue limit) prevents overlapping runs. This is critical for ETL pipelines where running twice simultaneously could cause data duplication or corruption."},
{domain:"Production Pipelines",q:"What does dbutils.notebook.run() return and what's its limitation?",opts:["Returns the notebook's last cell output; limited to 5MB","Returns the exit value from dbutils.notebook.exit() as a string; limited to 5MB and the called notebook runs on the same cluster synchronously","Returns the notebook URL for async access","Returns a JSON object with all cell outputs"],ans:1,exp:"dbutils.notebook.run() executes a notebook synchronously on the same cluster and returns the string passed to dbutils.notebook.exit(). The return value is limited to 5MB. For async execution or passing large data, use Databricks Jobs with task values or Delta tables."},
{domain:"Production Pipelines",q:"Which Databricks feature enables data engineers to monitor job SLA violations?",opts:["Spark UI alerts","Job duration warnings ‚Äî configure expected duration; Databricks sends notifications if a run exceeds the threshold","Unity Catalog audit logs","DBFS monitoring"],ans:1,exp:"Databricks Jobs support duration warning thresholds. If a run takes longer than the configured time, a notification is sent (email or webhook). Combined with 'on failure' and 'on start' notifications, this provides complete operational monitoring without external tooling."},
{domain:"Production Pipelines",q:"A data engineer wants to test a pipeline change locally before deploying to Databricks. Which tool supports this?",opts:["Databricks Simulator","Databricks Connect ‚Äî runs Spark DataFrame code on a remote Databricks cluster from a local IDE/terminal","Local Spark installation with Databricks libraries","This is not possible ‚Äî Databricks code must run on Databricks"],ans:1,exp:"Databricks Connect allows running PySpark code in a local IDE (VS Code, PyCharm) or terminal against a Databricks cluster. You get local debugging, unit testing, and IDE integration while the actual compute runs on Databricks. Useful for development and testing workflows."},
{domain:"Production Pipelines",q:"What is the purpose of a Databricks cluster policy's 'maxDBUHour' attribute?",opts:["Limits the maximum DBU usage per day across the workspace","Limits the maximum DBU per hour that a cluster created from this policy can consume, controlling cost","Sets a minimum compute guarantee","Limits the number of clusters that can be created per hour"],ans:1,exp:"maxDBUHour in a cluster policy sets a cap on the hourly DBU consumption for clusters created from that policy. This prevents users from accidentally spinning up extremely expensive clusters (many large instance types) while still giving them flexibility within the policy's constraints."},
{domain:"Production Pipelines",q:"When using Databricks Workflows for multi-hop ETL (Bronze ‚Üí Silver ‚Üí Gold), what is the best practice for task dependencies?",opts:["Run all tasks in parallel for speed","Configure Silver tasks to depend on Bronze tasks, and Gold tasks to depend on Silver tasks ‚Äî creating a DAG that enforces data availability ordering","Use time delays between tasks instead of explicit dependencies","Use a single notebook for all three hops to avoid dependency complexity"],ans:1,exp:"Explicit task dependencies in the job DAG ensure correct execution order: Bronze completes ‚Üí Silver starts ‚Üí Gold starts. This prevents race conditions, enables automatic failure propagation (Gold is skipped if Silver fails), and provides clear lineage in the job run history."},
{domain:"Production Pipelines",q:"What is the Databricks CLI command to run a specific job?",opts:["databricks job run --job-id 123","databricks jobs run-now --job-id 123","databricks run job 123","databricks execute job --id 123"],ans:1,exp:"'databricks jobs run-now --job-id 123' triggers an immediate run of the specified job. With DABs, you can also use 'databricks bundle run <resource_key>'. The CLI uses the Jobs API under the hood, equivalent to clicking 'Run Now' in the UI."},
{domain:"Production Pipelines",q:"A Databricks Job has 3 parallel tasks (A, B, C) converging to task D. If task B fails and tasks A and C succeed, what happens to task D?",opts:["D runs using outputs from A and C only","D is skipped because not all dependencies succeeded (with default run_if='ALL_SUCCESS')","D runs after a delay","The entire job is cancelled"],ans:1,exp:"With the default run_if='ALL_SUCCESS', task D only runs when ALL of its dependencies (A, B, and C) succeed. Since B failed, D is skipped. To handle partial failures, change D's run_if to 'AT_LEAST_ONE_SUCCESS' or add a failure-handling task with run_if='AT_LEAST_ONE_FAILED'."},
{domain:"Production Pipelines",q:"How do you pass a parameter to a Databricks notebook when running as a job?",opts:["Set environment variables in cluster config","Define widgets in the notebook (dbutils.widgets.text('param','default')); in the Job, set notebook_params={\"param\":\"value\"}","Hard-code parameters in the notebook","Use a config file in DBFS"],ans:1,exp:"Notebook parameters flow through widgets: define dbutils.widgets.text('name','default') in the notebook, then in the Job configuration set notebook_params={'name': 'value'}. The notebook reads the value with dbutils.widgets.get('name'). This enables parameterized, reusable notebook templates."},
{domain:"Production Pipelines",q:"What is the difference between a Databricks 'Triggered' DLT pipeline and a 'Continuous' DLT pipeline?",opts:["Triggered uses batch compute; Continuous uses streaming compute","Triggered processes data on a schedule or manual run then stops; Continuous runs 24/7 processing new data as it arrives","Triggered supports SQL; Continuous supports only Python","Continuous mode has higher data quality guarantees"],ans:1,exp:"Triggered mode: the pipeline runs on schedule or manual trigger, processes all new data, then terminates (cost-efficient for scheduled batch). Continuous mode: the pipeline runs perpetually on a long-running cluster, processing new data with low latency (suited for near-real-time requirements)."},
{domain:"Production Pipelines",q:"Which approach allows a Databricks Job to dynamically create tasks at runtime based on data (fan-out pattern)?",opts:["Hardcode all possible tasks in the job DAG","Use a ForEach task (Databricks Workflows ForEach task) that iterates over a list of inputs and runs a child task for each element","Use Python subprocess to launch new jobs","Use Spark parallel execution with RDD.foreachPartition()"],ans:1,exp:"The ForEach task in Databricks Workflows enables dynamic fan-out: configure a list of inputs (static or from a preceding task's output) and a child task template. Databricks creates parallel task instances, one per input value, enabling data-driven parallelism."},
{domain:"Production Pipelines",q:"A data engineer needs to validate data quality before loading to production. Which pattern is recommended in Databricks?",opts:["Load to production, then validate, then rollback if bad","Use Delta Lake expectations via DLT, or implement a validation task that fails the job before the load step ‚Äî preventing bad data from reaching production","Validate using a separate Spark job after production load","Use DBFS quarantine files for invalid records"],ans:1,exp:"Shift-left data quality: validate BEFORE writing to production. DLT expectations enforce quality declaratively. For non-DLT pipelines, add a validation task that checks quality metrics and uses dbutils.notebook.exit() with a non-zero code to halt the job if quality fails."},
{domain:"Production Pipelines",q:"What does the Databricks 'Event Log' for a DLT pipeline contain?",opts:["Spark event logs from the cluster","A structured log of all pipeline events: updates, table processing, data quality metrics, errors, and lineage information ‚Äî queryable as a Delta table","Only error messages from failed runs","Cluster autoscaling events"],ans:1,exp:"The DLT Event Log is a Delta table storing all pipeline lifecycle events: pipeline start/stop, table processing events, data quality expectation results (pass/fail/drop counts), error messages, and pipeline status changes. Query it with spark.read.format('delta').load(event_log_path)."},
{domain:"Production Pipelines",q:"How should a data engineer handle secrets that vary between dev/staging/production environments in Databricks?",opts:["Hardcode secrets per environment in notebooks","Use Databricks Secret Scopes with environment-specific scope names, and reference them via dbutils.secrets.get(scope=env_scope, key=...) using DAB variables to pass the correct scope","Store secrets in a Delta table with column masking","Use environment variables that are set manually before each deployment"],ans:1,exp:"Use separate secret scopes per environment (e.g., 'dev-secrets', 'prod-secrets'). Reference the scope name via a DAB variable (e.g., ${var.env}_secrets) that's set per deployment target. This separates secret management from code and enables environment-specific configuration without code changes."},

// DATA GOVERNANCE (20 new)
{domain:"Data Governance",q:"In Unity Catalog, what is the difference between GRANT and DENY privileges?",opts:["DENY overrides GRANT; both exist in Unity Catalog","Unity Catalog does not have a DENY command ‚Äî access is additive (grant-only). Revoke privileges using REVOKE, not DENY","DENY is for column-level access; GRANT is for table-level","GRANT requires admin; DENY can be used by any user"],ans:1,exp:"Unity Catalog uses an additive privilege model with no DENY. To remove access, use REVOKE. If a user has a privilege through group membership and direct assignment, both grants apply. There's no way to override a grant for a specific user ‚Äî use groups carefully to avoid unintended access."},
{domain:"Data Governance",q:"What Unity Catalog privilege allows a user to create child catalogs within a metastore?",opts:["CREATE CATALOG on the metastore","CREATE on the metastore","Metastore Admin role ‚Äî only metastore admins can create catalogs","USE CATALOG + CREATE"],ans:0,exp:"CREATE CATALOG privilege on the metastore (or metastore admin role) allows creating new catalogs. Regular users with only USE CATALOG cannot create new catalogs. Catalog-level CREATE allows creating schemas within a catalog."},
{domain:"Data Governance",q:"Which Unity Catalog object represents the top level of the 3-part naming hierarchy?",opts:["Schema","Metastore","Catalog","Workspace"],ans:2,exp:"Unity Catalog uses a 3-part naming: catalog.schema.table. The catalog is the top level, containing schemas, which contain tables, views, functions, and volumes. The metastore is the control plane that hosts all catalogs but is not part of the query naming hierarchy."},
{domain:"Data Governance",q:"What does INFORMATION_SCHEMA provide in Unity Catalog?",opts:["A view of Spark cluster information","SQL-queryable metadata views for catalogs, schemas, tables, columns, privileges, and routines ‚Äî following ANSI SQL standards","System audit logs","Delta table transaction history"],ans:1,exp:"INFORMATION_SCHEMA is a standard SQL schema available within each catalog (catalog.information_schema.*). It contains views like TABLES, COLUMNS, TABLE_PRIVILEGES, ROUTINES, and more. Use SQL queries to programmatically discover and audit metadata."},
{domain:"Data Governance",q:"Which privilege is required to run VACUUM on a Delta table in Unity Catalog?",opts:["SELECT only","MODIFY privilege on the table","ALL PRIVILEGES on the table","Workspace admin role"],ans:1,exp:"MODIFY privilege (which includes INSERT, UPDATE, DELETE, TRUNCATE) on the table is required to run VACUUM. Additionally, USE CATALOG and USE SCHEMA are needed for navigation. VACUUM modifies the table (deletes files) so it requires write-level privileges."},
{domain:"Data Governance",q:"A data engineer needs to share a Delta table from their Databricks environment with a partner company using a different cloud platform. Which feature enables this?",opts:["Cross-cloud Unity Catalog federation","Delta Sharing ‚Äî provides a share, recipient, and share link allowing the partner to query the table without copying data, even from non-Databricks platforms","External Location with public access","Databricks Repos shared across accounts"],ans:1,exp:"Delta Sharing lets you create a Share (collection of tables/schemas), add Recipients (partner organizations), and generate share activation links. Recipients query data using REST endpoints compatible with Spark, pandas, Power BI, and more ‚Äî no Databricks account required."},
{domain:"Data Governance",q:"What is a Unity Catalog 'Volume'?",opts:["A storage volume for cluster data","A governance object representing a storage location for non-tabular data (files: CSV, JSON, images, ML models) with Unity Catalog access controls","A measure of data ingestion rate","A cluster configuration for storage I/O optimization"],ans:1,exp:"Unity Catalog Volumes provide governed access to files (non-tabular data) in cloud storage. Internal volumes are managed by UC; external volumes reference external storage locations. Files in volumes are accessed via /Volumes/catalog/schema/volume/path with full UC access controls."},
{domain:"Data Governance",q:"What does GRANT SELECT ON ALL TABLES IN SCHEMA main.sales TO GROUP analysts do versus GRANT SELECT ON TABLE main.sales.orders TO GROUP analysts?",opts:["They are equivalent","GRANT ON ALL TABLES applies to all currently existing tables in the schema; GRANT ON TABLE applies to one specific table","GRANT ON ALL TABLES also includes future tables","GRANT ON ALL TABLES requires metastore admin"],ans:1,exp:"ALL TABLES IN SCHEMA grants SELECT on every table existing at the time of the grant. Future tables created afterward are NOT included. To include future tables, additionally run GRANT SELECT ON FUTURE TABLES IN SCHEMA. Use both together for comprehensive access."},
{domain:"Data Governance",q:"What is the purpose of Unity Catalog's 'data products' concept?",opts:["Selling data in the Databricks Marketplace","Logical groupings of related tables, volumes, functions, and documentation that represent a consumable data asset ‚Äî enabling data mesh-style architectures","A type of external connection to data APIs","A billing model for data storage"],ans:1,exp:"Data products in Unity Catalog organize related assets (tables, views, functions, volumes, ML models) under a schema with documentation, owners, and tags. This enables data mesh: domain teams own and publish data products that other teams discover and consume through Unity Catalog."},
{domain:"Data Governance",q:"How does Unity Catalog handle cross-workspace data access for a user with permissions in one workspace but not another?",opts:["Access is workspace-specific ‚Äî permissions don't transfer","Unity Catalog privileges are metastore-wide. A user granted SELECT on a table in workspace A automatically has the same access in workspace B attached to the same metastore","Cross-workspace access requires Delta Sharing","Workspace admins must replicate permissions to each workspace"],ans:1,exp:"Unity Catalog privileges are tied to the metastore, not individual workspaces. If a user has SELECT on a table and accesses it from any workspace attached to the same metastore, the same permissions apply. This enables consistent governance across all workspaces without duplicating configurations."},
{domain:"Data Governance",q:"What is the purpose of the Unity Catalog 'owner' concept?",opts:["Owners pay for the storage costs","The owner of an object (catalog/schema/table) can manage privileges on that object, including granting others access ‚Äî acting as a delegated admin","Owners are the only ones who can query the object","Owners receive automatic ALL PRIVILEGES"],ans:1,exp:"Ownership provides delegated governance: the owner can GRANT and REVOKE privileges on their owned objects to other users/groups, and manage object properties. This enables a distributed governance model where domain teams own their catalogs/schemas and manage access without requiring metastore admin privileges."},
{domain:"Data Governance",q:"What is the Databricks Marketplace?",opts:["An internal company marketplace for sharing Databricks notebooks","A platform where data providers can publish and share data products, notebooks, ML models, and delta sharing datasets ‚Äî and consumers can discover and access them","A billing marketplace for Databricks services","A plugin marketplace for Databricks extensions"],ans:1,exp:"Databricks Marketplace enables publishing and discovering data, ML models, and solution accelerators. Data providers publish datasets (via Delta Sharing) and other assets. Consumers browse the marketplace in their workspace and install assets directly into their environment."},
{domain:"Data Governance",q:"Which SQL statement moves an existing external table to Unity Catalog governance without moving the underlying data?",opts:["MIGRATE TABLE old_table TO unity_catalog","There is no migration command ‚Äî tables must be recreated","SYNC TABLE old_table TO catalog.schema.new_table","CREATE TABLE catalog.schema.new_table LOCATION 'existing_path' ‚Äî register the existing files as a new UC external table"],ans:3,exp:"To bring external data under Unity Catalog governance without moving data: create an external table in UC pointing to the existing storage location using LOCATION. The files remain in place; UC controls access to them via the External Location and Storage Credential."},
{domain:"Data Governance",q:"What event triggers Unity Catalog to capture column-level lineage?",opts:["Manual lineage annotation via API","Any query (SQL, DataFrame API, DLT) executed through a Unity Catalog-governed cluster ‚Äî automatically without user action","Explicit tagging of columns with lineage tags","Running the ANALYZE TABLE command"],ans:1,exp:"Unity Catalog automatically captures column-level lineage for all queries executed through clusters/SQL Warehouses attached to a Unity Catalog metastore. No instrumentation needed. Lineage is queryable via the UI (table details) or system.lineage.* system tables."},
{domain:"Data Governance",q:"A company needs to prove data access for HIPAA compliance. Which Unity Catalog feature provides the required audit trail?",opts:["Delta DESCRIBE HISTORY for each table","Unity Catalog audit logs in system.access.audit ‚Äî capturing every data access event with user identity, table, operation, timestamp, and IP","Databricks notebook export history","Spark UI query history"],ans:1,exp:"system.access.audit (and the cloud audit log delivery) records every access event: user/service principal identity, accessed object (catalog/schema/table), operation type (SELECT, INSERT, etc.), timestamp, IP address, workspace, and cluster. This satisfies HIPAA, GDPR, SOC2, and other regulatory audit requirements."},
{domain:"Data Governance",q:"What is required to create a Unity Catalog external table pointing to a GCS bucket?",opts:["Only the GCS bucket URL","A Storage Credential (GCS service account) + External Location (mapping the GCS path to the credential) + CREATE EXTERNAL TABLE privilege on the External Location","Direct GCS access with DBFS mount","Unity Catalog does not support GCS ‚Äî only S3 and ADLS"],ans:1,exp:"For GCS external tables: (1) Storage Credential with GCS service account credentials, (2) External Location mapping the GCS path to that credential, (3) User has CREATE EXTERNAL TABLE privilege on the External Location. The same pattern applies for S3 (IAM role) and ADLS (service principal)."},
{domain:"Data Governance",q:"Which feature in Unity Catalog enables you to define which service principals can impersonate other identities for automated workflows?",opts:["Token management policies","Service principal groups with delegation rights","Service principal impersonation is not supported in Unity Catalog","Token-based access scoped to service principals configured in account settings"],ans:3,exp:"Service principals in Unity Catalog are managed at the account level. For workflows, service principals are granted appropriate Unity Catalog privileges directly. Impersonation patterns use OAuth M2M (machine-to-machine) tokens. Token policies control lifetime and rotation."},
{domain:"Data Governance",q:"What happens to Delta Sharing shares when a shared table's schema changes?",opts:["The share automatically breaks and must be recreated","Recipients continue to receive data; schema changes are propagated to recipients if they refresh their table metadata ‚Äî Delta Sharing is schema-evolution aware","Schema changes must be manually published to each share","Recipients are notified but must opt-in to receive the new columns"],ans:1,exp:"Delta Sharing handles schema evolution transparently. When the provider's table gains new columns, recipients who refresh their table definition see the new schema. New columns appear in subsequent queries. Providers should communicate breaking schema changes to recipients."},
{domain:"Data Governance",q:"A data engineer wants to implement attribute-based access control where only users with 'region=EMEA' attribute can see EMEA data. Which Unity Catalog pattern achieves this?",opts:["Create separate catalogs per region","Implement a Row Filter that checks the user's custom group membership (is_member('emea_team')) or uses session context","Delta table column encryption with region-based keys","This requires separate workspaces per region"],ans:1,exp:"Row Filters in Unity Catalog use SQL functions that return boolean expressions. By checking is_member('emea_team') or current_user() against a mapping table, the filter restricts rows to only those matching the user's authorized regions. Applied transparently at the table level for all access patterns."},
{domain:"Data Governance",q:"Which Unity Catalog privilege allows a user to INSERT, UPDATE, and DELETE but NOT truncate or drop the table?",opts:["WRITE","MODIFY","DML","INSERT + UPDATE + DELETE must be granted separately"],ans:1,exp:"MODIFY privilege in Unity Catalog covers INSERT, UPDATE, DELETE, and MERGE operations but NOT TRUNCATE or DROP TABLE (which requires additional privileges or ownership). MODIFY is the standard write-access privilege for data manipulation without destructive operations."},

];  // END OF ADDITIONAL QUESTIONS

// Merge with the original ALL_QUESTIONS
const FULL_ALL_QUESTIONS = [...ALL_QUESTIONS, ...ADDITIONAL_QUESTIONS];

// Remove duplicates by question text (safety)
const seen2 = new Set();
const QUESTIONS = FULL_ALL_QUESTIONS.filter(q => {
  const key = q.q.slice(0, 70);
  if (seen2.has(key)) return false;
  seen2.add(key);
  return true;
});

console.log(`Loaded ${QUESTIONS.length} questions across all domains`);
</script>
<script>// app.js ‚Äî Full exam platform logic

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// STORAGE HELPERS (localStorage)
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
const STORAGE_KEY = 'db_exam_users';
const SESSION_KEY = 'db_exam_session';

function getUsers() {
  try { return JSON.parse(localStorage.getItem(STORAGE_KEY) || '{}'); } catch { return {}; }
}
function saveUsers(users) {
  localStorage.setItem(STORAGE_KEY, JSON.stringify(users));
}
function getCurrentUser() {
  try { return JSON.parse(sessionStorage.getItem(SESSION_KEY) || 'null'); } catch { return null; }
}
function setCurrentUser(user) {
  sessionStorage.setItem(SESSION_KEY, JSON.stringify(user));
}
function clearCurrentUser() {
  sessionStorage.removeItem(SESSION_KEY);
}
function getUserData(username) {
  const users = getUsers();
  return users[username] || null;
}
function saveUserData(username, data) {
  const users = getUsers();
  users[username] = data;
  saveUsers(users);
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// AUTH
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
let activeTab = 'login';

function switchTab(tab) {
  activeTab = tab;
  document.querySelectorAll('.auth-tab').forEach((el, i) => {
    el.classList.toggle('active', (i === 0 && tab === 'login') || (i === 1 && tab === 'register'));
  });
  document.getElementById('tab-login').style.display = tab === 'login' ? 'block' : 'none';
  document.getElementById('tab-register').style.display = tab === 'register' ? 'block' : 'none';
  hideAuthError();
}

function showAuthError(msg) {
  const el = document.getElementById('auth-error');
  el.textContent = msg;
  el.classList.add('show');
}
function hideAuthError() {
  document.getElementById('auth-error').classList.remove('show');
}

function doLogin() {
  const username = document.getElementById('login-user').value.trim();
  const password = document.getElementById('login-pass').value;
  if (!username) return showAuthError('Please enter your username.');
  if (!password) return showAuthError('Please enter your password.');

  const users = getUsers();
  if (!users[username]) {
    // Auto-create account for demo convenience
    const newUser = { username, password, name: username, history: [], createdAt: Date.now() };
    users[username] = newUser;
    saveUsers(users);
    setCurrentUser({ username, name: username });
    loadDashboard();
    return;
  }
  const user = users[username];
  if (user.password !== password) return showAuthError('Incorrect password. Please try again.');
  setCurrentUser({ username, name: user.name || username });
  loadDashboard();
}

function doRegister() {
  const name = document.getElementById('reg-name').value.trim();
  const username = document.getElementById('reg-user').value.trim();
  const password = document.getElementById('reg-pass').value;
  if (!name) return showAuthError('Please enter your full name.');
  if (!username) return showAuthError('Please choose a username.');
  if (username.length < 3) return showAuthError('Username must be at least 3 characters.');
  if (password.length < 6) return showAuthError('Password must be at least 6 characters.');
  const users = getUsers();
  if (users[username]) return showAuthError('Username already taken. Please choose another.');
  users[username] = { username, password, name, history: [], createdAt: Date.now() };
  saveUsers(users);
  setCurrentUser({ username, name });
  loadDashboard();
}

function doLogout() {
  clearCurrentUser();
  showScreen('auth');
}

// Key events for auth
document.getElementById('login-pass').addEventListener('keydown', e => { if (e.key === 'Enter') doLogin(); });
document.getElementById('reg-pass').addEventListener('keydown', e => { if (e.key === 'Enter') doRegister(); });

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// SCREENS
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
function showScreen(id) {
  document.querySelectorAll('.screen').forEach(s => s.classList.remove('active'));
  document.getElementById(`screen-${id}`).classList.add('active');
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// DASHBOARD
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
let examType = 'full'; // 'full', 'quick', 'mini', 'domain'

function setExamType(type) {
  examType = type;
  ['full','quick','mini','domain'].forEach(t => {
    const el = document.getElementById(`type-${t}`);
    if (el) el.classList.toggle('active', t === type);
  });
  const domainArea = document.getElementById('domain-select-area');
  if (domainArea) domainArea.style.display = type === 'domain' ? 'block' : 'none';
}

function loadDashboard() {
  showScreen('dashboard');
  const session = getCurrentUser();
  if (!session) return;
  const userData = getUserData(session.username);
  const name = session.name || session.username;

  document.getElementById('hero-name').textContent = name.split(' ')[0];
  document.getElementById('nav-username').textContent = name;
  document.getElementById('nav-avatar').textContent = name[0].toUpperCase();

  const history = userData?.history || [];
  const examsCount = history.length;
  const scores = history.map(h => h.pct);
  const bestScore = scores.length ? Math.max(...scores) : null;
  const avgScore = scores.length ? Math.round(scores.reduce((a, b) => a + b, 0) / scores.length) : null;

  document.getElementById('stat-exams-taken').textContent = examsCount;
  document.getElementById('stat-best-score').textContent = bestScore !== null ? bestScore + '%' : '‚Äî';
  document.getElementById('stat-avg-score').textContent = avgScore !== null ? avgScore + '%' : '‚Äî';

  renderHistoryTable(history);
  renderDomainPerf(history);
  renderTrendChart(history);
}

function renderHistoryTable(history) {
  const container = document.getElementById('history-container');
  if (history.length === 0) {
    container.innerHTML = '<div class="empty-state"><div class="empty-icon">üìã</div><p>No exams taken yet. Start your first exam to track progress!</p></div>';
    return;
  }
  const rows = [...history].reverse().slice(0, 20).map((h, i) => {
    const pct = h.pct;
    const pillClass = pct >= 80 ? 'pass' : pct >= 70 ? 'pass' : pct >= 55 ? 'warn' : 'fail';
    const date = new Date(h.timestamp).toLocaleDateString('en-US', { month: 'short', day: 'numeric', year: 'numeric' });
    const time = new Date(h.timestamp).toLocaleTimeString('en-US', { hour: '2-digit', minute: '2-digit' });
    const elapsed = h.elapsed ? formatTime(h.elapsed) : '‚Äî';
    const label = h.examLabel || 'Full Exam';
    const trend = i > 0 ? (pct - [...history].reverse()[i-1].pct) : null;
    const trendStr = trend !== null ? (trend > 0 ? `<span style="color:var(--green);font-size:10px">‚ñ≤${trend}%</span>` : trend < 0 ? `<span style="color:var(--red);font-size:10px">‚ñº${Math.abs(trend)}%</span>` : '') : '';
    return `<tr>
      <td style="color:var(--text3);font-family:var(--mono);font-size:11px">#${history.length - i}</td>
      <td>${date} <span style="color:var(--text3);font-size:11px">${time}</span></td>
      <td style="font-size:11px;color:var(--text2)">${label}</td>
      <td><span class="score-pill ${pillClass}">${pct}%</span> ${trendStr}</td>
      <td style="font-family:var(--mono);font-size:12px;color:var(--green)">${h.correct}</td>
      <td style="font-family:var(--mono);font-size:12px;color:var(--red)">${h.total - h.correct}</td>
      <td style="font-family:var(--mono);font-size:12px">${h.total} Q</td>
      <td style="font-family:var(--mono);font-size:11px;color:var(--text3)">${elapsed}</td>
      <td><span class="score-pill ${h.passed ? 'pass' : 'fail'}" style="font-size:10px">${h.passed ? 'PASS' : 'FAIL'}</span></td>
    </tr>`;
  }).join('');
  container.innerHTML = `<table class="history-table">
    <thead><tr>
      <th>#</th><th>Date</th><th>Type</th><th>Score</th><th>‚úì</th><th>‚úó</th><th>Questions</th><th>Time</th><th>Result</th>
    </tr></thead>
    <tbody>${rows}</tbody>
  </table>`;
}

function renderDomainPerf(history) {
  const container = document.getElementById('domain-perf-container');
  if (history.length === 0) {
    container.innerHTML = '<div class="empty-state"><div class="empty-icon">üìä</div><p>Complete an exam to see domain performance</p></div>';
    return;
  }

  // Aggregate domain stats across all exams
  const domainAgg = {};
  history.forEach(exam => {
    (exam.domainStats || []).forEach(ds => {
      if (!domainAgg[ds.domain]) domainAgg[ds.domain] = { correct: 0, total: 0 };
      domainAgg[ds.domain].correct += ds.correct;
      domainAgg[ds.domain].total += ds.total;
    });
  });

  const domains = Object.entries(domainAgg).sort((a, b) => {
    const pctA = a[1].correct / a[1].total;
    const pctB = b[1].correct / b[1].total;
    return pctA - pctB; // weakest first
  });

  if (domains.length === 0) {
    container.innerHTML = '<div class="empty-state"><p>Domain data not available</p></div>';
    return;
  }

  container.innerHTML = domains.map(([domain, stats]) => {
    const pct = Math.round((stats.correct / stats.total) * 100);
    const color = pct >= 80 ? 'var(--green)' : pct >= 65 ? 'var(--yellow)' : 'var(--red)';
    const shortName = domain.replace('ELT with Spark & Python', 'ELT/Spark').replace('Incremental Data Processing', 'Incremental').replace('Production Pipelines', 'Pipelines').replace('Data Governance', 'Governance').replace('Lakehouse Platform', 'Lakehouse');
    return `<div class="domain-mini">
      <div class="domain-mini-top">
        <span class="domain-mini-name">${shortName}</span>
        <span class="domain-mini-pct" style="color:${color}">${pct}%</span>
      </div>
      <div class="domain-mini-bar">
        <div class="domain-mini-fill" style="width:${pct}%;background:${color}"></div>
      </div>
    </div>`;
  }).join('');
}

function renderTrendChart(history) {
  const trendCard = document.getElementById('trend-card');
  if (history.length < 2) { trendCard.style.display = 'none'; return; }
  trendCard.style.display = 'block';

  const last10 = history.slice(-10);
  const maxPct = 100;
  const passLine = 70;

  document.getElementById('trend-chart').innerHTML = last10.map((h, i) => {
    const pct = h.pct;
    const heightPct = (pct / maxPct) * 100;
    const color = pct >= 70 ? 'var(--green)' : pct >= 55 ? 'var(--yellow)' : 'var(--red)';
    const passLineTop = (1 - passLine / maxPct) * 100;
    return `<div class="chart-bar-wrap" title="Exam #${history.length - last10.length + i + 1}: ${pct}%">
      <div class="chart-bar-outer">
        <div class="chart-bar-inner" style="height:${heightPct}%;background:${color}"></div>
      </div>
      <div class="chart-bar-label" style="font-size:10px;color:${color}">${pct}%</div>
    </div>`;
  }).join('');
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// EXAM STATE
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
let examState = {
  questions: [],
  current: 0,
  answers: [],      // null | index (0-3)
  flags: [],
  timerInterval: null,
  secondsLeft: 0,
  startedAt: null
};

function shuffle(arr) {
  const a = [...arr];
  for (let i = a.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [a[i], a[j]] = [a[j], a[i]];
  }
  return a;
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// START EXAM
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
function startExam(mode) {
  let qCount, minutes, examLabel, sourcePool;

  if (examType === 'domain') {
    const selectedDomain = document.getElementById('domain-select')?.value || 'Lakehouse Platform';
    sourcePool = QUESTIONS.filter(q => q.domain === selectedDomain);
    qCount = Math.min(25, sourcePool.length);
    minutes = 35;
    examLabel = `Domain: ${selectedDomain.replace('ELT with Spark & Python','ELT/Spark').replace('Incremental Data Processing','Incremental').replace('Production Pipelines','Pipelines')}`;
  } else if (examType === 'mini') {
    qCount = 15;
    minutes = 20;
    examLabel = 'Mini Quiz';
    sourcePool = QUESTIONS;
  } else if (examType === 'quick') {
    qCount = 30;
    minutes = 40;
    examLabel = 'Quick Practice';
    sourcePool = QUESTIONS;
  } else {
    qCount = 90;
    minutes = 120;
    examLabel = 'Full Exam';
    sourcePool = QUESTIONS;
  }

  // Sample questions proportionally by domain
  const domains = [...new Set(sourcePool.map(q => q.domain))];
  const grouped = {};
  domains.forEach(d => { grouped[d] = shuffle(sourcePool.filter(q => q.domain === d)); });

  let pool = [];
  if (examType === 'domain') {
    pool = shuffle(sourcePool).slice(0, qCount);
  } else {
    const totalAvail = sourcePool.length;
    domains.forEach(d => {
      const domainShare = Math.max(1, Math.round((grouped[d].length / totalAvail) * qCount));
      pool.push(...grouped[d].slice(0, domainShare));
    });
    pool = shuffle(pool).slice(0, qCount);
  }

  // Shuffle answer options within each question and remap correct answer
  const processedPool = pool.map(q => {
    // Create indexed options and shuffle
    const indexed = q.opts.map((opt, i) => ({ opt, originalIdx: i }));
    const shuffledIndexed = shuffle(indexed);
    const newOpts = shuffledIndexed.map(x => x.opt);
    // Find where the original correct answer ended up
    const newAns = shuffledIndexed.findIndex(x => x.originalIdx === q.ans);
    return { ...q, opts: newOpts, ans: newAns };
  });

  examState = {
    questions: processedPool,
    current: 0,
    answers: new Array(processedPool.length).fill(null),
    flags: new Array(processedPool.length).fill(false),
    timerInterval: null,
    secondsLeft: minutes * 60,
    startedAt: Date.now(),
    examLabel
  };

  buildSidebar();
  renderQuestion();
  showScreen('exam');
  startTimer();
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// TIMER
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
function startTimer() {
  updateTimerDisplay();
  examState.timerInterval = setInterval(() => {
    examState.secondsLeft--;
    updateTimerDisplay();
    if (examState.secondsLeft <= 0) {
      clearInterval(examState.timerInterval);
      document.getElementById('modal-timeup').classList.add('open');
    }
  }, 1000);
}

function updateTimerDisplay() {
  const s = examState.secondsLeft;
  const m = Math.floor(s / 60), sec = s % 60;
  const el = document.getElementById('timer-display');
  el.textContent = `${String(m).padStart(2, '0')}:${String(sec).padStart(2, '0')}`;
  el.className = 'timer-display' + (s <= 300 ? ' danger' : s <= 600 ? ' warn' : '');
}

function formatTime(seconds) {
  const m = Math.floor(seconds / 60), s = seconds % 60;
  return `${String(m).padStart(2, '0')}:${String(s).padStart(2, '0')}`;
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// SIDEBAR
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
function buildSidebar() {
  const grid = document.getElementById('q-grid');
  grid.innerHTML = '';
  examState.questions.forEach((_, i) => {
    const btn = document.createElement('button');
    btn.className = 'q-btn';
    btn.id = `qb-${i}`;
    btn.textContent = i + 1;
    btn.onclick = () => navTo(i);
    grid.appendChild(btn);
  });
  updateSidebar();
}

function updateSidebar() {
  const { questions, current, answers, flags } = examState;
  questions.forEach((_, i) => {
    const btn = document.getElementById(`qb-${i}`);
    if (!btn) return;
    let cls = 'q-btn';
    if (i === current) cls += ' current';
    else if (flags[i]) cls += ' flagged';
    else if (answers[i] !== null) cls += ' answered';
    btn.className = cls;
  });
  const answered = answers.filter(a => a !== null).length;
  document.getElementById('exam-answered-count').textContent = `Answered: ${answered}`;
  document.getElementById('exam-qcounter').textContent = `${current + 1}/${questions.length}`;
  const pct = ((current + 1) / questions.length) * 100;
  document.getElementById('exam-progress-fill').style.width = pct + '%';
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// RENDER QUESTION
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
let currentQ = 0;

function navTo(idx) {
  if (idx < 0 || idx >= examState.questions.length) return;
  examState.current = idx;
  currentQ = idx;
  renderQuestion();
}

function renderQuestion() {
  const { questions, current, answers, flags } = examState;
  currentQ = current;
  const q = questions[current];
  const userAns = answers[current];
  const isAnswered = userAns !== null;
  const letters = ['A', 'B', 'C', 'D'];

  document.getElementById('exam-domain-tag').textContent = q.domain;
  document.getElementById('q-counter-label').textContent = `Question ${current + 1} of ${questions.length}`;

  // Flag button
  const flagBtn = document.getElementById('flag-btn');
  flagBtn.className = 'flag-toggle' + (flags[current] ? ' flagged' : '');
  flagBtn.textContent = flags[current] ? '‚öë Flagged' : '‚öë Flag';

  // Question stem
  document.getElementById('q-stem').textContent = q.q;

  // Code block
  const codeEl = document.getElementById('q-code');
  if (q.code) {
    codeEl.innerHTML = syntaxHighlight(q.code);
    codeEl.classList.remove('hidden');
  } else {
    codeEl.classList.add('hidden');
  }

  // Options
  const optsEl = document.getElementById('q-options');
  optsEl.innerHTML = q.opts.map((opt, i) => {
    let cls = 'opt';
    if (isAnswered) {
      cls += ' locked';
      if (i === q.ans) cls += ' correct';
      else if (i === userAns) cls += ' wrong';
    } else if (i === userAns) {
      cls += ' selected';
    }
    const icon = isAnswered ? (i === q.ans ? '<span style="margin-left:auto;font-size:14px">‚úì</span>' : (i === userAns ? '<span style="margin-left:auto;font-size:14px">‚úó</span>' : '')) : '';
    return `<div class="${cls}" onclick="selectAnswer(${i})">
      <div class="opt-key">${letters[i]}</div>
      <div class="opt-text">${opt}</div>
      ${icon}
    </div>`;
  }).join('');

  // Explanation
  const expEl = document.getElementById('q-explanation');
  if (isAnswered) {
    const correct = userAns === q.ans;
    expEl.className = 'explanation ' + (correct ? 'show-correct' : 'show-wrong');
    expEl.innerHTML = `<div class="exp-heading">${correct ? '‚úì Correct' : '‚úó Incorrect'}</div>${q.exp}`;
  } else {
    expEl.className = 'explanation hidden';
  }

  // Nav buttons
  document.getElementById('btn-prev').disabled = current === 0;
  document.getElementById('btn-next').textContent = current === questions.length - 1 ? 'Finish ‚Üí' : 'Next ‚Üí';

  // Status
  const answered = answers.filter(a => a !== null).length;
  document.getElementById('q-status-mid').innerHTML = `<strong>${answered}</strong> / ${questions.length} answered`;

  updateSidebar();
}

function selectAnswer(idx) {
  if (examState.answers[examState.current] !== null) return;
  examState.answers[examState.current] = idx;
  renderQuestion();
}

function toggleFlag() {
  examState.flags[examState.current] = !examState.flags[examState.current];
  renderQuestion();
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// SUBMIT
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
function openSubmitModal() {
  const unanswered = examState.answers.filter(a => a === null).length;
  document.getElementById('modal-submit-msg').innerHTML = unanswered > 0
    ? `You have <strong>${unanswered} unanswered question${unanswered !== 1 ? 's' : ''}</strong>. These will be marked incorrect. Submit now?`
    : `All ${examState.questions.length} questions answered. Submit your exam?`;
  openModal('modal-submit');
}

function openModal(id) { document.getElementById(id).classList.add('open'); }
function closeModal(id) { document.getElementById(id).classList.remove('open'); }

function submitExam() {
  clearInterval(examState.timerInterval);
  closeModal('modal-submit');
  closeModal('modal-timeup');

  const { questions, answers, secondsLeft, startedAt } = examState;
  const totalSeconds = Math.round((Date.now() - startedAt) / 1000);
  const elapsed = totalSeconds;

  const total = questions.length;
  const correct = answers.filter((a, i) => a !== null && a === questions[i].ans).length;
  const pct = Math.round((correct / total) * 100);
  const passed = pct >= 70;

  // Domain stats
  const domains = [...new Set(questions.map(q => q.domain))];
  const domainStats = domains.map(d => {
    const dqs = questions.map((q, i) => ({ q, i })).filter(x => x.q.domain === d);
    const dCorrect = dqs.filter(x => answers[x.i] === x.q.ans).length;
    return { domain: d, correct: dCorrect, total: dqs.length };
  });

  // Save to history
  const session = getCurrentUser();
  if (session) {
    const userData = getUserData(session.username) || { username: session.username, name: session.name, history: [], createdAt: Date.now() };
    userData.history = userData.history || [];
    userData.history.push({ timestamp: Date.now(), total, correct, pct, passed, elapsed, domainStats, examLabel: examState.examLabel || 'Full Exam' });
    saveUserData(session.username, userData);
  }

  showResults({ questions, answers, total, correct, pct, passed, elapsed, domainStats });
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// RESULTS
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
const DOMAIN_RECS = {
  "Lakehouse Platform": "Review Delta Lake architecture, cluster types, SQL Warehouses, Photon, Unity Catalog metastore structure, and DBFS.",
  "ELT with Spark & Python": "Practice DataFrame transformations, lazy evaluation, join strategies, window functions, UDFs, schema handling, and AQE.",
  "Incremental Data Processing": "Study Auto Loader, Structured Streaming (triggers/modes/checkpointing), DLT, Delta CDF, MERGE INTO, and VACUUM.",
  "Production Pipelines": "Understand Databricks Jobs, task dependencies, DABs, retry policies, secret management, and cluster policies.",
  "Data Governance": "Review Unity Catalog hierarchy, privilege model (USE CATALOG/USE SCHEMA/SELECT), column masks, row filters, and lineage."
};

function showResults({ questions, answers, total, correct, pct, passed, elapsed, domainStats }) {
  const wrong = answers.filter((a, i) => a !== null && a !== questions[i].ans).length;
  const skipped = answers.filter(a => a === null).length;
  const letters = ['A', 'B', 'C', 'D'];

  const elapsedStr = formatTime(elapsed);
  const domainsSorted = [...domainStats].sort((a, b) => (a.correct / a.total) - (b.correct / b.total));
  const weakDomains = domainsSorted.filter(d => Math.round((d.correct / d.total) * 100) < 70);

  const session = getCurrentUser();
  const userData = session ? getUserData(session.username) : null;
  const history = userData?.history || [];
  const isImprovement = history.length > 1 && pct > history[history.length - 2].pct;
  const prevScore = history.length > 1 ? history[history.length - 2].pct : null;

  const resultsEl = document.getElementById('screen-results');
  resultsEl.innerHTML = `
    <div class="results-hero">
      <div class="results-verdict ${passed ? 'verdict-pass' : 'verdict-fail'}">${passed ? '‚úì PASS' : '‚úó FAIL'} ‚Äî ${passed ? 'Congratulations!' : 'Keep Studying'}</div>
      <div class="results-pct ${passed ? 'pass' : 'fail'}">${pct}%</div>
      <div class="results-meta">
        Pass threshold: <strong>70%</strong> &nbsp;¬∑&nbsp; Time used: <strong>${elapsedStr}</strong>
        ${prevScore !== null ? `&nbsp;¬∑&nbsp; ${isImprovement ? 'üìà +' + (pct - prevScore) + '% vs last exam' : 'üìâ ' + (pct - prevScore) + '% vs last exam'}` : ''}
      </div>
      <div class="results-scorecards">
        <div class="scard"><div class="scard-val" style="color:var(--green)">${correct}</div><div class="scard-label">Correct</div></div>
        <div class="scard"><div class="scard-val" style="color:var(--red)">${wrong}</div><div class="scard-label">Incorrect</div></div>
        <div class="scard"><div class="scard-val" style="color:var(--yellow)">${skipped}</div><div class="scard-label">Skipped</div></div>
        <div class="scard"><div class="scard-val">${total}</div><div class="scard-label">Total</div></div>
      </div>
    </div>

    <div class="results-body">
      <div class="section-heading">Performance by Domain</div>
      <div class="domain-cards">
        ${domainsSorted.map(d => {
          const p = Math.round((d.correct / d.total) * 100);
          const color = p >= 80 ? 'var(--green)' : p >= 65 ? 'var(--yellow)' : 'var(--red)';
          const label = p >= 80 ? '‚úì Strong' : p >= 65 ? '‚ö† Needs Practice' : '‚úó Weak ‚Äî Study Required';
          return `<div class="dcard">
            <div class="dcard-top">
              <div class="dcard-name">${d.domain}</div>
              <div class="dcard-score" style="color:${color}">${d.correct}/${d.total} ¬∑ ${p}%</div>
            </div>
            <div class="dcard-bar"><div class="dcard-fill" style="width:${p}%;background:${color}"></div></div>
            <div class="dcard-rec ${p < 70 ? 'weak' : ''}">${label}${p < 70 && DOMAIN_RECS[d.domain] ? ' ‚Äî ' + DOMAIN_RECS[d.domain] : ''}</div>
          </div>`;
        }).join('')}
      </div>

      ${weakDomains.length > 0 ? `
      <div class="section-heading">üéØ Priority Study Areas</div>
      <div class="weak-box">
        <div class="weak-box-title">‚ö† Focus on these topics before your exam</div>
        ${weakDomains.map((d, i) => {
          const p = Math.round((d.correct / d.total) * 100);
          return `<div class="weak-item">
            <div class="weak-num">${i + 1}</div>
            <div class="weak-info">
              <strong>${d.domain} ‚Äî ${p}%</strong>
              <span>${DOMAIN_RECS[d.domain] || 'Review this topic area thoroughly.'}</span>
            </div>
          </div>`;
        }).join('')}
      </div>` : `
      <div style="background:var(--green-dim);border:1px solid rgba(34,197,94,0.3);border-radius:10px;padding:20px 24px;margin-bottom:32px;">
        <strong style="color:var(--green)">üéâ No critical weak areas!</strong>
        <span style="color:var(--text2);font-size:13px;margin-left:8px;">You scored above 70% in every domain. Focus on weak spots before exam day.</span>
      </div>`}

      <div class="section-heading">Question-by-Question Review</div>
      <div class="review-list">
        ${questions.map((q, i) => {
          const ua = answers[i];
          const isCorr = ua === q.ans;
          const isSkip = ua === null;
          const iconClass = isSkip ? 's' : isCorr ? 'c' : 'w';
          const icon = isSkip ? '‚Äî' : isCorr ? '‚úì' : '‚úó';
          return `<div class="review-item">
            <div class="review-header" onclick="this.nextElementSibling.classList.toggle('open')">
              <div class="r-icon ${iconClass}">${icon}</div>
              <div class="review-q-text">${q.q.substring(0, 120)}${q.q.length > 120 ? '‚Ä¶' : ''}</div>
              <div class="review-domain-tag">${q.domain.replace('Incremental Data Processing','Incremental').replace('ELT with Spark & Python','ELT/Spark').replace('Production Pipelines','Pipelines').replace('Data Governance','Governance').replace('Lakehouse Platform','Lakehouse')}</div>
            </div>
            <div class="review-body">
              ${ua !== null && ua !== q.ans ? `<div class="review-ans-row wrong">‚úó Your answer: ${letters[ua]}. ${q.opts[ua]}</div>` : ''}
              ${ua === null ? `<div class="review-ans-row wrong">‚Äî Not answered</div>` : ''}
              <div class="review-ans-row correct">‚úì Correct: ${letters[q.ans]}. ${q.opts[q.ans]}</div>
              <div class="review-exp-text">${q.exp}</div>
            </div>
          </div>`;
        }).join('')}
      </div>

      <div class="results-actions">
        <button class="btn-res ghost" onclick="loadDashboard()">‚Üê Dashboard</button>
        <button class="btn-res primary" onclick="startExam()">Retake Exam ‚Ü∫</button>
      </div>
    </div>`;

  showScreen('results');

  // Animate domain bars
  setTimeout(() => {
    document.querySelectorAll('.dcard-fill').forEach(el => {
      const w = el.style.width;
      el.style.width = '0%';
      setTimeout(() => el.style.width = w, 50);
    });
  }, 100);
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// SYNTAX HIGHLIGHTING
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
function syntaxHighlight(code) {
  return code
    .replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;')
    .replace(/(#[^\n]*)/g, '<span class="cm">$1</span>')
    .replace(/\b(def|return|import|from|for|in|if|else|elif|True|False|None|class|with|as|and|or|not|lambda|yield|try|except|finally|pass|break|continue|global|nonlocal|del|assert|raise|is)\b/g, '<span class="kw">$1</span>')
    .replace(/\b(SELECT|FROM|WHERE|GROUP|BY|ORDER|HAVING|JOIN|LEFT|RIGHT|INNER|OUTER|ON|AS|INTO|SET|UPDATE|DELETE|INSERT|CREATE|TABLE|VIEW|WITH|DISTINCT|LIMIT|AND|OR|NOT|IS|NULL|IN|LIKE|CASE|WHEN|THEN|ELSE|END|MERGE|MATCHED|USING|VALUES|ALTER|DROP|ADD|COLUMN|SCHEMA|DATABASE|CATALOG|GRANT|REVOKE|SHOW)\b/gi, '<span class="kw">$1</span>')
    .replace(/'([^']*)'/g, '<span class="str">\'$1\'</span>')
    .replace(/"([^"]*)"/g, '<span class="str">"$1"</span>')
    .replace(/\b(\d+\.?\d*)\b/g, '<span class="num">$1</span>');
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// INIT
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
(function init() {
  const session = getCurrentUser();
  if (session) {
    loadDashboard();
  } else {
    showScreen('auth');
  }
})();
</script>
</body>
</html>
